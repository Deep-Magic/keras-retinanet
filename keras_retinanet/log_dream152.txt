Creating model, this may take a second...
Using ResNet152 Backbone
Epoch 1/50

   1/3000 [..............................] - ETA: 193:26:36 - loss: 5.3118 - regression_loss: 4.1619 - classification_loss: 1.1500
   2/3000 [..............................] - ETA: 96:51:14 - loss: 5.3984 - regression_loss: 4.2539 - classification_loss: 1.1444 
   3/3000 [..............................] - ETA: 64:39:15 - loss: 5.3277 - regression_loss: 4.1833 - classification_loss: 1.1443
   4/3000 [..............................] - ETA: 48:33:20 - loss: 5.2532 - regression_loss: 4.1081 - classification_loss: 1.1451
   5/3000 [..............................] - ETA: 38:53:41 - loss: 5.2706 - regression_loss: 4.1247 - classification_loss: 1.1459
   6/3000 [..............................] - ETA: 32:27:16 - loss: 5.2566 - regression_loss: 4.1107 - classification_loss: 1.1459
   7/3000 [..............................] - ETA: 27:51:17 - loss: 5.2686 - regression_loss: 4.1239 - classification_loss: 1.1448
   8/3000 [..............................] - ETA: 24:25:36 - loss: 5.2871 - regression_loss: 4.1433 - classification_loss: 1.1438
   9/3000 [..............................] - ETA: 21:44:29 - loss: 5.3277 - regression_loss: 4.1838 - classification_loss: 1.1438
  10/3000 [..............................] - ETA: 19:35:35 - loss: 5.3390 - regression_loss: 4.1945 - classification_loss: 1.1444
  11/3000 [..............................] - ETA: 17:50:07 - loss: 5.3689 - regression_loss: 4.2245 - classification_loss: 1.1444
  12/3000 [..............................] - ETA: 16:22:15 - loss: 5.3835 - regression_loss: 4.2389 - classification_loss: 1.1446
  13/3000 [..............................] - ETA: 15:07:51 - loss: 5.3785 - regression_loss: 4.2351 - classification_loss: 1.1434
  14/3000 [..............................] - ETA: 14:06:35 - loss: 5.3862 - regression_loss: 4.2436 - classification_loss: 1.1425
  15/3000 [..............................] - ETA: 13:11:10 - loss: 5.3657 - regression_loss: 4.2228 - classification_loss: 1.1428
  16/3000 [..............................] - ETA: 12:22:42 - loss: 5.3582 - regression_loss: 4.2156 - classification_loss: 1.1426
  17/3000 [..............................] - ETA: 11:39:55 - loss: 5.3573 - regression_loss: 4.2143 - classification_loss: 1.1430
  18/3000 [..............................] - ETA: 11:01:53 - loss: 5.3474 - regression_loss: 4.2042 - classification_loss: 1.1432
  19/3000 [..............................] - ETA: 10:27:51 - loss: 5.3532 - regression_loss: 4.2095 - classification_loss: 1.1437
  20/3000 [..............................] - ETA: 9:57:14 - loss: 5.3507 - regression_loss: 4.2069 - classification_loss: 1.1438 
  21/3000 [..............................] - ETA: 9:29:32 - loss: 5.3420 - regression_loss: 4.1981 - classification_loss: 1.1439
  22/3000 [..............................] - ETA: 9:04:20 - loss: 5.3355 - regression_loss: 4.1914 - classification_loss: 1.1440
  23/3000 [..............................] - ETA: 8:41:21 - loss: 5.3305 - regression_loss: 4.1868 - classification_loss: 1.1437
  24/3000 [..............................] - ETA: 8:20:16 - loss: 5.3341 - regression_loss: 4.1910 - classification_loss: 1.1432
  25/3000 [..............................] - ETA: 8:00:50 - loss: 5.3336 - regression_loss: 4.1903 - classification_loss: 1.1433
  26/3000 [..............................] - ETA: 7:42:56 - loss: 5.3248 - regression_loss: 4.1814 - classification_loss: 1.1434
  27/3000 [..............................] - ETA: 7:26:22 - loss: 5.3255 - regression_loss: 4.1821 - classification_loss: 1.1434
  28/3000 [..............................] - ETA: 7:10:59 - loss: 5.3330 - regression_loss: 4.1896 - classification_loss: 1.1435
  29/3000 [..............................] - ETA: 6:56:39 - loss: 5.3301 - regression_loss: 4.1865 - classification_loss: 1.1436
  30/3000 [..............................] - ETA: 6:43:16 - loss: 5.3304 - regression_loss: 4.1868 - classification_loss: 1.1437
  31/3000 [..............................] - ETA: 6:30:46 - loss: 5.3362 - regression_loss: 4.1925 - classification_loss: 1.1437
  32/3000 [..............................] - ETA: 6:19:02 - loss: 5.3280 - regression_loss: 4.1842 - classification_loss: 1.1438
  33/3000 [..............................] - ETA: 6:08:01 - loss: 5.3223 - regression_loss: 4.1789 - classification_loss: 1.1434
  34/3000 [..............................] - ETA: 5:57:38 - loss: 5.3162 - regression_loss: 4.1727 - classification_loss: 1.1434
  35/3000 [..............................] - ETA: 5:47:51 - loss: 5.3112 - regression_loss: 4.1677 - classification_loss: 1.1435
  36/3000 [..............................] - ETA: 5:38:36 - loss: 5.3054 - regression_loss: 4.1618 - classification_loss: 1.1436
  37/3000 [..............................] - ETA: 5:29:52 - loss: 5.3078 - regression_loss: 4.1642 - classification_loss: 1.1436
  38/3000 [..............................] - ETA: 5:21:35 - loss: 5.3022 - regression_loss: 4.1586 - classification_loss: 1.1436
  39/3000 [..............................] - ETA: 5:13:42 - loss: 5.3030 - regression_loss: 4.1597 - classification_loss: 1.1432
  40/3000 [..............................] - ETA: 5:06:13 - loss: 5.3002 - regression_loss: 4.1571 - classification_loss: 1.1431
  41/3000 [..............................] - ETA: 4:59:07 - loss: 5.2947 - regression_loss: 4.1520 - classification_loss: 1.1427
  42/3000 [..............................] - ETA: 4:52:20 - loss: 5.2903 - regression_loss: 4.1475 - classification_loss: 1.1428
  43/3000 [..............................] - ETA: 4:45:52 - loss: 5.2887 - regression_loss: 4.1463 - classification_loss: 1.1425
  44/3000 [..............................] - ETA: 4:39:42 - loss: 5.2853 - regression_loss: 4.1432 - classification_loss: 1.1421
  45/3000 [..............................] - ETA: 4:33:50 - loss: 5.2822 - regression_loss: 4.1405 - classification_loss: 1.1417
  46/3000 [..............................] - ETA: 4:28:12 - loss: 5.2811 - regression_loss: 4.1394 - classification_loss: 1.1418
  47/3000 [..............................] - ETA: 4:22:49 - loss: 5.2810 - regression_loss: 4.1391 - classification_loss: 1.1418
  48/3000 [..............................] - ETA: 4:17:40 - loss: 5.2783 - regression_loss: 4.1369 - classification_loss: 1.1414
  49/3000 [..............................] - ETA: 4:12:43 - loss: 5.2792 - regression_loss: 4.1378 - classification_loss: 1.1414
  50/3000 [..............................] - ETA: 4:07:58 - loss: 5.2775 - regression_loss: 4.1364 - classification_loss: 1.1410
  51/3000 [..............................] - ETA: 4:03:25 - loss: 5.2687 - regression_loss: 4.1281 - classification_loss: 1.1406
  52/3000 [..............................] - ETA: 3:59:01 - loss: 5.2660 - regression_loss: 4.1255 - classification_loss: 1.1405
  53/3000 [..............................] - ETA: 3:54:48 - loss: 5.2657 - regression_loss: 4.1250 - classification_loss: 1.1406
  54/3000 [..............................] - ETA: 3:50:44 - loss: 5.2625 - regression_loss: 4.1220 - classification_loss: 1.1406
  55/3000 [..............................] - ETA: 3:46:48 - loss: 5.2596 - regression_loss: 4.1190 - classification_loss: 1.1406
  56/3000 [..............................] - ETA: 3:43:01 - loss: 5.2530 - regression_loss: 4.1125 - classification_loss: 1.1406
  57/3000 [..............................] - ETA: 3:39:22 - loss: 5.2456 - regression_loss: 4.1052 - classification_loss: 1.1405
  58/3000 [..............................] - ETA: 3:35:51 - loss: 5.2391 - regression_loss: 4.0987 - classification_loss: 1.1404
  59/3000 [..............................] - ETA: 3:32:27 - loss: 5.2327 - regression_loss: 4.0923 - classification_loss: 1.1403
  60/3000 [..............................] - ETA: 3:29:10 - loss: 5.2252 - regression_loss: 4.0849 - classification_loss: 1.1403
  61/3000 [..............................] - ETA: 3:25:59 - loss: 5.2162 - regression_loss: 4.0764 - classification_loss: 1.1398
  62/3000 [..............................] - ETA: 3:22:54 - loss: 5.2101 - regression_loss: 4.0703 - classification_loss: 1.1398
  63/3000 [..............................] - ETA: 3:19:56 - loss: 5.2032 - regression_loss: 4.0634 - classification_loss: 1.1398
  64/3000 [..............................] - ETA: 3:17:02 - loss: 5.2029 - regression_loss: 4.0635 - classification_loss: 1.1394
  65/3000 [..............................] - ETA: 3:14:14 - loss: 5.1991 - regression_loss: 4.0601 - classification_loss: 1.1390
  66/3000 [..............................] - ETA: 3:11:31 - loss: 5.1901 - regression_loss: 4.0514 - classification_loss: 1.1387
  67/3000 [..............................] - ETA: 3:08:53 - loss: 5.1855 - regression_loss: 4.0472 - classification_loss: 1.1383
  68/3000 [..............................] - ETA: 3:06:19 - loss: 5.1772 - regression_loss: 4.0393 - classification_loss: 1.1379
  69/3000 [..............................] - ETA: 3:03:50 - loss: 5.1732 - regression_loss: 4.0353 - classification_loss: 1.1379
  70/3000 [..............................] - ETA: 3:01:26 - loss: 5.1703 - regression_loss: 4.0328 - classification_loss: 1.1374
  71/3000 [..............................] - ETA: 2:59:04 - loss: 5.1666 - regression_loss: 4.0293 - classification_loss: 1.1373
  72/3000 [..............................] - ETA: 2:56:47 - loss: 5.1653 - regression_loss: 4.0279 - classification_loss: 1.1373
  73/3000 [..............................] - ETA: 2:54:34 - loss: 5.1611 - regression_loss: 4.0239 - classification_loss: 1.1373
  74/3000 [..............................] - ETA: 2:52:24 - loss: 5.1547 - regression_loss: 4.0178 - classification_loss: 1.1369
  75/3000 [..............................] - ETA: 2:50:19 - loss: 5.1594 - regression_loss: 4.0228 - classification_loss: 1.1366
  76/3000 [..............................] - ETA: 2:48:15 - loss: 5.1499 - regression_loss: 4.0136 - classification_loss: 1.1363
  77/3000 [..............................] - ETA: 2:46:16 - loss: 5.1432 - regression_loss: 4.0072 - classification_loss: 1.1360
  78/3000 [..............................] - ETA: 2:44:19 - loss: 5.1415 - regression_loss: 4.0056 - classification_loss: 1.1359
  79/3000 [..............................] - ETA: 2:42:25 - loss: 5.1344 - regression_loss: 3.9988 - classification_loss: 1.1356
  80/3000 [..............................] - ETA: 2:40:34 - loss: 5.1305 - regression_loss: 3.9952 - classification_loss: 1.1354
  81/3000 [..............................] - ETA: 2:38:46 - loss: 5.1264 - regression_loss: 3.9913 - classification_loss: 1.1351
  82/3000 [..............................] - ETA: 2:37:01 - loss: 5.1201 - regression_loss: 3.9854 - classification_loss: 1.1348
  83/3000 [..............................] - ETA: 2:35:18 - loss: 5.1200 - regression_loss: 3.9854 - classification_loss: 1.1346
  84/3000 [..............................] - ETA: 2:33:37 - loss: 5.1167 - regression_loss: 3.9825 - classification_loss: 1.1342
  85/3000 [..............................] - ETA: 2:31:59 - loss: 5.1242 - regression_loss: 3.9904 - classification_loss: 1.1338
  86/3000 [..............................] - ETA: 2:30:23 - loss: 5.1195 - regression_loss: 3.9860 - classification_loss: 1.1335
  87/3000 [..............................] - ETA: 2:28:49 - loss: 5.1167 - regression_loss: 3.9838 - classification_loss: 1.1329
  88/3000 [..............................] - ETA: 2:27:18 - loss: 5.1086 - regression_loss: 3.9762 - classification_loss: 1.1324
  89/3000 [..............................] - ETA: 2:25:48 - loss: 5.1068 - regression_loss: 3.9748 - classification_loss: 1.1320
  90/3000 [..............................] - ETA: 2:24:20 - loss: 5.1031 - regression_loss: 3.9719 - classification_loss: 1.1313
  91/3000 [..............................] - ETA: 2:22:55 - loss: 5.0997 - regression_loss: 3.9691 - classification_loss: 1.1306
  92/3000 [..............................] - ETA: 2:21:31 - loss: 5.0986 - regression_loss: 3.9687 - classification_loss: 1.1299
  93/3000 [..............................] - ETA: 2:20:09 - loss: 5.0975 - regression_loss: 3.9682 - classification_loss: 1.1292
  94/3000 [..............................] - ETA: 2:18:49 - loss: 5.0930 - regression_loss: 3.9643 - classification_loss: 1.1287
  95/3000 [..............................] - ETA: 2:17:31 - loss: 5.0908 - regression_loss: 3.9627 - classification_loss: 1.1281
  96/3000 [..............................] - ETA: 2:16:14 - loss: 5.0885 - regression_loss: 3.9615 - classification_loss: 1.1270
  97/3000 [..............................] - ETA: 2:14:59 - loss: 5.0842 - regression_loss: 3.9578 - classification_loss: 1.1264
  98/3000 [..............................] - ETA: 2:13:44 - loss: 5.0767 - regression_loss: 3.9509 - classification_loss: 1.1258
  99/3000 [..............................] - ETA: 2:12:32 - loss: 5.0755 - regression_loss: 3.9505 - classification_loss: 1.1251
 100/3000 [>.............................] - ETA: 2:11:21 - loss: 5.0705 - regression_loss: 3.9469 - classification_loss: 1.1236
 101/3000 [>.............................] - ETA: 2:10:12 - loss: 5.0643 - regression_loss: 3.9415 - classification_loss: 1.1228
 102/3000 [>.............................] - ETA: 2:09:04 - loss: 5.0613 - regression_loss: 3.9400 - classification_loss: 1.1213
 103/3000 [>.............................] - ETA: 2:07:58 - loss: 5.0578 - regression_loss: 3.9388 - classification_loss: 1.1190
 104/3000 [>.............................] - ETA: 2:06:53 - loss: 5.0513 - regression_loss: 3.9344 - classification_loss: 1.1169
 105/3000 [>.............................] - ETA: 2:05:49 - loss: 5.0451 - regression_loss: 3.9307 - classification_loss: 1.1144
 106/3000 [>.............................] - ETA: 2:04:47 - loss: 5.0403 - regression_loss: 3.9272 - classification_loss: 1.1130
 107/3000 [>.............................] - ETA: 2:03:45 - loss: 5.0352 - regression_loss: 3.9252 - classification_loss: 1.1100
 108/3000 [>.............................] - ETA: 2:02:45 - loss: 5.0324 - regression_loss: 3.9251 - classification_loss: 1.1073
 109/3000 [>.............................] - ETA: 2:01:46 - loss: 5.0362 - regression_loss: 3.9298 - classification_loss: 1.1065
 110/3000 [>.............................] - ETA: 2:00:48 - loss: 5.0280 - regression_loss: 3.9231 - classification_loss: 1.1049
 111/3000 [>.............................] - ETA: 1:59:52 - loss: 5.0298 - regression_loss: 3.9232 - classification_loss: 1.1066
 112/3000 [>.............................] - ETA: 1:58:56 - loss: 5.0294 - regression_loss: 3.9252 - classification_loss: 1.1042
 113/3000 [>.............................] - ETA: 1:58:01 - loss: 5.0239 - regression_loss: 3.9225 - classification_loss: 1.1014
 114/3000 [>.............................] - ETA: 1:57:07 - loss: 5.0201 - regression_loss: 3.9197 - classification_loss: 1.1004
 115/3000 [>.............................] - ETA: 1:56:14 - loss: 5.0119 - regression_loss: 3.9140 - classification_loss: 1.0979
 116/3000 [>.............................] - ETA: 1:55:21 - loss: 5.0017 - regression_loss: 3.9068 - classification_loss: 1.0949
 117/3000 [>.............................] - ETA: 1:54:30 - loss: 4.9964 - regression_loss: 3.9052 - classification_loss: 1.0913
 118/3000 [>.............................] - ETA: 1:53:40 - loss: 4.9959 - regression_loss: 3.9053 - classification_loss: 1.0906
 119/3000 [>.............................] - ETA: 1:52:50 - loss: 4.9910 - regression_loss: 3.9029 - classification_loss: 1.0881
 120/3000 [>.............................] - ETA: 1:52:02 - loss: 4.9874 - regression_loss: 3.9012 - classification_loss: 1.0861
 121/3000 [>.............................] - ETA: 1:51:14 - loss: 4.9791 - regression_loss: 3.8965 - classification_loss: 1.0826
 122/3000 [>.............................] - ETA: 1:50:27 - loss: 4.9841 - regression_loss: 3.9039 - classification_loss: 1.0803
 123/3000 [>.............................] - ETA: 1:49:40 - loss: 4.9801 - regression_loss: 3.9008 - classification_loss: 1.0793
 124/3000 [>.............................] - ETA: 1:48:55 - loss: 4.9717 - regression_loss: 3.8963 - classification_loss: 1.0753
 125/3000 [>.............................] - ETA: 1:48:10 - loss: 4.9672 - regression_loss: 3.8947 - classification_loss: 1.0725
 126/3000 [>.............................] - ETA: 1:47:26 - loss: 4.9602 - regression_loss: 3.8915 - classification_loss: 1.0687
 127/3000 [>.............................] - ETA: 1:46:42 - loss: 4.9510 - regression_loss: 3.8854 - classification_loss: 1.0656
 128/3000 [>.............................] - ETA: 1:46:00 - loss: 4.9445 - regression_loss: 3.8808 - classification_loss: 1.0637
 129/3000 [>.............................] - ETA: 1:45:18 - loss: 4.9361 - regression_loss: 3.8739 - classification_loss: 1.0622
 130/3000 [>.............................] - ETA: 1:44:36 - loss: 4.9303 - regression_loss: 3.8714 - classification_loss: 1.0589
 131/3000 [>.............................] - ETA: 1:43:55 - loss: 4.9229 - regression_loss: 3.8677 - classification_loss: 1.0552
 132/3000 [>.............................] - ETA: 1:43:15 - loss: 4.9143 - regression_loss: 3.8620 - classification_loss: 1.0522
 133/3000 [>.............................] - ETA: 1:42:35 - loss: 4.9053 - regression_loss: 3.8571 - classification_loss: 1.0482
 134/3000 [>.............................] - ETA: 1:41:55 - loss: 4.8954 - regression_loss: 3.8511 - classification_loss: 1.0443
 135/3000 [>.............................] - ETA: 1:41:17 - loss: 4.8906 - regression_loss: 3.8483 - classification_loss: 1.0423
 136/3000 [>.............................] - ETA: 1:40:39 - loss: 4.8859 - regression_loss: 3.8458 - classification_loss: 1.0402
 137/3000 [>.............................] - ETA: 1:40:01 - loss: 4.8819 - regression_loss: 3.8448 - classification_loss: 1.0371
 138/3000 [>.............................] - ETA: 1:39:24 - loss: 4.8747 - regression_loss: 3.8398 - classification_loss: 1.0349
 139/3000 [>.............................] - ETA: 1:38:48 - loss: 4.8694 - regression_loss: 3.8369 - classification_loss: 1.0325
 140/3000 [>.............................] - ETA: 1:38:12 - loss: 4.8649 - regression_loss: 3.8355 - classification_loss: 1.0294
 141/3000 [>.............................] - ETA: 1:37:36 - loss: 4.8586 - regression_loss: 3.8314 - classification_loss: 1.0272
 142/3000 [>.............................] - ETA: 1:37:01 - loss: 4.8524 - regression_loss: 3.8284 - classification_loss: 1.0240
 143/3000 [>.............................] - ETA: 1:36:27 - loss: 4.8470 - regression_loss: 3.8253 - classification_loss: 1.0217
 144/3000 [>.............................] - ETA: 1:35:53 - loss: 4.8394 - regression_loss: 3.8216 - classification_loss: 1.0178
 145/3000 [>.............................] - ETA: 1:35:19 - loss: 4.8386 - regression_loss: 3.8222 - classification_loss: 1.0164
 146/3000 [>.............................] - ETA: 1:34:46 - loss: 4.8328 - regression_loss: 3.8189 - classification_loss: 1.0139
 147/3000 [>.............................] - ETA: 1:34:13 - loss: 4.8287 - regression_loss: 3.8173 - classification_loss: 1.0114
 148/3000 [>.............................] - ETA: 1:33:41 - loss: 4.8231 - regression_loss: 3.8139 - classification_loss: 1.0092
 149/3000 [>.............................] - ETA: 1:33:09 - loss: 4.8169 - regression_loss: 3.8101 - classification_loss: 1.0068
 150/3000 [>.............................] - ETA: 1:32:37 - loss: 4.8147 - regression_loss: 3.8095 - classification_loss: 1.0052
 151/3000 [>.............................] - ETA: 1:32:06 - loss: 4.8087 - regression_loss: 3.8058 - classification_loss: 1.0029
 152/3000 [>.............................] - ETA: 1:31:35 - loss: 4.8000 - regression_loss: 3.7999 - classification_loss: 1.0002
 153/3000 [>.............................] - ETA: 1:31:05 - loss: 4.7982 - regression_loss: 3.7998 - classification_loss: 0.9984
 154/3000 [>.............................] - ETA: 1:30:35 - loss: 4.7952 - regression_loss: 3.7960 - classification_loss: 0.9992
 155/3000 [>.............................] - ETA: 1:30:06 - loss: 4.7928 - regression_loss: 3.7930 - classification_loss: 0.9997
 156/3000 [>.............................] - ETA: 1:29:36 - loss: 4.7938 - regression_loss: 3.7957 - classification_loss: 0.9981
 157/3000 [>.............................] - ETA: 1:29:07 - loss: 4.7876 - regression_loss: 3.7916 - classification_loss: 0.9959
 158/3000 [>.............................] - ETA: 1:28:39 - loss: 4.7799 - regression_loss: 3.7861 - classification_loss: 0.9939
 159/3000 [>.............................] - ETA: 1:28:11 - loss: 4.7728 - regression_loss: 3.7809 - classification_loss: 0.9919
 160/3000 [>.............................] - ETA: 1:27:43 - loss: 4.7682 - regression_loss: 3.7783 - classification_loss: 0.9900
 161/3000 [>.............................] - ETA: 1:27:16 - loss: 4.7614 - regression_loss: 3.7732 - classification_loss: 0.9881
 162/3000 [>.............................] - ETA: 1:26:49 - loss: 4.7543 - regression_loss: 3.7687 - classification_loss: 0.9856
 163/3000 [>.............................] - ETA: 1:26:22 - loss: 4.7519 - regression_loss: 3.7672 - classification_loss: 0.9847
 164/3000 [>.............................] - ETA: 1:25:56 - loss: 4.7450 - regression_loss: 3.7624 - classification_loss: 0.9827
 165/3000 [>.............................] - ETA: 1:25:30 - loss: 4.7394 - regression_loss: 3.7589 - classification_loss: 0.9805
 166/3000 [>.............................] - ETA: 1:25:04 - loss: 4.7420 - regression_loss: 3.7622 - classification_loss: 0.9798
 167/3000 [>.............................] - ETA: 1:24:39 - loss: 4.7358 - regression_loss: 3.7585 - classification_loss: 0.9773
 168/3000 [>.............................] - ETA: 1:24:14 - loss: 4.7308 - regression_loss: 3.7557 - classification_loss: 0.9751
 169/3000 [>.............................] - ETA: 1:23:50 - loss: 4.7260 - regression_loss: 3.7524 - classification_loss: 0.9736
 170/3000 [>.............................] - ETA: 1:23:25 - loss: 4.7190 - regression_loss: 3.7473 - classification_loss: 0.9717
 171/3000 [>.............................] - ETA: 1:23:01 - loss: 4.7164 - regression_loss: 3.7454 - classification_loss: 0.9710
 172/3000 [>.............................] - ETA: 1:22:37 - loss: 4.7132 - regression_loss: 3.7437 - classification_loss: 0.9695
 173/3000 [>.............................] - ETA: 1:22:13 - loss: 4.7072 - regression_loss: 3.7397 - classification_loss: 0.9676
 174/3000 [>.............................] - ETA: 1:21:50 - loss: 4.6983 - regression_loss: 3.7329 - classification_loss: 0.9653
 175/3000 [>.............................] - ETA: 1:21:26 - loss: 4.6926 - regression_loss: 3.7290 - classification_loss: 0.9636
 176/3000 [>.............................] - ETA: 1:21:03 - loss: 4.6872 - regression_loss: 3.7249 - classification_loss: 0.9623
 177/3000 [>.............................] - ETA: 1:20:41 - loss: 4.6823 - regression_loss: 3.7222 - classification_loss: 0.9602
 178/3000 [>.............................] - ETA: 1:20:18 - loss: 4.6755 - regression_loss: 3.7177 - classification_loss: 0.9578
 179/3000 [>.............................] - ETA: 1:19:56 - loss: 4.6713 - regression_loss: 3.7152 - classification_loss: 0.9561
 180/3000 [>.............................] - ETA: 1:19:34 - loss: 4.6627 - regression_loss: 3.7087 - classification_loss: 0.9540
 181/3000 [>.............................] - ETA: 1:19:12 - loss: 4.6603 - regression_loss: 3.7075 - classification_loss: 0.9528
 182/3000 [>.............................] - ETA: 1:18:51 - loss: 4.6538 - regression_loss: 3.7026 - classification_loss: 0.9512
 183/3000 [>.............................] - ETA: 1:18:30 - loss: 4.6487 - regression_loss: 3.6991 - classification_loss: 0.9496
 184/3000 [>.............................] - ETA: 1:18:09 - loss: 4.6473 - regression_loss: 3.6981 - classification_loss: 0.9492
 185/3000 [>.............................] - ETA: 1:17:48 - loss: 4.6415 - regression_loss: 3.6935 - classification_loss: 0.9480
 186/3000 [>.............................] - ETA: 1:17:27 - loss: 4.6374 - regression_loss: 3.6912 - classification_loss: 0.9462
 187/3000 [>.............................] - ETA: 1:17:07 - loss: 4.6324 - regression_loss: 3.6873 - classification_loss: 0.9451
 188/3000 [>.............................] - ETA: 1:16:47 - loss: 4.6262 - regression_loss: 3.6828 - classification_loss: 0.9434
 189/3000 [>.............................] - ETA: 1:16:27 - loss: 4.6244 - regression_loss: 3.6821 - classification_loss: 0.9423
 190/3000 [>.............................] - ETA: 1:16:07 - loss: 4.6147 - regression_loss: 3.6743 - classification_loss: 0.9405
 191/3000 [>.............................] - ETA: 1:15:48 - loss: 4.6054 - regression_loss: 3.6673 - classification_loss: 0.9381
 192/3000 [>.............................] - ETA: 1:15:29 - loss: 4.6008 - regression_loss: 3.6647 - classification_loss: 0.9361
 193/3000 [>.............................] - ETA: 1:15:09 - loss: 4.5931 - regression_loss: 3.6589 - classification_loss: 0.9343
 194/3000 [>.............................] - ETA: 1:14:50 - loss: 4.5917 - regression_loss: 3.6587 - classification_loss: 0.9329
 195/3000 [>.............................] - ETA: 1:14:32 - loss: 4.5854 - regression_loss: 3.6542 - classification_loss: 0.9312
 196/3000 [>.............................] - ETA: 1:14:13 - loss: 4.5827 - regression_loss: 3.6530 - classification_loss: 0.9297
 197/3000 [>.............................] - ETA: 1:13:55 - loss: 4.5734 - regression_loss: 3.6456 - classification_loss: 0.9278
 198/3000 [>.............................] - ETA: 1:13:37 - loss: 4.5681 - regression_loss: 3.6422 - classification_loss: 0.9260
 199/3000 [>.............................] - ETA: 1:13:19 - loss: 4.5636 - regression_loss: 3.6390 - classification_loss: 0.9245
 200/3000 [=>............................] - ETA: 1:13:01 - loss: 4.5596 - regression_loss: 3.6366 - classification_loss: 0.9231
 201/3000 [=>............................] - ETA: 1:12:43 - loss: 4.5531 - regression_loss: 3.6320 - classification_loss: 0.9211
 202/3000 [=>............................] - ETA: 1:12:25 - loss: 4.5494 - regression_loss: 3.6293 - classification_loss: 0.9201
 203/3000 [=>............................] - ETA: 1:12:08 - loss: 4.5454 - regression_loss: 3.6264 - classification_loss: 0.9191
 204/3000 [=>............................] - ETA: 1:11:51 - loss: 4.5369 - regression_loss: 3.6193 - classification_loss: 0.9176
 205/3000 [=>............................] - ETA: 1:11:34 - loss: 4.5343 - regression_loss: 3.6180 - classification_loss: 0.9163
 206/3000 [=>............................] - ETA: 1:11:17 - loss: 4.5327 - regression_loss: 3.6176 - classification_loss: 0.9151
 207/3000 [=>............................] - ETA: 1:11:00 - loss: 4.5310 - regression_loss: 3.6170 - classification_loss: 0.9141
 208/3000 [=>............................] - ETA: 1:10:44 - loss: 4.5267 - regression_loss: 3.6139 - classification_loss: 0.9128
 209/3000 [=>............................] - ETA: 1:10:27 - loss: 4.5177 - regression_loss: 3.6060 - classification_loss: 0.9117
 210/3000 [=>............................] - ETA: 1:10:11 - loss: 4.5114 - regression_loss: 3.6011 - classification_loss: 0.9102
 211/3000 [=>............................] - ETA: 1:09:55 - loss: 4.5065 - regression_loss: 3.5977 - classification_loss: 0.9088
 212/3000 [=>............................] - ETA: 1:09:39 - loss: 4.4961 - regression_loss: 3.5891 - classification_loss: 0.9071
 213/3000 [=>............................] - ETA: 1:09:23 - loss: 4.4902 - regression_loss: 3.5845 - classification_loss: 0.9057
 214/3000 [=>............................] - ETA: 1:09:07 - loss: 4.4874 - regression_loss: 3.5827 - classification_loss: 0.9047
 215/3000 [=>............................] - ETA: 1:08:52 - loss: 4.4844 - regression_loss: 3.5807 - classification_loss: 0.9037
 216/3000 [=>............................] - ETA: 1:08:36 - loss: 4.4785 - regression_loss: 3.5767 - classification_loss: 0.9018
 217/3000 [=>............................] - ETA: 1:08:21 - loss: 4.4719 - regression_loss: 3.5721 - classification_loss: 0.8998
 218/3000 [=>............................] - ETA: 1:08:05 - loss: 4.4641 - regression_loss: 3.5658 - classification_loss: 0.8983
 219/3000 [=>............................] - ETA: 1:07:50 - loss: 4.4580 - regression_loss: 3.5612 - classification_loss: 0.8968
 220/3000 [=>............................] - ETA: 1:07:35 - loss: 4.4544 - regression_loss: 3.5589 - classification_loss: 0.8955
 221/3000 [=>............................] - ETA: 1:07:21 - loss: 4.4443 - regression_loss: 3.5505 - classification_loss: 0.8937
 222/3000 [=>............................] - ETA: 1:07:06 - loss: 4.4377 - regression_loss: 3.5456 - classification_loss: 0.8921
 223/3000 [=>............................] - ETA: 1:06:51 - loss: 4.4307 - regression_loss: 3.5403 - classification_loss: 0.8904
 224/3000 [=>............................] - ETA: 1:06:37 - loss: 4.4233 - regression_loss: 3.5345 - classification_loss: 0.8888
 225/3000 [=>............................] - ETA: 1:06:23 - loss: 4.4159 - regression_loss: 3.5282 - classification_loss: 0.8877
 226/3000 [=>............................] - ETA: 1:06:09 - loss: 4.4091 - regression_loss: 3.5228 - classification_loss: 0.8863
 227/3000 [=>............................] - ETA: 1:05:55 - loss: 4.4035 - regression_loss: 3.5185 - classification_loss: 0.8850
 228/3000 [=>............................] - ETA: 1:05:41 - loss: 4.3983 - regression_loss: 3.5145 - classification_loss: 0.8838
 229/3000 [=>............................] - ETA: 1:05:28 - loss: 4.3916 - regression_loss: 3.5095 - classification_loss: 0.8821
 230/3000 [=>............................] - ETA: 1:05:14 - loss: 4.3826 - regression_loss: 3.5023 - classification_loss: 0.8802
 231/3000 [=>............................] - ETA: 1:05:01 - loss: 4.3787 - regression_loss: 3.4996 - classification_loss: 0.8791
 232/3000 [=>............................] - ETA: 1:04:47 - loss: 4.3733 - regression_loss: 3.4954 - classification_loss: 0.8779
 233/3000 [=>............................] - ETA: 1:04:34 - loss: 4.3705 - regression_loss: 3.4937 - classification_loss: 0.8768
 234/3000 [=>............................] - ETA: 1:04:21 - loss: 4.3679 - regression_loss: 3.4920 - classification_loss: 0.8760
 235/3000 [=>............................] - ETA: 1:04:08 - loss: 4.3605 - regression_loss: 3.4858 - classification_loss: 0.8747
 236/3000 [=>............................] - ETA: 1:03:55 - loss: 4.3585 - regression_loss: 3.4847 - classification_loss: 0.8738
 237/3000 [=>............................] - ETA: 1:03:42 - loss: 4.3516 - regression_loss: 3.4787 - classification_loss: 0.8729
 238/3000 [=>............................] - ETA: 1:03:29 - loss: 4.3477 - regression_loss: 3.4758 - classification_loss: 0.8718
 239/3000 [=>............................] - ETA: 1:03:17 - loss: 4.3401 - regression_loss: 3.4696 - classification_loss: 0.8705
 240/3000 [=>............................] - ETA: 1:03:05 - loss: 4.3311 - regression_loss: 3.4621 - classification_loss: 0.8690
 241/3000 [=>............................] - ETA: 1:02:52 - loss: 4.3234 - regression_loss: 3.4558 - classification_loss: 0.8676
 242/3000 [=>............................] - ETA: 1:02:40 - loss: 4.3173 - regression_loss: 3.4507 - classification_loss: 0.8666
 243/3000 [=>............................] - ETA: 1:02:28 - loss: 4.3137 - regression_loss: 3.4469 - classification_loss: 0.8668
 244/3000 [=>............................] - ETA: 1:02:15 - loss: 4.3059 - regression_loss: 3.4402 - classification_loss: 0.8657
 245/3000 [=>............................] - ETA: 1:02:03 - loss: 4.3065 - regression_loss: 3.4413 - classification_loss: 0.8653
 246/3000 [=>............................] - ETA: 1:01:52 - loss: 4.3029 - regression_loss: 3.4387 - classification_loss: 0.8642
 247/3000 [=>............................] - ETA: 1:01:40 - loss: 4.2964 - regression_loss: 3.4336 - classification_loss: 0.8628
 248/3000 [=>............................] - ETA: 1:01:29 - loss: 4.2895 - regression_loss: 3.4278 - classification_loss: 0.8616
 249/3000 [=>............................] - ETA: 1:01:17 - loss: 4.2867 - regression_loss: 3.4261 - classification_loss: 0.8606
 250/3000 [=>............................] - ETA: 1:01:05 - loss: 4.2797 - regression_loss: 3.4206 - classification_loss: 0.8591
 251/3000 [=>............................] - ETA: 1:00:54 - loss: 4.2744 - regression_loss: 3.4164 - classification_loss: 0.8580
 252/3000 [=>............................] - ETA: 1:00:43 - loss: 4.2723 - regression_loss: 3.4150 - classification_loss: 0.8573
 253/3000 [=>............................] - ETA: 1:00:31 - loss: 4.2688 - regression_loss: 3.4125 - classification_loss: 0.8563
 254/3000 [=>............................] - ETA: 1:00:20 - loss: 4.2623 - regression_loss: 3.4068 - classification_loss: 0.8554
 255/3000 [=>............................] - ETA: 1:00:09 - loss: 4.2552 - regression_loss: 3.4012 - classification_loss: 0.8541
 256/3000 [=>............................] - ETA: 59:58 - loss: 4.2492 - regression_loss: 3.3964 - classification_loss: 0.8528  
 257/3000 [=>............................] - ETA: 59:47 - loss: 4.2450 - regression_loss: 3.3931 - classification_loss: 0.8519
 258/3000 [=>............................] - ETA: 59:35 - loss: 4.2429 - regression_loss: 3.3917 - classification_loss: 0.8513
 259/3000 [=>............................] - ETA: 59:24 - loss: 4.2340 - regression_loss: 3.3843 - classification_loss: 0.8497
 260/3000 [=>............................] - ETA: 59:14 - loss: 4.2314 - regression_loss: 3.3817 - classification_loss: 0.8497
 261/3000 [=>............................] - ETA: 59:03 - loss: 4.2234 - regression_loss: 3.3752 - classification_loss: 0.8482
 262/3000 [=>............................] - ETA: 58:52 - loss: 4.2203 - regression_loss: 3.3733 - classification_loss: 0.8470
 263/3000 [=>............................] - ETA: 58:42 - loss: 4.2166 - regression_loss: 3.3705 - classification_loss: 0.8461
 264/3000 [=>............................] - ETA: 58:31 - loss: 4.2124 - regression_loss: 3.3671 - classification_loss: 0.8453
 265/3000 [=>............................] - ETA: 58:20 - loss: 4.2051 - regression_loss: 3.3608 - classification_loss: 0.8443
 266/3000 [=>............................] - ETA: 58:10 - loss: 4.2002 - regression_loss: 3.3568 - classification_loss: 0.8435
 267/3000 [=>............................] - ETA: 58:00 - loss: 4.1976 - regression_loss: 3.3548 - classification_loss: 0.8428
 268/3000 [=>............................] - ETA: 57:50 - loss: 4.1943 - regression_loss: 3.3524 - classification_loss: 0.8419
 269/3000 [=>............................] - ETA: 57:40 - loss: 4.1899 - regression_loss: 3.3490 - classification_loss: 0.8410
 270/3000 [=>............................] - ETA: 57:30 - loss: 4.1832 - regression_loss: 3.3433 - classification_loss: 0.8398
 271/3000 [=>............................] - ETA: 57:20 - loss: 4.1760 - regression_loss: 3.3368 - classification_loss: 0.8392
 272/3000 [=>............................] - ETA: 57:10 - loss: 4.1680 - regression_loss: 3.3303 - classification_loss: 0.8377
 273/3000 [=>............................] - ETA: 57:00 - loss: 4.1618 - regression_loss: 3.3250 - classification_loss: 0.8368
 274/3000 [=>............................] - ETA: 56:51 - loss: 4.1600 - regression_loss: 3.3236 - classification_loss: 0.8363
 275/3000 [=>............................] - ETA: 56:41 - loss: 4.1550 - regression_loss: 3.3194 - classification_loss: 0.8356
 276/3000 [=>............................] - ETA: 56:31 - loss: 4.1499 - regression_loss: 3.3155 - classification_loss: 0.8343
 277/3000 [=>............................] - ETA: 56:22 - loss: 4.1463 - regression_loss: 3.3126 - classification_loss: 0.8337
 278/3000 [=>............................] - ETA: 56:12 - loss: 4.1439 - regression_loss: 3.3111 - classification_loss: 0.8328
 279/3000 [=>............................] - ETA: 56:03 - loss: 4.1390 - regression_loss: 3.3073 - classification_loss: 0.8317
 280/3000 [=>............................] - ETA: 55:54 - loss: 4.1325 - regression_loss: 3.3021 - classification_loss: 0.8303
 281/3000 [=>............................] - ETA: 55:45 - loss: 4.1306 - regression_loss: 3.3009 - classification_loss: 0.8297
 282/3000 [=>............................] - ETA: 55:36 - loss: 4.1260 - regression_loss: 3.2969 - classification_loss: 0.8290
 283/3000 [=>............................] - ETA: 55:26 - loss: 4.1198 - regression_loss: 3.2920 - classification_loss: 0.8278
 284/3000 [=>............................] - ETA: 55:18 - loss: 4.1114 - regression_loss: 3.2851 - classification_loss: 0.8263
 285/3000 [=>............................] - ETA: 55:09 - loss: 4.1045 - regression_loss: 3.2795 - classification_loss: 0.8250
 286/3000 [=>............................] - ETA: 55:00 - loss: 4.0976 - regression_loss: 3.2737 - classification_loss: 0.8238
 287/3000 [=>............................] - ETA: 54:51 - loss: 4.0924 - regression_loss: 3.2697 - classification_loss: 0.8227
 288/3000 [=>............................] - ETA: 54:42 - loss: 4.0879 - regression_loss: 3.2661 - classification_loss: 0.8218
 289/3000 [=>............................] - ETA: 54:33 - loss: 4.0818 - regression_loss: 3.2612 - classification_loss: 0.8206
 290/3000 [=>............................] - ETA: 54:25 - loss: 4.0789 - regression_loss: 3.2590 - classification_loss: 0.8198
 291/3000 [=>............................] - ETA: 54:16 - loss: 4.0747 - regression_loss: 3.2553 - classification_loss: 0.8194
 292/3000 [=>............................] - ETA: 54:08 - loss: 4.0667 - regression_loss: 3.2485 - classification_loss: 0.8181
 293/3000 [=>............................] - ETA: 53:59 - loss: 4.0614 - regression_loss: 3.2442 - classification_loss: 0.8173
 294/3000 [=>............................] - ETA: 53:51 - loss: 4.0553 - regression_loss: 3.2393 - classification_loss: 0.8160
 295/3000 [=>............................] - ETA: 53:43 - loss: 4.0500 - regression_loss: 3.2350 - classification_loss: 0.8149
 296/3000 [=>............................] - ETA: 53:34 - loss: 4.0463 - regression_loss: 3.2322 - classification_loss: 0.8141
 297/3000 [=>............................] - ETA: 53:26 - loss: 4.0399 - regression_loss: 3.2270 - classification_loss: 0.8129
 298/3000 [=>............................] - ETA: 53:18 - loss: 4.0347 - regression_loss: 3.2223 - classification_loss: 0.8123
 299/3000 [=>............................] - ETA: 53:10 - loss: 4.0283 - regression_loss: 3.2170 - classification_loss: 0.8113
 300/3000 [==>...........................] - ETA: 53:01 - loss: 4.0255 - regression_loss: 3.2153 - classification_loss: 0.8103
 301/3000 [==>...........................] - ETA: 52:53 - loss: 4.0206 - regression_loss: 3.2110 - classification_loss: 0.8095
 302/3000 [==>...........................] - ETA: 52:45 - loss: 4.0161 - regression_loss: 3.2074 - classification_loss: 0.8088
 303/3000 [==>...........................] - ETA: 52:37 - loss: 4.0144 - regression_loss: 3.2065 - classification_loss: 0.8079
 304/3000 [==>...........................] - ETA: 52:29 - loss: 4.0122 - regression_loss: 3.2048 - classification_loss: 0.8074
 305/3000 [==>...........................] - ETA: 52:21 - loss: 4.0065 - regression_loss: 3.2005 - classification_loss: 0.8060
 306/3000 [==>...........................] - ETA: 52:13 - loss: 4.0013 - regression_loss: 3.1959 - classification_loss: 0.8054
 307/3000 [==>...........................] - ETA: 52:05 - loss: 3.9978 - regression_loss: 3.1930 - classification_loss: 0.8048
 308/3000 [==>...........................] - ETA: 51:57 - loss: 3.9930 - regression_loss: 3.1888 - classification_loss: 0.8041
 309/3000 [==>...........................] - ETA: 51:50 - loss: 3.9922 - regression_loss: 3.1886 - classification_loss: 0.8036
 310/3000 [==>...........................] - ETA: 51:42 - loss: 3.9866 - regression_loss: 3.1841 - classification_loss: 0.8024
 311/3000 [==>...........................] - ETA: 51:34 - loss: 3.9836 - regression_loss: 3.1813 - classification_loss: 0.8024
 312/3000 [==>...........................] - ETA: 51:27 - loss: 3.9774 - regression_loss: 3.1762 - classification_loss: 0.8011
 313/3000 [==>...........................] - ETA: 51:19 - loss: 3.9720 - regression_loss: 3.1718 - classification_loss: 0.8002
 314/3000 [==>...........................] - ETA: 51:11 - loss: 3.9687 - regression_loss: 3.1693 - classification_loss: 0.7994
 315/3000 [==>...........................] - ETA: 51:04 - loss: 3.9630 - regression_loss: 3.1645 - classification_loss: 0.7985
 316/3000 [==>...........................] - ETA: 50:57 - loss: 3.9637 - regression_loss: 3.1656 - classification_loss: 0.7981
 317/3000 [==>...........................] - ETA: 50:49 - loss: 3.9577 - regression_loss: 3.1608 - classification_loss: 0.7969
 318/3000 [==>...........................] - ETA: 50:42 - loss: 3.9521 - regression_loss: 3.1560 - classification_loss: 0.7961
 319/3000 [==>...........................] - ETA: 50:35 - loss: 3.9479 - regression_loss: 3.1525 - classification_loss: 0.7954
 320/3000 [==>...........................] - ETA: 50:28 - loss: 3.9461 - regression_loss: 3.1512 - classification_loss: 0.7949
 321/3000 [==>...........................] - ETA: 50:20 - loss: 3.9393 - regression_loss: 3.1454 - classification_loss: 0.7939
 322/3000 [==>...........................] - ETA: 50:13 - loss: 3.9361 - regression_loss: 3.1427 - classification_loss: 0.7934
 323/3000 [==>...........................] - ETA: 50:06 - loss: 3.9312 - regression_loss: 3.1385 - classification_loss: 0.7927
 324/3000 [==>...........................] - ETA: 49:59 - loss: 3.9271 - regression_loss: 3.1351 - classification_loss: 0.7920
 325/3000 [==>...........................] - ETA: 49:52 - loss: 3.9255 - regression_loss: 3.1341 - classification_loss: 0.7914
 326/3000 [==>...........................] - ETA: 49:45 - loss: 3.9230 - regression_loss: 3.1322 - classification_loss: 0.7908
 327/3000 [==>...........................] - ETA: 49:38 - loss: 3.9168 - regression_loss: 3.1269 - classification_loss: 0.7899
 328/3000 [==>...........................] - ETA: 49:32 - loss: 3.9117 - regression_loss: 3.1226 - classification_loss: 0.7891
 329/3000 [==>...........................] - ETA: 49:25 - loss: 3.9062 - regression_loss: 3.1180 - classification_loss: 0.7882
 330/3000 [==>...........................] - ETA: 49:18 - loss: 3.9009 - regression_loss: 3.1137 - classification_loss: 0.7872
 331/3000 [==>...........................] - ETA: 49:11 - loss: 3.8939 - regression_loss: 3.1076 - classification_loss: 0.7863
 332/3000 [==>...........................] - ETA: 49:05 - loss: 3.8920 - regression_loss: 3.1062 - classification_loss: 0.7858
 333/3000 [==>...........................] - ETA: 48:58 - loss: 3.8851 - regression_loss: 3.1002 - classification_loss: 0.7849
 334/3000 [==>...........................] - ETA: 48:51 - loss: 3.8805 - regression_loss: 3.0962 - classification_loss: 0.7843
 335/3000 [==>...........................] - ETA: 48:45 - loss: 3.8779 - regression_loss: 3.0943 - classification_loss: 0.7836
 336/3000 [==>...........................] - ETA: 48:38 - loss: 3.8746 - regression_loss: 3.0915 - classification_loss: 0.7831
 337/3000 [==>...........................] - ETA: 48:31 - loss: 3.8712 - regression_loss: 3.0886 - classification_loss: 0.7826
 338/3000 [==>...........................] - ETA: 48:25 - loss: 3.8655 - regression_loss: 3.0835 - classification_loss: 0.7819
 339/3000 [==>...........................] - ETA: 48:18 - loss: 3.8615 - regression_loss: 3.0804 - classification_loss: 0.7812
 340/3000 [==>...........................] - ETA: 48:12 - loss: 3.8581 - regression_loss: 3.0772 - classification_loss: 0.7809
 341/3000 [==>...........................] - ETA: 48:06 - loss: 3.8541 - regression_loss: 3.0739 - classification_loss: 0.7802
 342/3000 [==>...........................] - ETA: 47:59 - loss: 3.8496 - regression_loss: 3.0700 - classification_loss: 0.7796
 343/3000 [==>...........................] - ETA: 47:53 - loss: 3.8471 - regression_loss: 3.0682 - classification_loss: 0.7790
 344/3000 [==>...........................] - ETA: 47:47 - loss: 3.8423 - regression_loss: 3.0639 - classification_loss: 0.7784
 345/3000 [==>...........................] - ETA: 47:41 - loss: 3.8369 - regression_loss: 3.0594 - classification_loss: 0.7776
 346/3000 [==>...........................] - ETA: 47:35 - loss: 3.8349 - regression_loss: 3.0577 - classification_loss: 0.7772
 347/3000 [==>...........................] - ETA: 47:29 - loss: 3.8294 - regression_loss: 3.0531 - classification_loss: 0.7763
 348/3000 [==>...........................] - ETA: 47:22 - loss: 3.8274 - regression_loss: 3.0514 - classification_loss: 0.7759
 349/3000 [==>...........................] - ETA: 47:16 - loss: 3.8222 - regression_loss: 3.0470 - classification_loss: 0.7751
 350/3000 [==>...........................] - ETA: 47:10 - loss: 3.8186 - regression_loss: 3.0441 - classification_loss: 0.7746
 351/3000 [==>...........................] - ETA: 47:04 - loss: 3.8174 - regression_loss: 3.0434 - classification_loss: 0.7740
 352/3000 [==>...........................] - ETA: 46:58 - loss: 3.8124 - regression_loss: 3.0392 - classification_loss: 0.7732
 353/3000 [==>...........................] - ETA: 46:52 - loss: 3.8084 - regression_loss: 3.0359 - classification_loss: 0.7725
 354/3000 [==>...........................] - ETA: 46:46 - loss: 3.8102 - regression_loss: 3.0378 - classification_loss: 0.7724
 355/3000 [==>...........................] - ETA: 46:40 - loss: 3.8084 - regression_loss: 3.0365 - classification_loss: 0.7719
 356/3000 [==>...........................] - ETA: 46:34 - loss: 3.8024 - regression_loss: 3.0312 - classification_loss: 0.7712
 357/3000 [==>...........................] - ETA: 46:28 - loss: 3.7991 - regression_loss: 3.0280 - classification_loss: 0.7711
 358/3000 [==>...........................] - ETA: 46:22 - loss: 3.7955 - regression_loss: 3.0249 - classification_loss: 0.7706
 359/3000 [==>...........................] - ETA: 46:16 - loss: 3.7903 - regression_loss: 3.0204 - classification_loss: 0.7699
 360/3000 [==>...........................] - ETA: 46:11 - loss: 3.7849 - regression_loss: 3.0158 - classification_loss: 0.7691
 361/3000 [==>...........................] - ETA: 46:05 - loss: 3.7819 - regression_loss: 3.0135 - classification_loss: 0.7684
 362/3000 [==>...........................] - ETA: 45:59 - loss: 3.7790 - regression_loss: 3.0115 - classification_loss: 0.7675
 363/3000 [==>...........................] - ETA: 45:53 - loss: 3.7738 - regression_loss: 3.0070 - classification_loss: 0.7668
 364/3000 [==>...........................] - ETA: 45:48 - loss: 3.7718 - regression_loss: 3.0056 - classification_loss: 0.7662
 365/3000 [==>...........................] - ETA: 45:42 - loss: 3.7690 - regression_loss: 3.0033 - classification_loss: 0.7656
 366/3000 [==>...........................] - ETA: 45:37 - loss: 3.7661 - regression_loss: 3.0011 - classification_loss: 0.7649
 367/3000 [==>...........................] - ETA: 45:31 - loss: 3.7619 - regression_loss: 2.9974 - classification_loss: 0.7644
 368/3000 [==>...........................] - ETA: 45:26 - loss: 3.7602 - regression_loss: 2.9963 - classification_loss: 0.7640
 369/3000 [==>...........................] - ETA: 45:20 - loss: 3.7568 - regression_loss: 2.9935 - classification_loss: 0.7633
 370/3000 [==>...........................] - ETA: 45:15 - loss: 3.7536 - regression_loss: 2.9907 - classification_loss: 0.7629
 371/3000 [==>...........................] - ETA: 45:09 - loss: 3.7500 - regression_loss: 2.9876 - classification_loss: 0.7624
 372/3000 [==>...........................] - ETA: 45:04 - loss: 3.7452 - regression_loss: 2.9834 - classification_loss: 0.7619
 373/3000 [==>...........................] - ETA: 44:58 - loss: 3.7407 - regression_loss: 2.9796 - classification_loss: 0.7611
 374/3000 [==>...........................] - ETA: 44:53 - loss: 3.7394 - regression_loss: 2.9788 - classification_loss: 0.7606
 375/3000 [==>...........................] - ETA: 44:48 - loss: 3.7358 - regression_loss: 2.9759 - classification_loss: 0.7600
 376/3000 [==>...........................] - ETA: 44:42 - loss: 3.7329 - regression_loss: 2.9734 - classification_loss: 0.7594
 377/3000 [==>...........................] - ETA: 44:37 - loss: 3.7303 - regression_loss: 2.9712 - classification_loss: 0.7590
 378/3000 [==>...........................] - ETA: 44:32 - loss: 3.7276 - regression_loss: 2.9691 - classification_loss: 0.7585
 379/3000 [==>...........................] - ETA: 44:26 - loss: 3.7239 - regression_loss: 2.9660 - classification_loss: 0.7579
 380/3000 [==>...........................] - ETA: 44:21 - loss: 3.7190 - regression_loss: 2.9619 - classification_loss: 0.7571
 381/3000 [==>...........................] - ETA: 44:16 - loss: 3.7142 - regression_loss: 2.9580 - classification_loss: 0.7562
 382/3000 [==>...........................] - ETA: 44:11 - loss: 3.7102 - regression_loss: 2.9546 - classification_loss: 0.7556
 383/3000 [==>...........................] - ETA: 44:06 - loss: 3.7080 - regression_loss: 2.9529 - classification_loss: 0.7551
 384/3000 [==>...........................] - ETA: 44:01 - loss: 3.7022 - regression_loss: 2.9479 - classification_loss: 0.7543
 385/3000 [==>...........................] - ETA: 43:56 - loss: 3.6978 - regression_loss: 2.9441 - classification_loss: 0.7537
 386/3000 [==>...........................] - ETA: 43:51 - loss: 3.6942 - regression_loss: 2.9412 - classification_loss: 0.7530
 387/3000 [==>...........................] - ETA: 43:46 - loss: 3.6897 - regression_loss: 2.9373 - classification_loss: 0.7524
 388/3000 [==>...........................] - ETA: 43:41 - loss: 3.6848 - regression_loss: 2.9331 - classification_loss: 0.7517
 389/3000 [==>...........................] - ETA: 43:36 - loss: 3.6803 - regression_loss: 2.9291 - classification_loss: 0.7511
 390/3000 [==>...........................] - ETA: 43:31 - loss: 3.6799 - regression_loss: 2.9290 - classification_loss: 0.7509
 391/3000 [==>...........................] - ETA: 43:26 - loss: 3.6758 - regression_loss: 2.9257 - classification_loss: 0.7501
 392/3000 [==>...........................] - ETA: 43:21 - loss: 3.6724 - regression_loss: 2.9229 - classification_loss: 0.7495
 393/3000 [==>...........................] - ETA: 43:16 - loss: 3.6704 - regression_loss: 2.9214 - classification_loss: 0.7490
 394/3000 [==>...........................] - ETA: 43:11 - loss: 3.6673 - regression_loss: 2.9191 - classification_loss: 0.7482
 395/3000 [==>...........................] - ETA: 43:06 - loss: 3.6649 - regression_loss: 2.9172 - classification_loss: 0.7476
 396/3000 [==>...........................] - ETA: 43:01 - loss: 3.6645 - regression_loss: 2.9171 - classification_loss: 0.7474
 397/3000 [==>...........................] - ETA: 42:57 - loss: 3.6600 - regression_loss: 2.9131 - classification_loss: 0.7469
 398/3000 [==>...........................] - ETA: 42:52 - loss: 3.6563 - regression_loss: 2.9099 - classification_loss: 0.7464
 399/3000 [==>...........................] - ETA: 42:47 - loss: 3.6520 - regression_loss: 2.9061 - classification_loss: 0.7459
 400/3000 [===>..........................] - ETA: 42:43 - loss: 3.6491 - regression_loss: 2.9035 - classification_loss: 0.7456
 401/3000 [===>..........................] - ETA: 42:38 - loss: 3.6468 - regression_loss: 2.9017 - classification_loss: 0.7451
 402/3000 [===>..........................] - ETA: 42:33 - loss: 3.6447 - regression_loss: 2.9000 - classification_loss: 0.7447
 403/3000 [===>..........................] - ETA: 42:28 - loss: 3.6406 - regression_loss: 2.8965 - classification_loss: 0.7441
 404/3000 [===>..........................] - ETA: 42:24 - loss: 3.6390 - regression_loss: 2.8952 - classification_loss: 0.7438
 405/3000 [===>..........................] - ETA: 42:19 - loss: 3.6362 - regression_loss: 2.8930 - classification_loss: 0.7432
 406/3000 [===>..........................] - ETA: 42:14 - loss: 3.6343 - regression_loss: 2.8915 - classification_loss: 0.7427
 407/3000 [===>..........................] - ETA: 42:10 - loss: 3.6300 - regression_loss: 2.8879 - classification_loss: 0.7421
 408/3000 [===>..........................] - ETA: 42:05 - loss: 3.6262 - regression_loss: 2.8844 - classification_loss: 0.7418
 409/3000 [===>..........................] - ETA: 42:01 - loss: 3.6220 - regression_loss: 2.8809 - classification_loss: 0.7411
 410/3000 [===>..........................] - ETA: 41:56 - loss: 3.6198 - regression_loss: 2.8790 - classification_loss: 0.7408
 411/3000 [===>..........................] - ETA: 41:51 - loss: 3.6190 - regression_loss: 2.8781 - classification_loss: 0.7408
 412/3000 [===>..........................] - ETA: 41:47 - loss: 3.6150 - regression_loss: 2.8745 - classification_loss: 0.7404
 413/3000 [===>..........................] - ETA: 41:42 - loss: 3.6124 - regression_loss: 2.8725 - classification_loss: 0.7399
 414/3000 [===>..........................] - ETA: 41:38 - loss: 3.6109 - regression_loss: 2.8715 - classification_loss: 0.7394
 415/3000 [===>..........................] - ETA: 41:34 - loss: 3.6072 - regression_loss: 2.8683 - classification_loss: 0.7390
 416/3000 [===>..........................] - ETA: 41:29 - loss: 3.6052 - regression_loss: 2.8667 - classification_loss: 0.7385
 417/3000 [===>..........................] - ETA: 41:25 - loss: 3.6016 - regression_loss: 2.8636 - classification_loss: 0.7380
 418/3000 [===>..........................] - ETA: 41:21 - loss: 3.5974 - regression_loss: 2.8600 - classification_loss: 0.7375
 419/3000 [===>..........................] - ETA: 41:16 - loss: 3.5936 - regression_loss: 2.8566 - classification_loss: 0.7370
 420/3000 [===>..........................] - ETA: 41:12 - loss: 3.5901 - regression_loss: 2.8533 - classification_loss: 0.7368
 421/3000 [===>..........................] - ETA: 41:08 - loss: 3.5863 - regression_loss: 2.8498 - classification_loss: 0.7365
 422/3000 [===>..........................] - ETA: 41:03 - loss: 3.5856 - regression_loss: 2.8493 - classification_loss: 0.7364
 423/3000 [===>..........................] - ETA: 40:59 - loss: 3.5835 - regression_loss: 2.8474 - classification_loss: 0.7360
 424/3000 [===>..........................] - ETA: 40:55 - loss: 3.5791 - regression_loss: 2.8436 - classification_loss: 0.7355
 425/3000 [===>..........................] - ETA: 40:51 - loss: 3.5748 - regression_loss: 2.8399 - classification_loss: 0.7349
 426/3000 [===>..........................] - ETA: 40:46 - loss: 3.5701 - regression_loss: 2.8357 - classification_loss: 0.7344
 427/3000 [===>..........................] - ETA: 40:42 - loss: 3.5679 - regression_loss: 2.8339 - classification_loss: 0.7340
 428/3000 [===>..........................] - ETA: 40:38 - loss: 3.5658 - regression_loss: 2.8323 - classification_loss: 0.7335
 429/3000 [===>..........................] - ETA: 40:34 - loss: 3.5618 - regression_loss: 2.8288 - classification_loss: 0.7330
 430/3000 [===>..........................] - ETA: 40:30 - loss: 3.5590 - regression_loss: 2.8266 - classification_loss: 0.7324
 431/3000 [===>..........................] - ETA: 40:26 - loss: 3.5581 - regression_loss: 2.8260 - classification_loss: 0.7321
 432/3000 [===>..........................] - ETA: 40:21 - loss: 3.5551 - regression_loss: 2.8235 - classification_loss: 0.7316
 433/3000 [===>..........................] - ETA: 40:17 - loss: 3.5513 - regression_loss: 2.8201 - classification_loss: 0.7313
 434/3000 [===>..........................] - ETA: 40:13 - loss: 3.5500 - regression_loss: 2.8191 - classification_loss: 0.7309
 435/3000 [===>..........................] - ETA: 40:09 - loss: 3.5455 - regression_loss: 2.8153 - classification_loss: 0.7302
 436/3000 [===>..........................] - ETA: 40:05 - loss: 3.5414 - regression_loss: 2.8118 - classification_loss: 0.7296
 437/3000 [===>..........................] - ETA: 40:01 - loss: 3.5371 - regression_loss: 2.8081 - classification_loss: 0.7290
 438/3000 [===>..........................] - ETA: 39:57 - loss: 3.5334 - regression_loss: 2.8049 - classification_loss: 0.7285
 439/3000 [===>..........................] - ETA: 39:53 - loss: 3.5321 - regression_loss: 2.8034 - classification_loss: 0.7288
 440/3000 [===>..........................] - ETA: 39:49 - loss: 3.5283 - regression_loss: 2.8001 - classification_loss: 0.7282
 441/3000 [===>..........................] - ETA: 39:45 - loss: 3.5273 - regression_loss: 2.7995 - classification_loss: 0.7278
 442/3000 [===>..........................] - ETA: 39:41 - loss: 3.5244 - regression_loss: 2.7969 - classification_loss: 0.7275
 443/3000 [===>..........................] - ETA: 39:37 - loss: 3.5226 - regression_loss: 2.7954 - classification_loss: 0.7271
 444/3000 [===>..........................] - ETA: 39:33 - loss: 3.5202 - regression_loss: 2.7933 - classification_loss: 0.7269
 445/3000 [===>..........................] - ETA: 39:29 - loss: 3.5172 - regression_loss: 2.7907 - classification_loss: 0.7265
 446/3000 [===>..........................] - ETA: 39:26 - loss: 3.5158 - regression_loss: 2.7896 - classification_loss: 0.7262
 447/3000 [===>..........................] - ETA: 39:22 - loss: 3.5118 - regression_loss: 2.7860 - classification_loss: 0.7258
 448/3000 [===>..........................] - ETA: 39:18 - loss: 3.5077 - regression_loss: 2.7823 - classification_loss: 0.7254
 449/3000 [===>..........................] - ETA: 39:14 - loss: 3.5036 - regression_loss: 2.7786 - classification_loss: 0.7250
 450/3000 [===>..........................] - ETA: 39:10 - loss: 3.5038 - regression_loss: 2.7791 - classification_loss: 0.7247
 451/3000 [===>..........................] - ETA: 39:07 - loss: 3.5021 - regression_loss: 2.7779 - classification_loss: 0.7243
 452/3000 [===>..........................] - ETA: 39:03 - loss: 3.4982 - regression_loss: 2.7745 - classification_loss: 0.7237
 453/3000 [===>..........................] - ETA: 38:59 - loss: 3.4951 - regression_loss: 2.7717 - classification_loss: 0.7234
 454/3000 [===>..........................] - ETA: 38:55 - loss: 3.4946 - regression_loss: 2.7715 - classification_loss: 0.7230
 455/3000 [===>..........................] - ETA: 38:51 - loss: 3.4916 - regression_loss: 2.7691 - classification_loss: 0.7225
 456/3000 [===>..........................] - ETA: 38:48 - loss: 3.4881 - regression_loss: 2.7659 - classification_loss: 0.7222
 457/3000 [===>..........................] - ETA: 38:44 - loss: 3.4853 - regression_loss: 2.7634 - classification_loss: 0.7219
 458/3000 [===>..........................] - ETA: 38:40 - loss: 3.4833 - regression_loss: 2.7618 - classification_loss: 0.7216
 459/3000 [===>..........................] - ETA: 38:37 - loss: 3.4792 - regression_loss: 2.7581 - classification_loss: 0.7211
 460/3000 [===>..........................] - ETA: 38:33 - loss: 3.4753 - regression_loss: 2.7547 - classification_loss: 0.7206
 461/3000 [===>..........................] - ETA: 38:29 - loss: 3.4711 - regression_loss: 2.7509 - classification_loss: 0.7202
 462/3000 [===>..........................] - ETA: 38:26 - loss: 3.4689 - regression_loss: 2.7492 - classification_loss: 0.7197
 463/3000 [===>..........................] - ETA: 38:22 - loss: 3.4647 - regression_loss: 2.7456 - classification_loss: 0.7190
 464/3000 [===>..........................] - ETA: 38:18 - loss: 3.4637 - regression_loss: 2.7450 - classification_loss: 0.7187
 465/3000 [===>..........................] - ETA: 38:15 - loss: 3.4618 - regression_loss: 2.7434 - classification_loss: 0.7184
 466/3000 [===>..........................] - ETA: 38:11 - loss: 3.4580 - regression_loss: 2.7402 - classification_loss: 0.7178
 467/3000 [===>..........................] - ETA: 38:07 - loss: 3.4554 - regression_loss: 2.7381 - classification_loss: 0.7173
 468/3000 [===>..........................] - ETA: 38:04 - loss: 3.4517 - regression_loss: 2.7348 - classification_loss: 0.7169
 469/3000 [===>..........................] - ETA: 38:00 - loss: 3.4497 - regression_loss: 2.7329 - classification_loss: 0.7168
 470/3000 [===>..........................] - ETA: 37:57 - loss: 3.4468 - regression_loss: 2.7305 - classification_loss: 0.7163
 471/3000 [===>..........................] - ETA: 37:53 - loss: 3.4453 - regression_loss: 2.7294 - classification_loss: 0.7159
 472/3000 [===>..........................] - ETA: 37:50 - loss: 3.4417 - regression_loss: 2.7263 - classification_loss: 0.7154
 473/3000 [===>..........................] - ETA: 37:46 - loss: 3.4391 - regression_loss: 2.7242 - classification_loss: 0.7149
 474/3000 [===>..........................] - ETA: 37:42 - loss: 3.4352 - regression_loss: 2.7208 - classification_loss: 0.7144
 475/3000 [===>..........................] - ETA: 37:39 - loss: 3.4355 - regression_loss: 2.7212 - classification_loss: 0.7143
 476/3000 [===>..........................] - ETA: 37:36 - loss: 3.4313 - regression_loss: 2.7176 - classification_loss: 0.7138
 477/3000 [===>..........................] - ETA: 37:32 - loss: 3.4281 - regression_loss: 2.7147 - classification_loss: 0.7133
 478/3000 [===>..........................] - ETA: 37:29 - loss: 3.4258 - regression_loss: 2.7129 - classification_loss: 0.7129
 479/3000 [===>..........................] - ETA: 37:25 - loss: 3.4216 - regression_loss: 2.7092 - classification_loss: 0.7123
 480/3000 [===>..........................] - ETA: 37:22 - loss: 3.4205 - regression_loss: 2.7085 - classification_loss: 0.7119
 481/3000 [===>..........................] - ETA: 37:18 - loss: 3.4174 - regression_loss: 2.7061 - classification_loss: 0.7114
 482/3000 [===>..........................] - ETA: 37:15 - loss: 3.4147 - regression_loss: 2.7038 - classification_loss: 0.7109
 483/3000 [===>..........................] - ETA: 37:12 - loss: 3.4119 - regression_loss: 2.7014 - classification_loss: 0.7104
 484/3000 [===>..........................] - ETA: 37:08 - loss: 3.4083 - regression_loss: 2.6983 - classification_loss: 0.7099
 485/3000 [===>..........................] - ETA: 37:05 - loss: 3.4070 - regression_loss: 2.6975 - classification_loss: 0.7095
 486/3000 [===>..........................] - ETA: 37:01 - loss: 3.4051 - regression_loss: 2.6961 - classification_loss: 0.7090
 487/3000 [===>..........................] - ETA: 36:58 - loss: 3.4020 - regression_loss: 2.6933 - classification_loss: 0.7087
 488/3000 [===>..........................] - ETA: 36:55 - loss: 3.4010 - regression_loss: 2.6926 - classification_loss: 0.7085
 489/3000 [===>..........................] - ETA: 36:51 - loss: 3.3985 - regression_loss: 2.6905 - classification_loss: 0.7080
 490/3000 [===>..........................] - ETA: 36:48 - loss: 3.3947 - regression_loss: 2.6871 - classification_loss: 0.7076
 491/3000 [===>..........................] - ETA: 36:45 - loss: 3.3922 - regression_loss: 2.6848 - classification_loss: 0.7075
 492/3000 [===>..........................] - ETA: 36:42 - loss: 3.3912 - regression_loss: 2.6841 - classification_loss: 0.7071
 493/3000 [===>..........................] - ETA: 36:38 - loss: 3.3884 - regression_loss: 2.6818 - classification_loss: 0.7066
 494/3000 [===>..........................] - ETA: 36:35 - loss: 3.3847 - regression_loss: 2.6785 - classification_loss: 0.7062
 495/3000 [===>..........................] - ETA: 36:32 - loss: 3.3809 - regression_loss: 2.6749 - classification_loss: 0.7060
 496/3000 [===>..........................] - ETA: 36:29 - loss: 3.3791 - regression_loss: 2.6733 - classification_loss: 0.7057
 497/3000 [===>..........................] - ETA: 36:26 - loss: 3.3770 - regression_loss: 2.6717 - classification_loss: 0.7053
 498/3000 [===>..........................] - ETA: 36:22 - loss: 3.3744 - regression_loss: 2.6693 - classification_loss: 0.7050
 499/3000 [===>..........................] - ETA: 36:19 - loss: 3.3713 - regression_loss: 2.6666 - classification_loss: 0.7047
 500/3000 [====>.........................] - ETA: 36:16 - loss: 3.3694 - regression_loss: 2.6650 - classification_loss: 0.7044
 501/3000 [====>.........................] - ETA: 36:13 - loss: 3.3652 - regression_loss: 2.6614 - classification_loss: 0.7038
 502/3000 [====>.........................] - ETA: 36:09 - loss: 3.3631 - regression_loss: 2.6595 - classification_loss: 0.7036
 503/3000 [====>.........................] - ETA: 36:06 - loss: 3.3614 - regression_loss: 2.6581 - classification_loss: 0.7033
 504/3000 [====>.........................] - ETA: 36:03 - loss: 3.3588 - regression_loss: 2.6558 - classification_loss: 0.7030
 505/3000 [====>.........................] - ETA: 36:00 - loss: 3.3557 - regression_loss: 2.6532 - classification_loss: 0.7025
 506/3000 [====>.........................] - ETA: 35:57 - loss: 3.3541 - regression_loss: 2.6518 - classification_loss: 0.7023
 507/3000 [====>.........................] - ETA: 35:54 - loss: 3.3507 - regression_loss: 2.6489 - classification_loss: 0.7019
 508/3000 [====>.........................] - ETA: 35:51 - loss: 3.3488 - regression_loss: 2.6474 - classification_loss: 0.7014
 509/3000 [====>.........................] - ETA: 35:48 - loss: 3.3468 - regression_loss: 2.6457 - classification_loss: 0.7011
 510/3000 [====>.........................] - ETA: 35:45 - loss: 3.3453 - regression_loss: 2.6445 - classification_loss: 0.7007
 511/3000 [====>.........................] - ETA: 35:41 - loss: 3.3423 - regression_loss: 2.6420 - classification_loss: 0.7002
 512/3000 [====>.........................] - ETA: 35:38 - loss: 3.3390 - regression_loss: 2.6394 - classification_loss: 0.6996
 513/3000 [====>.........................] - ETA: 35:35 - loss: 3.3357 - regression_loss: 2.6367 - classification_loss: 0.6990
 514/3000 [====>.........................] - ETA: 35:32 - loss: 3.3343 - regression_loss: 2.6356 - classification_loss: 0.6987
 515/3000 [====>.........................] - ETA: 35:29 - loss: 3.3308 - regression_loss: 2.6325 - classification_loss: 0.6983
 516/3000 [====>.........................] - ETA: 35:26 - loss: 3.3270 - regression_loss: 2.6295 - classification_loss: 0.6976
 517/3000 [====>.........................] - ETA: 35:23 - loss: 3.3238 - regression_loss: 2.6268 - classification_loss: 0.6970
 518/3000 [====>.........................] - ETA: 35:20 - loss: 3.3210 - regression_loss: 2.6246 - classification_loss: 0.6964
 519/3000 [====>.........................] - ETA: 35:17 - loss: 3.3190 - regression_loss: 2.6229 - classification_loss: 0.6961
 520/3000 [====>.........................] - ETA: 35:14 - loss: 3.3158 - regression_loss: 2.6201 - classification_loss: 0.6957
 521/3000 [====>.........................] - ETA: 35:11 - loss: 3.3135 - regression_loss: 2.6182 - classification_loss: 0.6953
 522/3000 [====>.........................] - ETA: 35:08 - loss: 3.3110 - regression_loss: 2.6161 - classification_loss: 0.6950
 523/3000 [====>.........................] - ETA: 35:06 - loss: 3.3088 - regression_loss: 2.6142 - classification_loss: 0.6946
 524/3000 [====>.........................] - ETA: 35:03 - loss: 3.3060 - regression_loss: 2.6117 - classification_loss: 0.6943
 525/3000 [====>.........................] - ETA: 35:00 - loss: 3.3031 - regression_loss: 2.6094 - classification_loss: 0.6938
 526/3000 [====>.........................] - ETA: 34:57 - loss: 3.3020 - regression_loss: 2.6083 - classification_loss: 0.6938
 527/3000 [====>.........................] - ETA: 34:54 - loss: 3.3001 - regression_loss: 2.6067 - classification_loss: 0.6935
 528/3000 [====>.........................] - ETA: 34:51 - loss: 3.2971 - regression_loss: 2.6042 - classification_loss: 0.6929
 529/3000 [====>.........................] - ETA: 34:48 - loss: 3.2942 - regression_loss: 2.6017 - classification_loss: 0.6925
 530/3000 [====>.........................] - ETA: 34:45 - loss: 3.2913 - regression_loss: 2.5991 - classification_loss: 0.6921
 531/3000 [====>.........................] - ETA: 34:42 - loss: 3.2890 - regression_loss: 2.5972 - classification_loss: 0.6919
 532/3000 [====>.........................] - ETA: 34:40 - loss: 3.2865 - regression_loss: 2.5949 - classification_loss: 0.6916
 533/3000 [====>.........................] - ETA: 34:37 - loss: 3.2842 - regression_loss: 2.5928 - classification_loss: 0.6914
 534/3000 [====>.........................] - ETA: 34:34 - loss: 3.2811 - regression_loss: 2.5903 - classification_loss: 0.6909
 535/3000 [====>.........................] - ETA: 34:31 - loss: 3.2776 - regression_loss: 2.5871 - classification_loss: 0.6904
 536/3000 [====>.........................] - ETA: 34:28 - loss: 3.2767 - regression_loss: 2.5865 - classification_loss: 0.6901
 537/3000 [====>.........................] - ETA: 34:25 - loss: 3.2742 - regression_loss: 2.5843 - classification_loss: 0.6899
 538/3000 [====>.........................] - ETA: 34:23 - loss: 3.2716 - regression_loss: 2.5820 - classification_loss: 0.6896
 539/3000 [====>.........................] - ETA: 34:20 - loss: 3.2684 - regression_loss: 2.5792 - classification_loss: 0.6892
 540/3000 [====>.........................] - ETA: 34:17 - loss: 3.2658 - regression_loss: 2.5770 - classification_loss: 0.6888
 541/3000 [====>.........................] - ETA: 34:14 - loss: 3.2638 - regression_loss: 2.5754 - classification_loss: 0.6884
 542/3000 [====>.........................] - ETA: 34:11 - loss: 3.2609 - regression_loss: 2.5729 - classification_loss: 0.6880
 543/3000 [====>.........................] - ETA: 34:09 - loss: 3.2596 - regression_loss: 2.5718 - classification_loss: 0.6877
 544/3000 [====>.........................] - ETA: 34:06 - loss: 3.2566 - regression_loss: 2.5693 - classification_loss: 0.6873
 545/3000 [====>.........................] - ETA: 34:03 - loss: 3.2549 - regression_loss: 2.5679 - classification_loss: 0.6870
 546/3000 [====>.........................] - ETA: 34:01 - loss: 3.2529 - regression_loss: 2.5663 - classification_loss: 0.6866
 547/3000 [====>.........................] - ETA: 33:58 - loss: 3.2513 - regression_loss: 2.5650 - classification_loss: 0.6862
 548/3000 [====>.........................] - ETA: 33:55 - loss: 3.2488 - regression_loss: 2.5629 - classification_loss: 0.6859
 549/3000 [====>.........................] - ETA: 33:53 - loss: 3.2477 - regression_loss: 2.5622 - classification_loss: 0.6856
 550/3000 [====>.........................] - ETA: 33:50 - loss: 3.2440 - regression_loss: 2.5588 - classification_loss: 0.6852
 551/3000 [====>.........................] - ETA: 33:47 - loss: 3.2414 - regression_loss: 2.5566 - classification_loss: 0.6848
 552/3000 [====>.........................] - ETA: 33:44 - loss: 3.2398 - regression_loss: 2.5552 - classification_loss: 0.6846
 553/3000 [====>.........................] - ETA: 33:42 - loss: 3.2362 - regression_loss: 2.5520 - classification_loss: 0.6842
 554/3000 [====>.........................] - ETA: 33:39 - loss: 3.2332 - regression_loss: 2.5494 - classification_loss: 0.6838
 555/3000 [====>.........................] - ETA: 33:36 - loss: 3.2311 - regression_loss: 2.5478 - classification_loss: 0.6833
 556/3000 [====>.........................] - ETA: 33:34 - loss: 3.2281 - regression_loss: 2.5452 - classification_loss: 0.6829
 557/3000 [====>.........................] - ETA: 33:31 - loss: 3.2251 - regression_loss: 2.5426 - classification_loss: 0.6826
 558/3000 [====>.........................] - ETA: 33:28 - loss: 3.2227 - regression_loss: 2.5406 - classification_loss: 0.6821
 559/3000 [====>.........................] - ETA: 33:26 - loss: 3.2197 - regression_loss: 2.5379 - classification_loss: 0.6818
 560/3000 [====>.........................] - ETA: 33:23 - loss: 3.2169 - regression_loss: 2.5356 - classification_loss: 0.6813
 561/3000 [====>.........................] - ETA: 33:21 - loss: 3.2135 - regression_loss: 2.5328 - classification_loss: 0.6808
 562/3000 [====>.........................] - ETA: 33:18 - loss: 3.2121 - regression_loss: 2.5315 - classification_loss: 0.6806
 563/3000 [====>.........................] - ETA: 33:15 - loss: 3.2102 - regression_loss: 2.5297 - classification_loss: 0.6805
 564/3000 [====>.........................] - ETA: 33:13 - loss: 3.2088 - regression_loss: 2.5284 - classification_loss: 0.6804
 565/3000 [====>.........................] - ETA: 33:10 - loss: 3.2065 - regression_loss: 2.5263 - classification_loss: 0.6802
 566/3000 [====>.........................] - ETA: 33:08 - loss: 3.2046 - regression_loss: 2.5249 - classification_loss: 0.6798
 567/3000 [====>.........................] - ETA: 33:05 - loss: 3.2023 - regression_loss: 2.5227 - classification_loss: 0.6796
 568/3000 [====>.........................] - ETA: 33:03 - loss: 3.2001 - regression_loss: 2.5207 - classification_loss: 0.6794
 569/3000 [====>.........................] - ETA: 33:00 - loss: 3.2005 - regression_loss: 2.5210 - classification_loss: 0.6794
 570/3000 [====>.........................] - ETA: 32:58 - loss: 3.1979 - regression_loss: 2.5187 - classification_loss: 0.6792
 571/3000 [====>.........................] - ETA: 32:55 - loss: 3.1949 - regression_loss: 2.5161 - classification_loss: 0.6788
 572/3000 [====>.........................] - ETA: 32:52 - loss: 3.1936 - regression_loss: 2.5150 - classification_loss: 0.6786
 573/3000 [====>.........................] - ETA: 32:50 - loss: 3.1912 - regression_loss: 2.5129 - classification_loss: 0.6783
 574/3000 [====>.........................] - ETA: 32:48 - loss: 3.1882 - regression_loss: 2.5103 - classification_loss: 0.6778
 575/3000 [====>.........................] - ETA: 32:45 - loss: 3.1856 - regression_loss: 2.5082 - classification_loss: 0.6774
 576/3000 [====>.........................] - ETA: 32:43 - loss: 3.1838 - regression_loss: 2.5067 - classification_loss: 0.6771
 577/3000 [====>.........................] - ETA: 32:40 - loss: 3.1817 - regression_loss: 2.5049 - classification_loss: 0.6768
 578/3000 [====>.........................] - ETA: 32:38 - loss: 3.1785 - regression_loss: 2.5021 - classification_loss: 0.6765
 579/3000 [====>.........................] - ETA: 32:35 - loss: 3.1755 - regression_loss: 2.4994 - classification_loss: 0.6761
 580/3000 [====>.........................] - ETA: 32:33 - loss: 3.1732 - regression_loss: 2.4974 - classification_loss: 0.6758
 581/3000 [====>.........................] - ETA: 32:30 - loss: 3.1706 - regression_loss: 2.4951 - classification_loss: 0.6754
 582/3000 [====>.........................] - ETA: 32:28 - loss: 3.1691 - regression_loss: 2.4936 - classification_loss: 0.6755
 583/3000 [====>.........................] - ETA: 32:25 - loss: 3.1674 - regression_loss: 2.4922 - classification_loss: 0.6752
 584/3000 [====>.........................] - ETA: 32:23 - loss: 3.1649 - regression_loss: 2.4901 - classification_loss: 0.6748
 585/3000 [====>.........................] - ETA: 32:20 - loss: 3.1624 - regression_loss: 2.4880 - classification_loss: 0.6744
 586/3000 [====>.........................] - ETA: 32:18 - loss: 3.1607 - regression_loss: 2.4867 - classification_loss: 0.6740
 587/3000 [====>.........................] - ETA: 32:16 - loss: 3.1578 - regression_loss: 2.4842 - classification_loss: 0.6736
 588/3000 [====>.........................] - ETA: 32:13 - loss: 3.1559 - regression_loss: 2.4828 - classification_loss: 0.6732
 589/3000 [====>.........................] - ETA: 32:11 - loss: 3.1549 - regression_loss: 2.4820 - classification_loss: 0.6729
 590/3000 [====>.........................] - ETA: 32:08 - loss: 3.1552 - regression_loss: 2.4823 - classification_loss: 0.6729
 591/3000 [====>.........................] - ETA: 32:06 - loss: 3.1537 - regression_loss: 2.4810 - classification_loss: 0.6727
 592/3000 [====>.........................] - ETA: 32:04 - loss: 3.1517 - regression_loss: 2.4794 - classification_loss: 0.6723
 593/3000 [====>.........................] - ETA: 32:01 - loss: 3.1499 - regression_loss: 2.4778 - classification_loss: 0.6721
 594/3000 [====>.........................] - ETA: 31:59 - loss: 3.1479 - regression_loss: 2.4762 - classification_loss: 0.6717
 595/3000 [====>.........................] - ETA: 31:57 - loss: 3.1451 - regression_loss: 2.4737 - classification_loss: 0.6714
 596/3000 [====>.........................] - ETA: 31:54 - loss: 3.1423 - regression_loss: 2.4712 - classification_loss: 0.6711
 597/3000 [====>.........................] - ETA: 31:52 - loss: 3.1390 - regression_loss: 2.4683 - classification_loss: 0.6707
 598/3000 [====>.........................] - ETA: 31:50 - loss: 3.1367 - regression_loss: 2.4666 - classification_loss: 0.6702
 599/3000 [====>.........................] - ETA: 31:47 - loss: 3.1338 - regression_loss: 2.4638 - classification_loss: 0.6700
 600/3000 [=====>........................] - ETA: 31:45 - loss: 3.1310 - regression_loss: 2.4614 - classification_loss: 0.6696
 601/3000 [=====>........................] - ETA: 31:43 - loss: 3.1291 - regression_loss: 2.4599 - classification_loss: 0.6692
 602/3000 [=====>........................] - ETA: 31:40 - loss: 3.1270 - regression_loss: 2.4582 - classification_loss: 0.6688
 603/3000 [=====>........................] - ETA: 31:38 - loss: 3.1245 - regression_loss: 2.4562 - classification_loss: 0.6683
 604/3000 [=====>........................] - ETA: 31:36 - loss: 3.1212 - regression_loss: 2.4535 - classification_loss: 0.6677
 605/3000 [=====>........................] - ETA: 31:34 - loss: 3.1198 - regression_loss: 2.4524 - classification_loss: 0.6674
 606/3000 [=====>........................] - ETA: 31:31 - loss: 3.1186 - regression_loss: 2.4514 - classification_loss: 0.6672
 607/3000 [=====>........................] - ETA: 31:29 - loss: 3.1167 - regression_loss: 2.4499 - classification_loss: 0.6668
 608/3000 [=====>........................] - ETA: 31:27 - loss: 3.1146 - regression_loss: 2.4482 - classification_loss: 0.6664
 609/3000 [=====>........................] - ETA: 31:24 - loss: 3.1123 - regression_loss: 2.4462 - classification_loss: 0.6662
 610/3000 [=====>........................] - ETA: 31:22 - loss: 3.1099 - regression_loss: 2.4440 - classification_loss: 0.6659
 611/3000 [=====>........................] - ETA: 31:20 - loss: 3.1066 - regression_loss: 2.4412 - classification_loss: 0.6654
 612/3000 [=====>........................] - ETA: 31:18 - loss: 3.1057 - regression_loss: 2.4405 - classification_loss: 0.6652
 613/3000 [=====>........................] - ETA: 31:15 - loss: 3.1038 - regression_loss: 2.4389 - classification_loss: 0.6649
 614/3000 [=====>........................] - ETA: 31:13 - loss: 3.1010 - regression_loss: 2.4365 - classification_loss: 0.6645
 615/3000 [=====>........................] - ETA: 31:11 - loss: 3.1006 - regression_loss: 2.4362 - classification_loss: 0.6643
 616/3000 [=====>........................] - ETA: 31:09 - loss: 3.0995 - regression_loss: 2.4355 - classification_loss: 0.6640
 617/3000 [=====>........................] - ETA: 31:06 - loss: 3.0966 - regression_loss: 2.4331 - classification_loss: 0.6635
 618/3000 [=====>........................] - ETA: 31:04 - loss: 3.0942 - regression_loss: 2.4311 - classification_loss: 0.6631
 619/3000 [=====>........................] - ETA: 31:02 - loss: 3.0928 - regression_loss: 2.4297 - classification_loss: 0.6631
 620/3000 [=====>........................] - ETA: 31:00 - loss: 3.0917 - regression_loss: 2.4288 - classification_loss: 0.6629
 621/3000 [=====>........................] - ETA: 30:57 - loss: 3.0894 - regression_loss: 2.4268 - classification_loss: 0.6626
 622/3000 [=====>........................] - ETA: 30:55 - loss: 3.0870 - regression_loss: 2.4247 - classification_loss: 0.6622
 623/3000 [=====>........................] - ETA: 30:53 - loss: 3.0845 - regression_loss: 2.4227 - classification_loss: 0.6618
 624/3000 [=====>........................] - ETA: 30:51 - loss: 3.0819 - regression_loss: 2.4202 - classification_loss: 0.6617
 625/3000 [=====>........................] - ETA: 30:49 - loss: 3.0799 - regression_loss: 2.4185 - classification_loss: 0.6613
 626/3000 [=====>........................] - ETA: 30:47 - loss: 3.0773 - regression_loss: 2.4160 - classification_loss: 0.6613
 627/3000 [=====>........................] - ETA: 30:44 - loss: 3.0747 - regression_loss: 2.4139 - classification_loss: 0.6608
 628/3000 [=====>........................] - ETA: 30:42 - loss: 3.0720 - regression_loss: 2.4115 - classification_loss: 0.6605
 629/3000 [=====>........................] - ETA: 30:40 - loss: 3.0692 - regression_loss: 2.4093 - classification_loss: 0.6599
 630/3000 [=====>........................] - ETA: 30:38 - loss: 3.0679 - regression_loss: 2.4083 - classification_loss: 0.6596
 631/3000 [=====>........................] - ETA: 30:36 - loss: 3.0663 - regression_loss: 2.4069 - classification_loss: 0.6594
 632/3000 [=====>........................] - ETA: 30:34 - loss: 3.0645 - regression_loss: 2.4055 - classification_loss: 0.6590
 633/3000 [=====>........................] - ETA: 30:32 - loss: 3.0631 - regression_loss: 2.4043 - classification_loss: 0.6588
 634/3000 [=====>........................] - ETA: 30:29 - loss: 3.0610 - regression_loss: 2.4025 - classification_loss: 0.6584
 635/3000 [=====>........................] - ETA: 30:27 - loss: 3.0597 - regression_loss: 2.4016 - classification_loss: 0.6582
 636/3000 [=====>........................] - ETA: 30:25 - loss: 3.0573 - regression_loss: 2.3995 - classification_loss: 0.6578
 637/3000 [=====>........................] - ETA: 30:23 - loss: 3.0570 - regression_loss: 2.3994 - classification_loss: 0.6576
 638/3000 [=====>........................] - ETA: 30:21 - loss: 3.0550 - regression_loss: 2.3976 - classification_loss: 0.6573
 639/3000 [=====>........................] - ETA: 30:19 - loss: 3.0526 - regression_loss: 2.3955 - classification_loss: 0.6571
 640/3000 [=====>........................] - ETA: 30:16 - loss: 3.0506 - regression_loss: 2.3939 - classification_loss: 0.6567
 641/3000 [=====>........................] - ETA: 30:14 - loss: 3.0496 - regression_loss: 2.3932 - classification_loss: 0.6564
 642/3000 [=====>........................] - ETA: 30:12 - loss: 3.0473 - regression_loss: 2.3911 - classification_loss: 0.6562
 643/3000 [=====>........................] - ETA: 30:10 - loss: 3.0462 - regression_loss: 2.3902 - classification_loss: 0.6561
 644/3000 [=====>........................] - ETA: 30:08 - loss: 3.0438 - regression_loss: 2.3882 - classification_loss: 0.6556
 645/3000 [=====>........................] - ETA: 30:06 - loss: 3.0416 - regression_loss: 2.3863 - classification_loss: 0.6553
 646/3000 [=====>........................] - ETA: 30:04 - loss: 3.0390 - regression_loss: 2.3841 - classification_loss: 0.6548
 647/3000 [=====>........................] - ETA: 30:02 - loss: 3.0367 - regression_loss: 2.3823 - classification_loss: 0.6544
 648/3000 [=====>........................] - ETA: 30:00 - loss: 3.0347 - regression_loss: 2.3805 - classification_loss: 0.6542
 649/3000 [=====>........................] - ETA: 29:58 - loss: 3.0334 - regression_loss: 2.3794 - classification_loss: 0.6540
 650/3000 [=====>........................] - ETA: 29:56 - loss: 3.0317 - regression_loss: 2.3778 - classification_loss: 0.6539
 651/3000 [=====>........................] - ETA: 29:54 - loss: 3.0300 - regression_loss: 2.3764 - classification_loss: 0.6535
 652/3000 [=====>........................] - ETA: 29:52 - loss: 3.0279 - regression_loss: 2.3747 - classification_loss: 0.6532
 653/3000 [=====>........................] - ETA: 29:50 - loss: 3.0256 - regression_loss: 2.3727 - classification_loss: 0.6529
 654/3000 [=====>........................] - ETA: 29:48 - loss: 3.0236 - regression_loss: 2.3709 - classification_loss: 0.6527
 655/3000 [=====>........................] - ETA: 29:46 - loss: 3.0212 - regression_loss: 2.3688 - classification_loss: 0.6524
 656/3000 [=====>........................] - ETA: 29:44 - loss: 3.0199 - regression_loss: 2.3678 - classification_loss: 0.6522
 657/3000 [=====>........................] - ETA: 29:42 - loss: 3.0193 - regression_loss: 2.3673 - classification_loss: 0.6519
 658/3000 [=====>........................] - ETA: 29:40 - loss: 3.0172 - regression_loss: 2.3656 - classification_loss: 0.6516
 659/3000 [=====>........................] - ETA: 29:38 - loss: 3.0146 - regression_loss: 2.3633 - classification_loss: 0.6513
 660/3000 [=====>........................] - ETA: 29:36 - loss: 3.0131 - regression_loss: 2.3621 - classification_loss: 0.6510
 661/3000 [=====>........................] - ETA: 29:34 - loss: 3.0104 - regression_loss: 2.3598 - classification_loss: 0.6506
 662/3000 [=====>........................] - ETA: 29:32 - loss: 3.0099 - regression_loss: 2.3594 - classification_loss: 0.6505
 663/3000 [=====>........................] - ETA: 29:30 - loss: 3.0094 - regression_loss: 2.3591 - classification_loss: 0.6503
 664/3000 [=====>........................] - ETA: 29:28 - loss: 3.0087 - regression_loss: 2.3585 - classification_loss: 0.6502
 665/3000 [=====>........................] - ETA: 29:26 - loss: 3.0070 - regression_loss: 2.3570 - classification_loss: 0.6500
 666/3000 [=====>........................] - ETA: 29:24 - loss: 3.0041 - regression_loss: 2.3545 - classification_loss: 0.6496
 667/3000 [=====>........................] - ETA: 29:22 - loss: 3.0032 - regression_loss: 2.3538 - classification_loss: 0.6494
 668/3000 [=====>........................] - ETA: 29:20 - loss: 3.0009 - regression_loss: 2.3519 - classification_loss: 0.6490
 669/3000 [=====>........................] - ETA: 29:18 - loss: 3.0000 - regression_loss: 2.3511 - classification_loss: 0.6488
 670/3000 [=====>........................] - ETA: 29:16 - loss: 2.9978 - regression_loss: 2.3491 - classification_loss: 0.6487
 671/3000 [=====>........................] - ETA: 29:14 - loss: 2.9958 - regression_loss: 2.3474 - classification_loss: 0.6484
 672/3000 [=====>........................] - ETA: 29:12 - loss: 2.9938 - regression_loss: 2.3458 - classification_loss: 0.6480
 673/3000 [=====>........................] - ETA: 29:10 - loss: 2.9916 - regression_loss: 2.3438 - classification_loss: 0.6478
 674/3000 [=====>........................] - ETA: 29:08 - loss: 2.9907 - regression_loss: 2.3431 - classification_loss: 0.6476
 675/3000 [=====>........................] - ETA: 29:06 - loss: 2.9882 - regression_loss: 2.3409 - classification_loss: 0.6473
 676/3000 [=====>........................] - ETA: 29:04 - loss: 2.9880 - regression_loss: 2.3409 - classification_loss: 0.6471
 677/3000 [=====>........................] - ETA: 29:02 - loss: 2.9864 - regression_loss: 2.3395 - classification_loss: 0.6470
 678/3000 [=====>........................] - ETA: 29:00 - loss: 2.9838 - regression_loss: 2.3370 - classification_loss: 0.6468
 679/3000 [=====>........................] - ETA: 28:59 - loss: 2.9827 - regression_loss: 2.3362 - classification_loss: 0.6465
 680/3000 [=====>........................] - ETA: 28:57 - loss: 2.9817 - regression_loss: 2.3355 - classification_loss: 0.6463
 681/3000 [=====>........................] - ETA: 28:55 - loss: 2.9804 - regression_loss: 2.3345 - classification_loss: 0.6460
 682/3000 [=====>........................] - ETA: 28:53 - loss: 2.9780 - regression_loss: 2.3322 - classification_loss: 0.6458
 683/3000 [=====>........................] - ETA: 28:51 - loss: 2.9771 - regression_loss: 2.3316 - classification_loss: 0.6456
 684/3000 [=====>........................] - ETA: 28:49 - loss: 2.9755 - regression_loss: 2.3303 - classification_loss: 0.6452
 685/3000 [=====>........................] - ETA: 28:47 - loss: 2.9735 - regression_loss: 2.3285 - classification_loss: 0.6449
 686/3000 [=====>........................] - ETA: 28:45 - loss: 2.9720 - regression_loss: 2.3275 - classification_loss: 0.6446
 687/3000 [=====>........................] - ETA: 28:44 - loss: 2.9701 - regression_loss: 2.3258 - classification_loss: 0.6443
 688/3000 [=====>........................] - ETA: 28:42 - loss: 2.9678 - regression_loss: 2.3238 - classification_loss: 0.6440
 689/3000 [=====>........................] - ETA: 28:40 - loss: 2.9663 - regression_loss: 2.3227 - classification_loss: 0.6437
 690/3000 [=====>........................] - ETA: 28:38 - loss: 2.9641 - regression_loss: 2.3208 - classification_loss: 0.6433
 691/3000 [=====>........................] - ETA: 28:36 - loss: 2.9619 - regression_loss: 2.3189 - classification_loss: 0.6430
 692/3000 [=====>........................] - ETA: 28:34 - loss: 2.9596 - regression_loss: 2.3169 - classification_loss: 0.6426
 693/3000 [=====>........................] - ETA: 28:32 - loss: 2.9577 - regression_loss: 2.3153 - classification_loss: 0.6424
 694/3000 [=====>........................] - ETA: 28:31 - loss: 2.9568 - regression_loss: 2.3146 - classification_loss: 0.6422
 695/3000 [=====>........................] - ETA: 28:29 - loss: 2.9558 - regression_loss: 2.3138 - classification_loss: 0.6420
 696/3000 [=====>........................] - ETA: 28:27 - loss: 2.9537 - regression_loss: 2.3120 - classification_loss: 0.6417
 697/3000 [=====>........................] - ETA: 28:25 - loss: 2.9534 - regression_loss: 2.3118 - classification_loss: 0.6416
 698/3000 [=====>........................] - ETA: 28:23 - loss: 2.9510 - regression_loss: 2.3096 - classification_loss: 0.6415
 699/3000 [=====>........................] - ETA: 28:21 - loss: 2.9487 - regression_loss: 2.3076 - classification_loss: 0.6411
 700/3000 [======>.......................] - ETA: 28:19 - loss: 2.9471 - regression_loss: 2.3063 - classification_loss: 0.6408
 701/3000 [======>.......................] - ETA: 28:18 - loss: 2.9452 - regression_loss: 2.3048 - classification_loss: 0.6404
 702/3000 [======>.......................] - ETA: 28:16 - loss: 2.9446 - regression_loss: 2.3044 - classification_loss: 0.6402
 703/3000 [======>.......................] - ETA: 28:14 - loss: 2.9425 - regression_loss: 2.3026 - classification_loss: 0.6399
 704/3000 [======>.......................] - ETA: 28:12 - loss: 2.9411 - regression_loss: 2.3014 - classification_loss: 0.6397
 705/3000 [======>.......................] - ETA: 28:10 - loss: 2.9389 - regression_loss: 2.2995 - classification_loss: 0.6394
 706/3000 [======>.......................] - ETA: 28:08 - loss: 2.9377 - regression_loss: 2.2985 - classification_loss: 0.6392
 707/3000 [======>.......................] - ETA: 28:07 - loss: 2.9356 - regression_loss: 2.2968 - classification_loss: 0.6388
 708/3000 [======>.......................] - ETA: 28:05 - loss: 2.9339 - regression_loss: 2.2954 - classification_loss: 0.6385
 709/3000 [======>.......................] - ETA: 28:03 - loss: 2.9326 - regression_loss: 2.2945 - classification_loss: 0.6381
 710/3000 [======>.......................] - ETA: 28:01 - loss: 2.9312 - regression_loss: 2.2932 - classification_loss: 0.6380
 711/3000 [======>.......................] - ETA: 28:00 - loss: 2.9289 - regression_loss: 2.2912 - classification_loss: 0.6377
 712/3000 [======>.......................] - ETA: 27:58 - loss: 2.9271 - regression_loss: 2.2897 - classification_loss: 0.6374
 713/3000 [======>.......................] - ETA: 27:56 - loss: 2.9263 - regression_loss: 2.2892 - classification_loss: 0.6372
 714/3000 [======>.......................] - ETA: 27:54 - loss: 2.9245 - regression_loss: 2.2877 - classification_loss: 0.6368
 715/3000 [======>.......................] - ETA: 27:53 - loss: 2.9235 - regression_loss: 2.2868 - classification_loss: 0.6367
 716/3000 [======>.......................] - ETA: 27:51 - loss: 2.9229 - regression_loss: 2.2863 - classification_loss: 0.6365
 717/3000 [======>.......................] - ETA: 27:49 - loss: 2.9219 - regression_loss: 2.2857 - classification_loss: 0.6363
 718/3000 [======>.......................] - ETA: 27:47 - loss: 2.9200 - regression_loss: 2.2840 - classification_loss: 0.6360
 719/3000 [======>.......................] - ETA: 27:46 - loss: 2.9178 - regression_loss: 2.2820 - classification_loss: 0.6357
 720/3000 [======>.......................] - ETA: 27:44 - loss: 2.9166 - regression_loss: 2.2812 - classification_loss: 0.6354
 721/3000 [======>.......................] - ETA: 27:42 - loss: 2.9149 - regression_loss: 2.2797 - classification_loss: 0.6352
 722/3000 [======>.......................] - ETA: 27:40 - loss: 2.9144 - regression_loss: 2.2794 - classification_loss: 0.6350
 723/3000 [======>.......................] - ETA: 27:39 - loss: 2.9135 - regression_loss: 2.2786 - classification_loss: 0.6348
 724/3000 [======>.......................] - ETA: 27:37 - loss: 2.9119 - regression_loss: 2.2773 - classification_loss: 0.6345
 725/3000 [======>.......................] - ETA: 27:35 - loss: 2.9096 - regression_loss: 2.2752 - classification_loss: 0.6344
 726/3000 [======>.......................] - ETA: 27:33 - loss: 2.9074 - regression_loss: 2.2733 - classification_loss: 0.6341
 727/3000 [======>.......................] - ETA: 27:32 - loss: 2.9056 - regression_loss: 2.2718 - classification_loss: 0.6338
 728/3000 [======>.......................] - ETA: 27:30 - loss: 2.9036 - regression_loss: 2.2701 - classification_loss: 0.6335
 729/3000 [======>.......................] - ETA: 27:28 - loss: 2.9012 - regression_loss: 2.2679 - classification_loss: 0.6333
 730/3000 [======>.......................] - ETA: 27:27 - loss: 2.8993 - regression_loss: 2.2660 - classification_loss: 0.6332
 731/3000 [======>.......................] - ETA: 27:25 - loss: 2.8982 - regression_loss: 2.2652 - classification_loss: 0.6330
 732/3000 [======>.......................] - ETA: 27:23 - loss: 2.8958 - regression_loss: 2.2632 - classification_loss: 0.6327
 733/3000 [======>.......................] - ETA: 27:21 - loss: 2.8949 - regression_loss: 2.2624 - classification_loss: 0.6325
 734/3000 [======>.......................] - ETA: 27:20 - loss: 2.8929 - regression_loss: 2.2607 - classification_loss: 0.6322
 735/3000 [======>.......................] - ETA: 27:18 - loss: 2.8910 - regression_loss: 2.2589 - classification_loss: 0.6320
 736/3000 [======>.......................] - ETA: 27:16 - loss: 2.8890 - regression_loss: 2.2573 - classification_loss: 0.6318
 737/3000 [======>.......................] - ETA: 27:15 - loss: 2.8887 - regression_loss: 2.2571 - classification_loss: 0.6317
 738/3000 [======>.......................] - ETA: 27:13 - loss: 2.8875 - regression_loss: 2.2560 - classification_loss: 0.6315
 739/3000 [======>.......................] - ETA: 27:11 - loss: 2.8857 - regression_loss: 2.2545 - classification_loss: 0.6311
 740/3000 [======>.......................] - ETA: 27:10 - loss: 2.8835 - regression_loss: 2.2527 - classification_loss: 0.6307
 741/3000 [======>.......................] - ETA: 27:08 - loss: 2.8815 - regression_loss: 2.2509 - classification_loss: 0.6306
 742/3000 [======>.......................] - ETA: 27:06 - loss: 2.8790 - regression_loss: 2.2489 - classification_loss: 0.6301
 743/3000 [======>.......................] - ETA: 27:05 - loss: 2.8775 - regression_loss: 2.2477 - classification_loss: 0.6298
 744/3000 [======>.......................] - ETA: 27:03 - loss: 2.8755 - regression_loss: 2.2461 - classification_loss: 0.6294
 745/3000 [======>.......................] - ETA: 27:01 - loss: 2.8737 - regression_loss: 2.2444 - classification_loss: 0.6293
 746/3000 [======>.......................] - ETA: 27:00 - loss: 2.8715 - regression_loss: 2.2425 - classification_loss: 0.6291
 747/3000 [======>.......................] - ETA: 26:58 - loss: 2.8708 - regression_loss: 2.2418 - classification_loss: 0.6289
 748/3000 [======>.......................] - ETA: 26:57 - loss: 2.8684 - regression_loss: 2.2398 - classification_loss: 0.6286
 749/3000 [======>.......................] - ETA: 26:55 - loss: 2.8670 - regression_loss: 2.2386 - classification_loss: 0.6283
 750/3000 [======>.......................] - ETA: 26:53 - loss: 2.8659 - regression_loss: 2.2378 - classification_loss: 0.6281
 751/3000 [======>.......................] - ETA: 26:52 - loss: 2.8651 - regression_loss: 2.2372 - classification_loss: 0.6280
 752/3000 [======>.......................] - ETA: 26:50 - loss: 2.8638 - regression_loss: 2.2361 - classification_loss: 0.6277
 753/3000 [======>.......................] - ETA: 26:48 - loss: 2.8619 - regression_loss: 2.2345 - classification_loss: 0.6275
 754/3000 [======>.......................] - ETA: 26:47 - loss: 2.8599 - regression_loss: 2.2327 - classification_loss: 0.6273
 755/3000 [======>.......................] - ETA: 26:45 - loss: 2.8581 - regression_loss: 2.2312 - classification_loss: 0.6268
 756/3000 [======>.......................] - ETA: 26:43 - loss: 2.8562 - regression_loss: 2.2295 - classification_loss: 0.6267
 757/3000 [======>.......................] - ETA: 26:42 - loss: 2.8545 - regression_loss: 2.2280 - classification_loss: 0.6265
 758/3000 [======>.......................] - ETA: 26:40 - loss: 2.8527 - regression_loss: 2.2264 - classification_loss: 0.6263
 759/3000 [======>.......................] - ETA: 26:39 - loss: 2.8512 - regression_loss: 2.2253 - classification_loss: 0.6260
 760/3000 [======>.......................] - ETA: 26:37 - loss: 2.8498 - regression_loss: 2.2240 - classification_loss: 0.6257
 761/3000 [======>.......................] - ETA: 26:35 - loss: 2.8486 - regression_loss: 2.2230 - classification_loss: 0.6256
 762/3000 [======>.......................] - ETA: 26:34 - loss: 2.8480 - regression_loss: 2.2226 - classification_loss: 0.6254
 763/3000 [======>.......................] - ETA: 26:32 - loss: 2.8463 - regression_loss: 2.2211 - classification_loss: 0.6252
 764/3000 [======>.......................] - ETA: 26:31 - loss: 2.8442 - regression_loss: 2.2192 - classification_loss: 0.6250
 765/3000 [======>.......................] - ETA: 26:29 - loss: 2.8438 - regression_loss: 2.2189 - classification_loss: 0.6249
 766/3000 [======>.......................] - ETA: 26:27 - loss: 2.8427 - regression_loss: 2.2180 - classification_loss: 0.6247
 767/3000 [======>.......................] - ETA: 26:26 - loss: 2.8422 - regression_loss: 2.2176 - classification_loss: 0.6246
 768/3000 [======>.......................] - ETA: 26:24 - loss: 2.8410 - regression_loss: 2.2166 - classification_loss: 0.6244
 769/3000 [======>.......................] - ETA: 26:23 - loss: 2.8401 - regression_loss: 2.2159 - classification_loss: 0.6242
 770/3000 [======>.......................] - ETA: 26:21 - loss: 2.8379 - regression_loss: 2.2140 - classification_loss: 0.6239
 771/3000 [======>.......................] - ETA: 26:20 - loss: 2.8356 - regression_loss: 2.2119 - classification_loss: 0.6237
 772/3000 [======>.......................] - ETA: 26:18 - loss: 2.8345 - regression_loss: 2.2110 - classification_loss: 0.6235
 773/3000 [======>.......................] - ETA: 26:17 - loss: 2.8326 - regression_loss: 2.2094 - classification_loss: 0.6232
 774/3000 [======>.......................] - ETA: 26:15 - loss: 2.8301 - regression_loss: 2.2073 - classification_loss: 0.6229
 775/3000 [======>.......................] - ETA: 26:13 - loss: 2.8281 - regression_loss: 2.2055 - classification_loss: 0.6226
 776/3000 [======>.......................] - ETA: 26:12 - loss: 2.8263 - regression_loss: 2.2039 - classification_loss: 0.6224
 777/3000 [======>.......................] - ETA: 26:10 - loss: 2.8247 - regression_loss: 2.2026 - classification_loss: 0.6221
 778/3000 [======>.......................] - ETA: 26:09 - loss: 2.8230 - regression_loss: 2.2012 - classification_loss: 0.6218
 779/3000 [======>.......................] - ETA: 26:07 - loss: 2.8223 - regression_loss: 2.2006 - classification_loss: 0.6217
 780/3000 [======>.......................] - ETA: 26:06 - loss: 2.8205 - regression_loss: 2.1990 - classification_loss: 0.6214
 781/3000 [======>.......................] - ETA: 26:04 - loss: 2.8191 - regression_loss: 2.1978 - classification_loss: 0.6213
 782/3000 [======>.......................] - ETA: 26:03 - loss: 2.8186 - regression_loss: 2.1974 - classification_loss: 0.6212
 783/3000 [======>.......................] - ETA: 26:01 - loss: 2.8175 - regression_loss: 2.1965 - classification_loss: 0.6210
 784/3000 [======>.......................] - ETA: 26:00 - loss: 2.8159 - regression_loss: 2.1951 - classification_loss: 0.6208
 785/3000 [======>.......................] - ETA: 25:58 - loss: 2.8137 - regression_loss: 2.1932 - classification_loss: 0.6205
 786/3000 [======>.......................] - ETA: 25:57 - loss: 2.8122 - regression_loss: 2.1919 - classification_loss: 0.6203
 787/3000 [======>.......................] - ETA: 25:55 - loss: 2.8121 - regression_loss: 2.1919 - classification_loss: 0.6201
 788/3000 [======>.......................] - ETA: 25:54 - loss: 2.8108 - regression_loss: 2.1908 - classification_loss: 0.6201
 789/3000 [======>.......................] - ETA: 25:52 - loss: 2.8094 - regression_loss: 2.1896 - classification_loss: 0.6198
 790/3000 [======>.......................] - ETA: 25:51 - loss: 2.8075 - regression_loss: 2.1878 - classification_loss: 0.6197
 791/3000 [======>.......................] - ETA: 25:49 - loss: 2.8060 - regression_loss: 2.1865 - classification_loss: 0.6195
 792/3000 [======>.......................] - ETA: 25:48 - loss: 2.8042 - regression_loss: 2.1849 - classification_loss: 0.6193
 793/3000 [======>.......................] - ETA: 25:46 - loss: 2.8028 - regression_loss: 2.1838 - classification_loss: 0.6190
 794/3000 [======>.......................] - ETA: 25:45 - loss: 2.8010 - regression_loss: 2.1822 - classification_loss: 0.6188
 795/3000 [======>.......................] - ETA: 25:43 - loss: 2.7994 - regression_loss: 2.1807 - classification_loss: 0.6186
 796/3000 [======>.......................] - ETA: 25:42 - loss: 2.7984 - regression_loss: 2.1800 - classification_loss: 0.6184
 797/3000 [======>.......................] - ETA: 25:40 - loss: 2.7966 - regression_loss: 2.1783 - classification_loss: 0.6182
 798/3000 [======>.......................] - ETA: 25:39 - loss: 2.7954 - regression_loss: 2.1773 - classification_loss: 0.6181
 799/3000 [======>.......................] - ETA: 25:37 - loss: 2.7942 - regression_loss: 2.1763 - classification_loss: 0.6178
 800/3000 [=======>......................] - ETA: 25:36 - loss: 2.7921 - regression_loss: 2.1745 - classification_loss: 0.6176
 801/3000 [=======>......................] - ETA: 25:34 - loss: 2.7907 - regression_loss: 2.1733 - classification_loss: 0.6174
 802/3000 [=======>......................] - ETA: 25:33 - loss: 2.7887 - regression_loss: 2.1714 - classification_loss: 0.6173
 803/3000 [=======>......................] - ETA: 25:31 - loss: 2.7866 - regression_loss: 2.1696 - classification_loss: 0.6170
 804/3000 [=======>......................] - ETA: 25:30 - loss: 2.7853 - regression_loss: 2.1686 - classification_loss: 0.6167
 805/3000 [=======>......................] - ETA: 25:28 - loss: 2.7832 - regression_loss: 2.1668 - classification_loss: 0.6165
 806/3000 [=======>......................] - ETA: 25:27 - loss: 2.7821 - regression_loss: 2.1658 - classification_loss: 0.6164
 807/3000 [=======>......................] - ETA: 25:25 - loss: 2.7803 - regression_loss: 2.1641 - classification_loss: 0.6161
 808/3000 [=======>......................] - ETA: 25:24 - loss: 2.7791 - regression_loss: 2.1632 - classification_loss: 0.6159
 809/3000 [=======>......................] - ETA: 25:22 - loss: 2.7770 - regression_loss: 2.1614 - classification_loss: 0.6156
 810/3000 [=======>......................] - ETA: 25:21 - loss: 2.7756 - regression_loss: 2.1604 - classification_loss: 0.6152
 811/3000 [=======>......................] - ETA: 25:20 - loss: 2.7740 - regression_loss: 2.1589 - classification_loss: 0.6151
 812/3000 [=======>......................] - ETA: 25:18 - loss: 2.7728 - regression_loss: 2.1578 - classification_loss: 0.6150
 813/3000 [=======>......................] - ETA: 25:17 - loss: 2.7723 - regression_loss: 2.1575 - classification_loss: 0.6148
 814/3000 [=======>......................] - ETA: 25:15 - loss: 2.7712 - regression_loss: 2.1566 - classification_loss: 0.6146
 815/3000 [=======>......................] - ETA: 25:14 - loss: 2.7696 - regression_loss: 2.1552 - classification_loss: 0.6144
 816/3000 [=======>......................] - ETA: 25:12 - loss: 2.7678 - regression_loss: 2.1536 - classification_loss: 0.6143
 817/3000 [=======>......................] - ETA: 25:11 - loss: 2.7663 - regression_loss: 2.1523 - classification_loss: 0.6140
 818/3000 [=======>......................] - ETA: 25:09 - loss: 2.7653 - regression_loss: 2.1515 - classification_loss: 0.6139
 819/3000 [=======>......................] - ETA: 25:08 - loss: 2.7647 - regression_loss: 2.1509 - classification_loss: 0.6138
 820/3000 [=======>......................] - ETA: 25:06 - loss: 2.7632 - regression_loss: 2.1496 - classification_loss: 0.6136
 821/3000 [=======>......................] - ETA: 25:05 - loss: 2.7614 - regression_loss: 2.1480 - classification_loss: 0.6134
 822/3000 [=======>......................] - ETA: 25:04 - loss: 2.7606 - regression_loss: 2.1473 - classification_loss: 0.6133
 823/3000 [=======>......................] - ETA: 25:02 - loss: 2.7597 - regression_loss: 2.1465 - classification_loss: 0.6131
 824/3000 [=======>......................] - ETA: 25:01 - loss: 2.7577 - regression_loss: 2.1448 - classification_loss: 0.6129
 825/3000 [=======>......................] - ETA: 24:59 - loss: 2.7560 - regression_loss: 2.1433 - classification_loss: 0.6127
 826/3000 [=======>......................] - ETA: 24:58 - loss: 2.7563 - regression_loss: 2.1437 - classification_loss: 0.6126
 827/3000 [=======>......................] - ETA: 24:56 - loss: 2.7545 - regression_loss: 2.1421 - classification_loss: 0.6124
 828/3000 [=======>......................] - ETA: 24:55 - loss: 2.7534 - regression_loss: 2.1411 - classification_loss: 0.6123
 829/3000 [=======>......................] - ETA: 24:54 - loss: 2.7525 - regression_loss: 2.1403 - classification_loss: 0.6121
 830/3000 [=======>......................] - ETA: 24:52 - loss: 2.7510 - regression_loss: 2.1391 - classification_loss: 0.6119
 831/3000 [=======>......................] - ETA: 24:51 - loss: 2.7492 - regression_loss: 2.1374 - classification_loss: 0.6117
 832/3000 [=======>......................] - ETA: 24:49 - loss: 2.7480 - regression_loss: 2.1366 - classification_loss: 0.6115
 833/3000 [=======>......................] - ETA: 24:48 - loss: 2.7478 - regression_loss: 2.1365 - classification_loss: 0.6113
 834/3000 [=======>......................] - ETA: 24:47 - loss: 2.7466 - regression_loss: 2.1356 - classification_loss: 0.6110
 835/3000 [=======>......................] - ETA: 24:45 - loss: 2.7452 - regression_loss: 2.1343 - classification_loss: 0.6109
 836/3000 [=======>......................] - ETA: 24:44 - loss: 2.7435 - regression_loss: 2.1329 - classification_loss: 0.6106
 837/3000 [=======>......................] - ETA: 24:42 - loss: 2.7418 - regression_loss: 2.1314 - classification_loss: 0.6104
 838/3000 [=======>......................] - ETA: 24:41 - loss: 2.7404 - regression_loss: 2.1302 - classification_loss: 0.6102
 839/3000 [=======>......................] - ETA: 24:40 - loss: 2.7386 - regression_loss: 2.1284 - classification_loss: 0.6101
 840/3000 [=======>......................] - ETA: 24:38 - loss: 2.7367 - regression_loss: 2.1268 - classification_loss: 0.6099
 841/3000 [=======>......................] - ETA: 24:37 - loss: 2.7361 - regression_loss: 2.1264 - classification_loss: 0.6097
 842/3000 [=======>......................] - ETA: 24:36 - loss: 2.7348 - regression_loss: 2.1253 - classification_loss: 0.6095
 843/3000 [=======>......................] - ETA: 24:34 - loss: 2.7342 - regression_loss: 2.1249 - classification_loss: 0.6093
 844/3000 [=======>......................] - ETA: 24:33 - loss: 2.7328 - regression_loss: 2.1238 - classification_loss: 0.6090
 845/3000 [=======>......................] - ETA: 24:31 - loss: 2.7317 - regression_loss: 2.1229 - classification_loss: 0.6088
 846/3000 [=======>......................] - ETA: 24:30 - loss: 2.7298 - regression_loss: 2.1211 - classification_loss: 0.6087
 847/3000 [=======>......................] - ETA: 24:29 - loss: 2.7291 - regression_loss: 2.1207 - classification_loss: 0.6084
 848/3000 [=======>......................] - ETA: 24:27 - loss: 2.7277 - regression_loss: 2.1195 - classification_loss: 0.6082
 849/3000 [=======>......................] - ETA: 24:26 - loss: 2.7265 - regression_loss: 2.1184 - classification_loss: 0.6080
 850/3000 [=======>......................] - ETA: 24:25 - loss: 2.7252 - regression_loss: 2.1175 - classification_loss: 0.6077
 851/3000 [=======>......................] - ETA: 24:23 - loss: 2.7237 - regression_loss: 2.1163 - classification_loss: 0.6074
 852/3000 [=======>......................] - ETA: 24:22 - loss: 2.7223 - regression_loss: 2.1150 - classification_loss: 0.6073
 853/3000 [=======>......................] - ETA: 24:21 - loss: 2.7212 - regression_loss: 2.1140 - classification_loss: 0.6072
 854/3000 [=======>......................] - ETA: 24:19 - loss: 2.7199 - regression_loss: 2.1127 - classification_loss: 0.6071
 855/3000 [=======>......................] - ETA: 24:18 - loss: 2.7185 - regression_loss: 2.1115 - classification_loss: 0.6070
 856/3000 [=======>......................] - ETA: 24:17 - loss: 2.7165 - regression_loss: 2.1097 - classification_loss: 0.6067
 857/3000 [=======>......................] - ETA: 24:15 - loss: 2.7147 - regression_loss: 2.1081 - classification_loss: 0.6066
 858/3000 [=======>......................] - ETA: 24:14 - loss: 2.7130 - regression_loss: 2.1067 - classification_loss: 0.6063
 859/3000 [=======>......................] - ETA: 24:12 - loss: 2.7113 - regression_loss: 2.1051 - classification_loss: 0.6062
 860/3000 [=======>......................] - ETA: 24:11 - loss: 2.7103 - regression_loss: 2.1043 - classification_loss: 0.6059
 861/3000 [=======>......................] - ETA: 24:10 - loss: 2.7091 - regression_loss: 2.1033 - classification_loss: 0.6058
 862/3000 [=======>......................] - ETA: 24:08 - loss: 2.7073 - regression_loss: 2.1017 - classification_loss: 0.6056
 863/3000 [=======>......................] - ETA: 24:07 - loss: 2.7058 - regression_loss: 2.1004 - classification_loss: 0.6054
 864/3000 [=======>......................] - ETA: 24:06 - loss: 2.7048 - regression_loss: 2.0995 - classification_loss: 0.6053
 865/3000 [=======>......................] - ETA: 24:04 - loss: 2.7032 - regression_loss: 2.0980 - classification_loss: 0.6052
 866/3000 [=======>......................] - ETA: 24:03 - loss: 2.7015 - regression_loss: 2.0965 - classification_loss: 0.6050
 867/3000 [=======>......................] - ETA: 24:02 - loss: 2.7002 - regression_loss: 2.0955 - classification_loss: 0.6048
 868/3000 [=======>......................] - ETA: 24:00 - loss: 2.6988 - regression_loss: 2.0944 - classification_loss: 0.6044
 869/3000 [=======>......................] - ETA: 23:59 - loss: 2.6978 - regression_loss: 2.0936 - classification_loss: 0.6042
 870/3000 [=======>......................] - ETA: 23:58 - loss: 2.6975 - regression_loss: 2.0934 - classification_loss: 0.6041
 871/3000 [=======>......................] - ETA: 23:56 - loss: 2.6969 - regression_loss: 2.0929 - classification_loss: 0.6040
 872/3000 [=======>......................] - ETA: 23:55 - loss: 2.6960 - regression_loss: 2.0922 - classification_loss: 0.6039
 873/3000 [=======>......................] - ETA: 23:54 - loss: 2.6945 - regression_loss: 2.0909 - classification_loss: 0.6036
 874/3000 [=======>......................] - ETA: 23:52 - loss: 2.6927 - regression_loss: 2.0894 - classification_loss: 0.6033
 875/3000 [=======>......................] - ETA: 23:51 - loss: 2.6918 - regression_loss: 2.0887 - classification_loss: 0.6031
 876/3000 [=======>......................] - ETA: 23:50 - loss: 2.6905 - regression_loss: 2.0876 - classification_loss: 0.6029
 877/3000 [=======>......................] - ETA: 23:48 - loss: 2.6898 - regression_loss: 2.0870 - classification_loss: 0.6028
 878/3000 [=======>......................] - ETA: 23:47 - loss: 2.6889 - regression_loss: 2.0862 - classification_loss: 0.6027
 879/3000 [=======>......................] - ETA: 23:46 - loss: 2.6873 - regression_loss: 2.0849 - classification_loss: 0.6025
 880/3000 [=======>......................] - ETA: 23:45 - loss: 2.6859 - regression_loss: 2.0836 - classification_loss: 0.6023
 881/3000 [=======>......................] - ETA: 23:43 - loss: 2.6850 - regression_loss: 2.0829 - classification_loss: 0.6021
 882/3000 [=======>......................] - ETA: 23:42 - loss: 2.6849 - regression_loss: 2.0828 - classification_loss: 0.6020
 883/3000 [=======>......................] - ETA: 23:41 - loss: 2.6836 - regression_loss: 2.0817 - classification_loss: 0.6019
 884/3000 [=======>......................] - ETA: 23:39 - loss: 2.6818 - regression_loss: 2.0801 - classification_loss: 0.6017
 885/3000 [=======>......................] - ETA: 23:38 - loss: 2.6806 - regression_loss: 2.0791 - classification_loss: 0.6015
 886/3000 [=======>......................] - ETA: 23:37 - loss: 2.6800 - regression_loss: 2.0786 - classification_loss: 0.6013
 887/3000 [=======>......................] - ETA: 23:35 - loss: 2.6784 - regression_loss: 2.0772 - classification_loss: 0.6012
 888/3000 [=======>......................] - ETA: 23:34 - loss: 2.6766 - regression_loss: 2.0757 - classification_loss: 0.6010
 889/3000 [=======>......................] - ETA: 23:33 - loss: 2.6747 - regression_loss: 2.0740 - classification_loss: 0.6008
 890/3000 [=======>......................] - ETA: 23:31 - loss: 2.6735 - regression_loss: 2.0730 - classification_loss: 0.6005
 891/3000 [=======>......................] - ETA: 23:30 - loss: 2.6722 - regression_loss: 2.0718 - classification_loss: 0.6004
 892/3000 [=======>......................] - ETA: 23:29 - loss: 2.6707 - regression_loss: 2.0704 - classification_loss: 0.6003
 893/3000 [=======>......................] - ETA: 23:28 - loss: 2.6690 - regression_loss: 2.0689 - classification_loss: 0.6001
 894/3000 [=======>......................] - ETA: 23:27 - loss: 2.6681 - regression_loss: 2.0682 - classification_loss: 0.5999
 895/3000 [=======>......................] - ETA: 23:25 - loss: 2.6670 - regression_loss: 2.0673 - classification_loss: 0.5997
 896/3000 [=======>......................] - ETA: 23:24 - loss: 2.6663 - regression_loss: 2.0667 - classification_loss: 0.5995
 897/3000 [=======>......................] - ETA: 23:23 - loss: 2.6650 - regression_loss: 2.0658 - classification_loss: 0.5993
 898/3000 [=======>......................] - ETA: 23:21 - loss: 2.6640 - regression_loss: 2.0649 - classification_loss: 0.5991
 899/3000 [=======>......................] - ETA: 23:20 - loss: 2.6627 - regression_loss: 2.0639 - classification_loss: 0.5989
 900/3000 [========>.....................] - ETA: 23:19 - loss: 2.6611 - regression_loss: 2.0624 - classification_loss: 0.5987
 901/3000 [========>.....................] - ETA: 23:18 - loss: 2.6598 - regression_loss: 2.0613 - classification_loss: 0.5985
 902/3000 [========>.....................] - ETA: 23:16 - loss: 2.6587 - regression_loss: 2.0604 - classification_loss: 0.5983
 903/3000 [========>.....................] - ETA: 23:15 - loss: 2.6572 - regression_loss: 2.0591 - classification_loss: 0.5981
 904/3000 [========>.....................] - ETA: 23:14 - loss: 2.6554 - regression_loss: 2.0575 - classification_loss: 0.5979
 905/3000 [========>.....................] - ETA: 23:13 - loss: 2.6543 - regression_loss: 2.0565 - classification_loss: 0.5978
 906/3000 [========>.....................] - ETA: 23:11 - loss: 2.6530 - regression_loss: 2.0554 - classification_loss: 0.5976
 907/3000 [========>.....................] - ETA: 23:10 - loss: 2.6515 - regression_loss: 2.0540 - classification_loss: 0.5974
 908/3000 [========>.....................] - ETA: 23:09 - loss: 2.6498 - regression_loss: 2.0527 - classification_loss: 0.5971
 909/3000 [========>.....................] - ETA: 23:08 - loss: 2.6484 - regression_loss: 2.0515 - classification_loss: 0.5969
 910/3000 [========>.....................] - ETA: 23:06 - loss: 2.6466 - regression_loss: 2.0498 - classification_loss: 0.5968
 911/3000 [========>.....................] - ETA: 23:05 - loss: 2.6454 - regression_loss: 2.0488 - classification_loss: 0.5966
 912/3000 [========>.....................] - ETA: 23:04 - loss: 2.6438 - regression_loss: 2.0474 - classification_loss: 0.5964
 913/3000 [========>.....................] - ETA: 23:02 - loss: 2.6430 - regression_loss: 2.0469 - classification_loss: 0.5962
 914/3000 [========>.....................] - ETA: 23:01 - loss: 2.6414 - regression_loss: 2.0454 - classification_loss: 0.5960
 915/3000 [========>.....................] - ETA: 23:00 - loss: 2.6401 - regression_loss: 2.0443 - classification_loss: 0.5958
 916/3000 [========>.....................] - ETA: 22:59 - loss: 2.6392 - regression_loss: 2.0436 - classification_loss: 0.5956
 917/3000 [========>.....................] - ETA: 22:58 - loss: 2.6376 - regression_loss: 2.0423 - classification_loss: 0.5953
 918/3000 [========>.....................] - ETA: 22:56 - loss: 2.6366 - regression_loss: 2.0414 - classification_loss: 0.5952
 919/3000 [========>.....................] - ETA: 22:55 - loss: 2.6354 - regression_loss: 2.0404 - classification_loss: 0.5950
 920/3000 [========>.....................] - ETA: 22:54 - loss: 2.6339 - regression_loss: 2.0390 - classification_loss: 0.5948
 921/3000 [========>.....................] - ETA: 22:53 - loss: 2.6330 - regression_loss: 2.0382 - classification_loss: 0.5948
 922/3000 [========>.....................] - ETA: 22:51 - loss: 2.6320 - regression_loss: 2.0374 - classification_loss: 0.5946
 923/3000 [========>.....................] - ETA: 22:50 - loss: 2.6306 - regression_loss: 2.0362 - classification_loss: 0.5944
 924/3000 [========>.....................] - ETA: 22:49 - loss: 2.6291 - regression_loss: 2.0351 - classification_loss: 0.5941
 925/3000 [========>.....................] - ETA: 22:48 - loss: 2.6274 - regression_loss: 2.0335 - classification_loss: 0.5939
 926/3000 [========>.....................] - ETA: 22:46 - loss: 2.6262 - regression_loss: 2.0325 - classification_loss: 0.5937
 927/3000 [========>.....................] - ETA: 22:45 - loss: 2.6246 - regression_loss: 2.0311 - classification_loss: 0.5935
 928/3000 [========>.....................] - ETA: 22:44 - loss: 2.6235 - regression_loss: 2.0303 - classification_loss: 0.5932
 929/3000 [========>.....................] - ETA: 22:43 - loss: 2.6220 - regression_loss: 2.0289 - classification_loss: 0.5930
 930/3000 [========>.....................] - ETA: 22:42 - loss: 2.6202 - regression_loss: 2.0274 - classification_loss: 0.5928
 931/3000 [========>.....................] - ETA: 22:40 - loss: 2.6191 - regression_loss: 2.0263 - classification_loss: 0.5927
 932/3000 [========>.....................] - ETA: 22:39 - loss: 2.6180 - regression_loss: 2.0254 - classification_loss: 0.5925
 933/3000 [========>.....................] - ETA: 22:38 - loss: 2.6165 - regression_loss: 2.0241 - classification_loss: 0.5924
 934/3000 [========>.....................] - ETA: 22:37 - loss: 2.6154 - regression_loss: 2.0231 - classification_loss: 0.5923
 935/3000 [========>.....................] - ETA: 22:36 - loss: 2.6149 - regression_loss: 2.0227 - classification_loss: 0.5922
 936/3000 [========>.....................] - ETA: 22:34 - loss: 2.6133 - regression_loss: 2.0213 - classification_loss: 0.5920
 937/3000 [========>.....................] - ETA: 22:33 - loss: 2.6120 - regression_loss: 2.0201 - classification_loss: 0.5919
 938/3000 [========>.....................] - ETA: 22:32 - loss: 2.6108 - regression_loss: 2.0191 - classification_loss: 0.5916
 939/3000 [========>.....................] - ETA: 22:31 - loss: 2.6103 - regression_loss: 2.0188 - classification_loss: 0.5915
 940/3000 [========>.....................] - ETA: 22:30 - loss: 2.6086 - regression_loss: 2.0173 - classification_loss: 0.5914
 941/3000 [========>.....................] - ETA: 22:29 - loss: 2.6072 - regression_loss: 2.0160 - classification_loss: 0.5912
 942/3000 [========>.....................] - ETA: 22:27 - loss: 2.6057 - regression_loss: 2.0147 - classification_loss: 0.5910
 943/3000 [========>.....................] - ETA: 22:26 - loss: 2.6038 - regression_loss: 2.0131 - classification_loss: 0.5907
 944/3000 [========>.....................] - ETA: 22:25 - loss: 2.6034 - regression_loss: 2.0129 - classification_loss: 0.5905
 945/3000 [========>.....................] - ETA: 22:24 - loss: 2.6030 - regression_loss: 2.0126 - classification_loss: 0.5904
 946/3000 [========>.....................] - ETA: 22:23 - loss: 2.6027 - regression_loss: 2.0124 - classification_loss: 0.5903
 947/3000 [========>.....................] - ETA: 22:21 - loss: 2.6016 - regression_loss: 2.0115 - classification_loss: 0.5901
 948/3000 [========>.....................] - ETA: 22:20 - loss: 2.6004 - regression_loss: 2.0105 - classification_loss: 0.5899
 949/3000 [========>.....................] - ETA: 22:19 - loss: 2.5994 - regression_loss: 2.0096 - classification_loss: 0.5898
 950/3000 [========>.....................] - ETA: 22:18 - loss: 2.5988 - regression_loss: 2.0092 - classification_loss: 0.5896
 951/3000 [========>.....................] - ETA: 22:17 - loss: 2.5974 - regression_loss: 2.0079 - classification_loss: 0.5894
 952/3000 [========>.....................] - ETA: 22:16 - loss: 2.5958 - regression_loss: 2.0066 - classification_loss: 0.5892
 953/3000 [========>.....................] - ETA: 22:14 - loss: 2.5945 - regression_loss: 2.0053 - classification_loss: 0.5891
 954/3000 [========>.....................] - ETA: 22:13 - loss: 2.5930 - regression_loss: 2.0040 - classification_loss: 0.5890
 955/3000 [========>.....................] - ETA: 22:12 - loss: 2.5916 - regression_loss: 2.0027 - classification_loss: 0.5888
 956/3000 [========>.....................] - ETA: 22:11 - loss: 2.5900 - regression_loss: 2.0014 - classification_loss: 0.5887
 957/3000 [========>.....................] - ETA: 22:10 - loss: 2.5893 - regression_loss: 2.0007 - classification_loss: 0.5885
 958/3000 [========>.....................] - ETA: 22:08 - loss: 2.5881 - regression_loss: 1.9996 - classification_loss: 0.5884
 959/3000 [========>.....................] - ETA: 22:07 - loss: 2.5871 - regression_loss: 1.9989 - classification_loss: 0.5882
 960/3000 [========>.....................] - ETA: 22:06 - loss: 2.5864 - regression_loss: 1.9984 - classification_loss: 0.5881
 961/3000 [========>.....................] - ETA: 22:05 - loss: 2.5849 - regression_loss: 1.9971 - classification_loss: 0.5878
 962/3000 [========>.....................] - ETA: 22:04 - loss: 2.5837 - regression_loss: 1.9961 - classification_loss: 0.5877
 963/3000 [========>.....................] - ETA: 22:03 - loss: 2.5824 - regression_loss: 1.9950 - classification_loss: 0.5874
 964/3000 [========>.....................] - ETA: 22:02 - loss: 2.5807 - regression_loss: 1.9935 - classification_loss: 0.5872
 965/3000 [========>.....................] - ETA: 22:00 - loss: 2.5789 - regression_loss: 1.9919 - classification_loss: 0.5870
 966/3000 [========>.....................] - ETA: 21:59 - loss: 2.5775 - regression_loss: 1.9906 - classification_loss: 0.5868
 967/3000 [========>.....................] - ETA: 21:58 - loss: 2.5769 - regression_loss: 1.9902 - classification_loss: 0.5867
 968/3000 [========>.....................] - ETA: 21:57 - loss: 2.5755 - regression_loss: 1.9890 - classification_loss: 0.5865
 969/3000 [========>.....................] - ETA: 21:56 - loss: 2.5744 - regression_loss: 1.9880 - classification_loss: 0.5864
 970/3000 [========>.....................] - ETA: 21:55 - loss: 2.5737 - regression_loss: 1.9874 - classification_loss: 0.5863
 971/3000 [========>.....................] - ETA: 21:54 - loss: 2.5721 - regression_loss: 1.9859 - classification_loss: 0.5861
 972/3000 [========>.....................] - ETA: 21:52 - loss: 2.5710 - regression_loss: 1.9850 - classification_loss: 0.5859
 973/3000 [========>.....................] - ETA: 21:51 - loss: 2.5696 - regression_loss: 1.9839 - classification_loss: 0.5857
 974/3000 [========>.....................] - ETA: 21:50 - loss: 2.5687 - regression_loss: 1.9830 - classification_loss: 0.5856
 975/3000 [========>.....................] - ETA: 21:49 - loss: 2.5672 - regression_loss: 1.9818 - classification_loss: 0.5855
 976/3000 [========>.....................] - ETA: 21:48 - loss: 2.5666 - regression_loss: 1.9812 - classification_loss: 0.5854
 977/3000 [========>.....................] - ETA: 21:47 - loss: 2.5651 - regression_loss: 1.9799 - classification_loss: 0.5852
 978/3000 [========>.....................] - ETA: 21:46 - loss: 2.5642 - regression_loss: 1.9792 - classification_loss: 0.5851
 979/3000 [========>.....................] - ETA: 21:44 - loss: 2.5633 - regression_loss: 1.9784 - classification_loss: 0.5849
 980/3000 [========>.....................] - ETA: 21:43 - loss: 2.5623 - regression_loss: 1.9776 - classification_loss: 0.5847
 981/3000 [========>.....................] - ETA: 21:42 - loss: 2.5608 - regression_loss: 1.9763 - classification_loss: 0.5846
 982/3000 [========>.....................] - ETA: 21:41 - loss: 2.5593 - regression_loss: 1.9749 - classification_loss: 0.5844
 983/3000 [========>.....................] - ETA: 21:40 - loss: 2.5579 - regression_loss: 1.9737 - classification_loss: 0.5842
 984/3000 [========>.....................] - ETA: 21:39 - loss: 2.5577 - regression_loss: 1.9735 - classification_loss: 0.5841
 985/3000 [========>.....................] - ETA: 21:38 - loss: 2.5574 - regression_loss: 1.9733 - classification_loss: 0.5841
 986/3000 [========>.....................] - ETA: 21:37 - loss: 2.5560 - regression_loss: 1.9722 - classification_loss: 0.5839
 987/3000 [========>.....................] - ETA: 21:35 - loss: 2.5558 - regression_loss: 1.9721 - classification_loss: 0.5838
 988/3000 [========>.....................] - ETA: 21:34 - loss: 2.5543 - regression_loss: 1.9707 - classification_loss: 0.5836
 989/3000 [========>.....................] - ETA: 21:33 - loss: 2.5531 - regression_loss: 1.9697 - classification_loss: 0.5834
 990/3000 [========>.....................] - ETA: 21:32 - loss: 2.5525 - regression_loss: 1.9692 - classification_loss: 0.5833
 991/3000 [========>.....................] - ETA: 21:31 - loss: 2.5512 - regression_loss: 1.9680 - classification_loss: 0.5833
 992/3000 [========>.....................] - ETA: 21:30 - loss: 2.5503 - regression_loss: 1.9671 - classification_loss: 0.5832
 993/3000 [========>.....................] - ETA: 21:29 - loss: 2.5488 - regression_loss: 1.9658 - classification_loss: 0.5830
 994/3000 [========>.....................] - ETA: 21:28 - loss: 2.5473 - regression_loss: 1.9643 - classification_loss: 0.5829
 995/3000 [========>.....................] - ETA: 21:27 - loss: 2.5468 - regression_loss: 1.9640 - classification_loss: 0.5827
 996/3000 [========>.....................] - ETA: 21:25 - loss: 2.5453 - regression_loss: 1.9627 - classification_loss: 0.5826
 997/3000 [========>.....................] - ETA: 21:24 - loss: 2.5445 - regression_loss: 1.9621 - classification_loss: 0.5825
 998/3000 [========>.....................] - ETA: 21:23 - loss: 2.5435 - regression_loss: 1.9612 - classification_loss: 0.5823
 999/3000 [========>.....................] - ETA: 21:22 - loss: 2.5423 - regression_loss: 1.9602 - classification_loss: 0.5821
1000/3000 [=========>....................] - ETA: 21:21 - loss: 2.5410 - regression_loss: 1.9590 - classification_loss: 0.5819
1001/3000 [=========>....................] - ETA: 21:20 - loss: 2.5396 - regression_loss: 1.9578 - classification_loss: 0.5818
1002/3000 [=========>....................] - ETA: 21:19 - loss: 2.5385 - regression_loss: 1.9568 - classification_loss: 0.5817
1003/3000 [=========>....................] - ETA: 21:18 - loss: 2.5372 - regression_loss: 1.9557 - classification_loss: 0.5815
1004/3000 [=========>....................] - ETA: 21:17 - loss: 2.5364 - regression_loss: 1.9551 - classification_loss: 0.5813
1005/3000 [=========>....................] - ETA: 21:16 - loss: 2.5350 - regression_loss: 1.9538 - classification_loss: 0.5812
1006/3000 [=========>....................] - ETA: 21:14 - loss: 2.5346 - regression_loss: 1.9535 - classification_loss: 0.5811
1007/3000 [=========>....................] - ETA: 21:13 - loss: 2.5334 - regression_loss: 1.9523 - classification_loss: 0.5810
1008/3000 [=========>....................] - ETA: 21:12 - loss: 2.5318 - regression_loss: 1.9511 - classification_loss: 0.5808
1009/3000 [=========>....................] - ETA: 21:11 - loss: 2.5310 - regression_loss: 1.9504 - classification_loss: 0.5806
1010/3000 [=========>....................] - ETA: 21:10 - loss: 2.5298 - regression_loss: 1.9494 - classification_loss: 0.5804
1011/3000 [=========>....................] - ETA: 21:09 - loss: 2.5284 - regression_loss: 1.9481 - classification_loss: 0.5802
1012/3000 [=========>....................] - ETA: 21:08 - loss: 2.5270 - regression_loss: 1.9469 - classification_loss: 0.5800
1013/3000 [=========>....................] - ETA: 21:07 - loss: 2.5263 - regression_loss: 1.9464 - classification_loss: 0.5799
1014/3000 [=========>....................] - ETA: 21:06 - loss: 2.5254 - regression_loss: 1.9456 - classification_loss: 0.5798
1015/3000 [=========>....................] - ETA: 21:05 - loss: 2.5240 - regression_loss: 1.9444 - classification_loss: 0.5796
1016/3000 [=========>....................] - ETA: 21:04 - loss: 2.5233 - regression_loss: 1.9439 - classification_loss: 0.5794
1017/3000 [=========>....................] - ETA: 21:02 - loss: 2.5227 - regression_loss: 1.9434 - classification_loss: 0.5793
1018/3000 [=========>....................] - ETA: 21:01 - loss: 2.5216 - regression_loss: 1.9425 - classification_loss: 0.5791
1019/3000 [=========>....................] - ETA: 21:00 - loss: 2.5205 - regression_loss: 1.9416 - classification_loss: 0.5789
1020/3000 [=========>....................] - ETA: 20:59 - loss: 2.5191 - regression_loss: 1.9403 - classification_loss: 0.5788
1021/3000 [=========>....................] - ETA: 20:58 - loss: 2.5189 - regression_loss: 1.9402 - classification_loss: 0.5787
1022/3000 [=========>....................] - ETA: 20:57 - loss: 2.5179 - regression_loss: 1.9394 - classification_loss: 0.5785
1023/3000 [=========>....................] - ETA: 20:56 - loss: 2.5168 - regression_loss: 1.9384 - classification_loss: 0.5784
1024/3000 [=========>....................] - ETA: 20:55 - loss: 2.5155 - regression_loss: 1.9373 - classification_loss: 0.5783
1025/3000 [=========>....................] - ETA: 20:54 - loss: 2.5147 - regression_loss: 1.9365 - classification_loss: 0.5782
1026/3000 [=========>....................] - ETA: 20:53 - loss: 2.5144 - regression_loss: 1.9363 - classification_loss: 0.5780
1027/3000 [=========>....................] - ETA: 20:52 - loss: 2.5134 - regression_loss: 1.9355 - classification_loss: 0.5779
1028/3000 [=========>....................] - ETA: 20:51 - loss: 2.5121 - regression_loss: 1.9344 - classification_loss: 0.5777
1029/3000 [=========>....................] - ETA: 20:49 - loss: 2.5106 - regression_loss: 1.9331 - classification_loss: 0.5775
1030/3000 [=========>....................] - ETA: 20:48 - loss: 2.5096 - regression_loss: 1.9323 - classification_loss: 0.5773
1031/3000 [=========>....................] - ETA: 20:47 - loss: 2.5087 - regression_loss: 1.9316 - classification_loss: 0.5772
1032/3000 [=========>....................] - ETA: 20:46 - loss: 2.5079 - regression_loss: 1.9307 - classification_loss: 0.5771
1033/3000 [=========>....................] - ETA: 20:45 - loss: 2.5072 - regression_loss: 1.9302 - classification_loss: 0.5770
1034/3000 [=========>....................] - ETA: 20:44 - loss: 2.5061 - regression_loss: 1.9293 - classification_loss: 0.5768
1035/3000 [=========>....................] - ETA: 20:43 - loss: 2.5052 - regression_loss: 1.9286 - classification_loss: 0.5766
1036/3000 [=========>....................] - ETA: 20:42 - loss: 2.5042 - regression_loss: 1.9277 - classification_loss: 0.5765
1037/3000 [=========>....................] - ETA: 20:41 - loss: 2.5030 - regression_loss: 1.9266 - classification_loss: 0.5764
1038/3000 [=========>....................] - ETA: 20:40 - loss: 2.5022 - regression_loss: 1.9259 - classification_loss: 0.5763
1039/3000 [=========>....................] - ETA: 20:39 - loss: 2.5009 - regression_loss: 1.9247 - classification_loss: 0.5762
1040/3000 [=========>....................] - ETA: 20:38 - loss: 2.5005 - regression_loss: 1.9244 - classification_loss: 0.5761
1041/3000 [=========>....................] - ETA: 20:37 - loss: 2.4998 - regression_loss: 1.9238 - classification_loss: 0.5760
1042/3000 [=========>....................] - ETA: 20:36 - loss: 2.4986 - regression_loss: 1.9228 - classification_loss: 0.5758
1043/3000 [=========>....................] - ETA: 20:35 - loss: 2.4985 - regression_loss: 1.9227 - classification_loss: 0.5757
1044/3000 [=========>....................] - ETA: 20:34 - loss: 2.4971 - regression_loss: 1.9216 - classification_loss: 0.5755
1045/3000 [=========>....................] - ETA: 20:33 - loss: 2.4962 - regression_loss: 1.9209 - classification_loss: 0.5753
1046/3000 [=========>....................] - ETA: 20:32 - loss: 2.4951 - regression_loss: 1.9200 - classification_loss: 0.5751
1047/3000 [=========>....................] - ETA: 20:31 - loss: 2.4948 - regression_loss: 1.9198 - classification_loss: 0.5750
1048/3000 [=========>....................] - ETA: 20:29 - loss: 2.4934 - regression_loss: 1.9185 - classification_loss: 0.5749
1049/3000 [=========>....................] - ETA: 20:28 - loss: 2.4921 - regression_loss: 1.9174 - classification_loss: 0.5747
1050/3000 [=========>....................] - ETA: 20:27 - loss: 2.4908 - regression_loss: 1.9162 - classification_loss: 0.5745
1051/3000 [=========>....................] - ETA: 20:26 - loss: 2.4895 - regression_loss: 1.9152 - classification_loss: 0.5744
1052/3000 [=========>....................] - ETA: 20:25 - loss: 2.4881 - regression_loss: 1.9139 - classification_loss: 0.5742
1053/3000 [=========>....................] - ETA: 20:24 - loss: 2.4876 - regression_loss: 1.9136 - classification_loss: 0.5740
1054/3000 [=========>....................] - ETA: 20:23 - loss: 2.4863 - regression_loss: 1.9124 - classification_loss: 0.5739
1055/3000 [=========>....................] - ETA: 20:22 - loss: 2.4855 - regression_loss: 1.9117 - classification_loss: 0.5737
1056/3000 [=========>....................] - ETA: 20:21 - loss: 2.4852 - regression_loss: 1.9115 - classification_loss: 0.5736
1057/3000 [=========>....................] - ETA: 20:20 - loss: 2.4839 - regression_loss: 1.9104 - classification_loss: 0.5734
1058/3000 [=========>....................] - ETA: 20:19 - loss: 2.4824 - regression_loss: 1.9091 - classification_loss: 0.5733
1059/3000 [=========>....................] - ETA: 20:18 - loss: 2.4813 - regression_loss: 1.9082 - classification_loss: 0.5731
1060/3000 [=========>....................] - ETA: 20:17 - loss: 2.4809 - regression_loss: 1.9079 - classification_loss: 0.5730
1061/3000 [=========>....................] - ETA: 20:16 - loss: 2.4797 - regression_loss: 1.9068 - classification_loss: 0.5729
1062/3000 [=========>....................] - ETA: 20:15 - loss: 2.4784 - regression_loss: 1.9056 - classification_loss: 0.5728
1063/3000 [=========>....................] - ETA: 20:14 - loss: 2.4774 - regression_loss: 1.9048 - classification_loss: 0.5726
1064/3000 [=========>....................] - ETA: 20:13 - loss: 2.4763 - regression_loss: 1.9038 - classification_loss: 0.5725
1065/3000 [=========>....................] - ETA: 20:12 - loss: 2.4755 - regression_loss: 1.9032 - classification_loss: 0.5722
1066/3000 [=========>....................] - ETA: 20:11 - loss: 2.4749 - regression_loss: 1.9027 - classification_loss: 0.5722
1067/3000 [=========>....................] - ETA: 20:10 - loss: 2.4740 - regression_loss: 1.9020 - classification_loss: 0.5720
1068/3000 [=========>....................] - ETA: 20:09 - loss: 2.4725 - regression_loss: 1.9007 - classification_loss: 0.5718
1069/3000 [=========>....................] - ETA: 20:08 - loss: 2.4712 - regression_loss: 1.8996 - classification_loss: 0.5716
1070/3000 [=========>....................] - ETA: 20:07 - loss: 2.4700 - regression_loss: 1.8985 - classification_loss: 0.5714
1071/3000 [=========>....................] - ETA: 20:06 - loss: 2.4690 - regression_loss: 1.8977 - classification_loss: 0.5713
1072/3000 [=========>....................] - ETA: 20:05 - loss: 2.4685 - regression_loss: 1.8973 - classification_loss: 0.5712
1073/3000 [=========>....................] - ETA: 20:04 - loss: 2.4674 - regression_loss: 1.8963 - classification_loss: 0.5711
1074/3000 [=========>....................] - ETA: 20:03 - loss: 2.4668 - regression_loss: 1.8959 - classification_loss: 0.5709
1075/3000 [=========>....................] - ETA: 20:02 - loss: 2.4659 - regression_loss: 1.8951 - classification_loss: 0.5707
1076/3000 [=========>....................] - ETA: 20:01 - loss: 2.4655 - regression_loss: 1.8948 - classification_loss: 0.5706
1077/3000 [=========>....................] - ETA: 20:00 - loss: 2.4647 - regression_loss: 1.8942 - classification_loss: 0.5705
1078/3000 [=========>....................] - ETA: 19:59 - loss: 2.4640 - regression_loss: 1.8936 - classification_loss: 0.5704
1079/3000 [=========>....................] - ETA: 19:58 - loss: 2.4629 - regression_loss: 1.8926 - classification_loss: 0.5702
1080/3000 [=========>....................] - ETA: 19:57 - loss: 2.4621 - regression_loss: 1.8920 - classification_loss: 0.5702
1081/3000 [=========>....................] - ETA: 19:56 - loss: 2.4609 - regression_loss: 1.8908 - classification_loss: 0.5700
1082/3000 [=========>....................] - ETA: 19:55 - loss: 2.4603 - regression_loss: 1.8903 - classification_loss: 0.5699
1083/3000 [=========>....................] - ETA: 19:54 - loss: 2.4593 - regression_loss: 1.8894 - classification_loss: 0.5699
1084/3000 [=========>....................] - ETA: 19:53 - loss: 2.4584 - regression_loss: 1.8887 - classification_loss: 0.5697
1085/3000 [=========>....................] - ETA: 19:52 - loss: 2.4573 - regression_loss: 1.8877 - classification_loss: 0.5696
1086/3000 [=========>....................] - ETA: 19:51 - loss: 2.4563 - regression_loss: 1.8868 - classification_loss: 0.5695
1087/3000 [=========>....................] - ETA: 19:50 - loss: 2.4550 - regression_loss: 1.8856 - classification_loss: 0.5693
1088/3000 [=========>....................] - ETA: 19:49 - loss: 2.4541 - regression_loss: 1.8848 - classification_loss: 0.5692
1089/3000 [=========>....................] - ETA: 19:48 - loss: 2.4527 - regression_loss: 1.8837 - classification_loss: 0.5690
1090/3000 [=========>....................] - ETA: 19:47 - loss: 2.4516 - regression_loss: 1.8828 - classification_loss: 0.5688
1091/3000 [=========>....................] - ETA: 19:46 - loss: 2.4513 - regression_loss: 1.8826 - classification_loss: 0.5687
1092/3000 [=========>....................] - ETA: 19:45 - loss: 2.4500 - regression_loss: 1.8814 - classification_loss: 0.5686
1093/3000 [=========>....................] - ETA: 19:44 - loss: 2.4488 - regression_loss: 1.8804 - classification_loss: 0.5684
1094/3000 [=========>....................] - ETA: 19:43 - loss: 2.4478 - regression_loss: 1.8795 - classification_loss: 0.5683
1095/3000 [=========>....................] - ETA: 19:42 - loss: 2.4467 - regression_loss: 1.8786 - classification_loss: 0.5681
1096/3000 [=========>....................] - ETA: 19:41 - loss: 2.4464 - regression_loss: 1.8783 - classification_loss: 0.5681
1097/3000 [=========>....................] - ETA: 19:40 - loss: 2.4455 - regression_loss: 1.8775 - classification_loss: 0.5680
1098/3000 [=========>....................] - ETA: 19:39 - loss: 2.4450 - regression_loss: 1.8771 - classification_loss: 0.5679
1099/3000 [=========>....................] - ETA: 19:38 - loss: 2.4439 - regression_loss: 1.8762 - classification_loss: 0.5677
1100/3000 [==========>...................] - ETA: 19:37 - loss: 2.4427 - regression_loss: 1.8751 - classification_loss: 0.5676
1101/3000 [==========>...................] - ETA: 19:36 - loss: 2.4414 - regression_loss: 1.8740 - classification_loss: 0.5674
1102/3000 [==========>...................] - ETA: 19:35 - loss: 2.4402 - regression_loss: 1.8729 - classification_loss: 0.5673
1103/3000 [==========>...................] - ETA: 19:34 - loss: 2.4394 - regression_loss: 1.8721 - classification_loss: 0.5672
1104/3000 [==========>...................] - ETA: 19:33 - loss: 2.4386 - regression_loss: 1.8714 - classification_loss: 0.5672
1105/3000 [==========>...................] - ETA: 19:32 - loss: 2.4377 - regression_loss: 1.8706 - classification_loss: 0.5671
1106/3000 [==========>...................] - ETA: 19:31 - loss: 2.4367 - regression_loss: 1.8697 - classification_loss: 0.5670
1107/3000 [==========>...................] - ETA: 19:30 - loss: 2.4356 - regression_loss: 1.8688 - classification_loss: 0.5668
1108/3000 [==========>...................] - ETA: 19:29 - loss: 2.4344 - regression_loss: 1.8678 - classification_loss: 0.5666
1109/3000 [==========>...................] - ETA: 19:28 - loss: 2.4331 - regression_loss: 1.8667 - classification_loss: 0.5665
1110/3000 [==========>...................] - ETA: 19:27 - loss: 2.4324 - regression_loss: 1.8660 - classification_loss: 0.5663
1111/3000 [==========>...................] - ETA: 19:26 - loss: 2.4311 - regression_loss: 1.8649 - classification_loss: 0.5662
1112/3000 [==========>...................] - ETA: 19:25 - loss: 2.4300 - regression_loss: 1.8639 - classification_loss: 0.5661
1113/3000 [==========>...................] - ETA: 19:24 - loss: 2.4292 - regression_loss: 1.8631 - classification_loss: 0.5660
1114/3000 [==========>...................] - ETA: 19:23 - loss: 2.4284 - regression_loss: 1.8625 - classification_loss: 0.5659
1115/3000 [==========>...................] - ETA: 19:22 - loss: 2.4272 - regression_loss: 1.8615 - classification_loss: 0.5657
1116/3000 [==========>...................] - ETA: 19:21 - loss: 2.4268 - regression_loss: 1.8611 - classification_loss: 0.5656
1117/3000 [==========>...................] - ETA: 19:20 - loss: 2.4257 - regression_loss: 1.8602 - classification_loss: 0.5656
1118/3000 [==========>...................] - ETA: 19:19 - loss: 2.4247 - regression_loss: 1.8592 - classification_loss: 0.5655
1119/3000 [==========>...................] - ETA: 19:18 - loss: 2.4241 - regression_loss: 1.8587 - classification_loss: 0.5654
1120/3000 [==========>...................] - ETA: 19:18 - loss: 2.4230 - regression_loss: 1.8578 - classification_loss: 0.5653
1121/3000 [==========>...................] - ETA: 19:17 - loss: 2.4223 - regression_loss: 1.8572 - classification_loss: 0.5651
1122/3000 [==========>...................] - ETA: 19:16 - loss: 2.4209 - regression_loss: 1.8561 - classification_loss: 0.5648
1123/3000 [==========>...................] - ETA: 19:15 - loss: 2.4200 - regression_loss: 1.8553 - classification_loss: 0.5647
1124/3000 [==========>...................] - ETA: 19:14 - loss: 2.4194 - regression_loss: 1.8547 - classification_loss: 0.5647
1125/3000 [==========>...................] - ETA: 19:13 - loss: 2.4181 - regression_loss: 1.8535 - classification_loss: 0.5645
1126/3000 [==========>...................] - ETA: 19:12 - loss: 2.4176 - regression_loss: 1.8532 - classification_loss: 0.5644
1127/3000 [==========>...................] - ETA: 19:11 - loss: 2.4164 - regression_loss: 1.8522 - classification_loss: 0.5642
1128/3000 [==========>...................] - ETA: 19:10 - loss: 2.4156 - regression_loss: 1.8514 - classification_loss: 0.5642
1129/3000 [==========>...................] - ETA: 19:09 - loss: 2.4149 - regression_loss: 1.8509 - classification_loss: 0.5641
1130/3000 [==========>...................] - ETA: 19:08 - loss: 2.4149 - regression_loss: 1.8509 - classification_loss: 0.5640
1131/3000 [==========>...................] - ETA: 19:07 - loss: 2.4139 - regression_loss: 1.8499 - classification_loss: 0.5640
1132/3000 [==========>...................] - ETA: 19:06 - loss: 2.4130 - regression_loss: 1.8491 - classification_loss: 0.5638
1133/3000 [==========>...................] - ETA: 19:05 - loss: 2.4126 - regression_loss: 1.8488 - classification_loss: 0.5637
1134/3000 [==========>...................] - ETA: 19:04 - loss: 2.4117 - regression_loss: 1.8480 - classification_loss: 0.5637
1135/3000 [==========>...................] - ETA: 19:03 - loss: 2.4109 - regression_loss: 1.8474 - classification_loss: 0.5635
1136/3000 [==========>...................] - ETA: 19:03 - loss: 2.4101 - regression_loss: 1.8467 - classification_loss: 0.5634
1137/3000 [==========>...................] - ETA: 19:02 - loss: 2.4096 - regression_loss: 1.8463 - classification_loss: 0.5632
1138/3000 [==========>...................] - ETA: 19:01 - loss: 2.4088 - regression_loss: 1.8456 - classification_loss: 0.5631
1139/3000 [==========>...................] - ETA: 19:00 - loss: 2.4075 - regression_loss: 1.8445 - classification_loss: 0.5630
1140/3000 [==========>...................] - ETA: 18:59 - loss: 2.4061 - regression_loss: 1.8432 - classification_loss: 0.5629
1141/3000 [==========>...................] - ETA: 18:58 - loss: 2.4047 - regression_loss: 1.8420 - classification_loss: 0.5627
1142/3000 [==========>...................] - ETA: 18:57 - loss: 2.4040 - regression_loss: 1.8415 - classification_loss: 0.5625
1143/3000 [==========>...................] - ETA: 18:56 - loss: 2.4032 - regression_loss: 1.8409 - classification_loss: 0.5623
1144/3000 [==========>...................] - ETA: 18:55 - loss: 2.4019 - regression_loss: 1.8398 - classification_loss: 0.5621
1145/3000 [==========>...................] - ETA: 18:54 - loss: 2.4011 - regression_loss: 1.8391 - classification_loss: 0.5620
1146/3000 [==========>...................] - ETA: 18:53 - loss: 2.4009 - regression_loss: 1.8391 - classification_loss: 0.5619
1147/3000 [==========>...................] - ETA: 18:52 - loss: 2.4000 - regression_loss: 1.8383 - classification_loss: 0.5618
1148/3000 [==========>...................] - ETA: 18:51 - loss: 2.3990 - regression_loss: 1.8373 - classification_loss: 0.5616
1149/3000 [==========>...................] - ETA: 18:50 - loss: 2.3977 - regression_loss: 1.8362 - classification_loss: 0.5615
1150/3000 [==========>...................] - ETA: 18:49 - loss: 2.3968 - regression_loss: 1.8355 - classification_loss: 0.5613
1151/3000 [==========>...................] - ETA: 18:49 - loss: 2.3956 - regression_loss: 1.8343 - classification_loss: 0.5613
1152/3000 [==========>...................] - ETA: 18:48 - loss: 2.3943 - regression_loss: 1.8332 - classification_loss: 0.5611
1153/3000 [==========>...................] - ETA: 18:47 - loss: 2.3936 - regression_loss: 1.8326 - classification_loss: 0.5610
1154/3000 [==========>...................] - ETA: 18:46 - loss: 2.3923 - regression_loss: 1.8313 - classification_loss: 0.5609
1155/3000 [==========>...................] - ETA: 18:45 - loss: 2.3914 - regression_loss: 1.8305 - classification_loss: 0.5609
1156/3000 [==========>...................] - ETA: 18:44 - loss: 2.3911 - regression_loss: 1.8302 - classification_loss: 0.5608
1157/3000 [==========>...................] - ETA: 18:43 - loss: 2.3903 - regression_loss: 1.8296 - classification_loss: 0.5607
1158/3000 [==========>...................] - ETA: 18:42 - loss: 2.3892 - regression_loss: 1.8287 - classification_loss: 0.5606
1159/3000 [==========>...................] - ETA: 18:41 - loss: 2.3886 - regression_loss: 1.8281 - classification_loss: 0.5605
1160/3000 [==========>...................] - ETA: 18:40 - loss: 2.3877 - regression_loss: 1.8273 - classification_loss: 0.5604
1161/3000 [==========>...................] - ETA: 18:39 - loss: 2.3869 - regression_loss: 1.8266 - classification_loss: 0.5603
1162/3000 [==========>...................] - ETA: 18:38 - loss: 2.3861 - regression_loss: 1.8259 - classification_loss: 0.5601
1163/3000 [==========>...................] - ETA: 18:37 - loss: 2.3854 - regression_loss: 1.8254 - classification_loss: 0.5600
1164/3000 [==========>...................] - ETA: 18:37 - loss: 2.3846 - regression_loss: 1.8247 - classification_loss: 0.5599
1165/3000 [==========>...................] - ETA: 18:36 - loss: 2.3835 - regression_loss: 1.8237 - classification_loss: 0.5598
1166/3000 [==========>...................] - ETA: 18:35 - loss: 2.3824 - regression_loss: 1.8227 - classification_loss: 0.5597
1167/3000 [==========>...................] - ETA: 18:34 - loss: 2.3814 - regression_loss: 1.8219 - classification_loss: 0.5596
1168/3000 [==========>...................] - ETA: 18:33 - loss: 2.3811 - regression_loss: 1.8217 - classification_loss: 0.5595
1169/3000 [==========>...................] - ETA: 18:32 - loss: 2.3806 - regression_loss: 1.8212 - classification_loss: 0.5594
1170/3000 [==========>...................] - ETA: 18:31 - loss: 2.3802 - regression_loss: 1.8209 - classification_loss: 0.5593
1171/3000 [==========>...................] - ETA: 18:30 - loss: 2.3791 - regression_loss: 1.8200 - classification_loss: 0.5591
1172/3000 [==========>...................] - ETA: 18:29 - loss: 2.3788 - regression_loss: 1.8197 - classification_loss: 0.5590
1173/3000 [==========>...................] - ETA: 18:28 - loss: 2.3777 - regression_loss: 1.8188 - classification_loss: 0.5589
1174/3000 [==========>...................] - ETA: 18:27 - loss: 2.3768 - regression_loss: 1.8181 - classification_loss: 0.5587
1175/3000 [==========>...................] - ETA: 18:27 - loss: 2.3757 - regression_loss: 1.8172 - classification_loss: 0.5585
1176/3000 [==========>...................] - ETA: 18:26 - loss: 2.3747 - regression_loss: 1.8163 - classification_loss: 0.5584
1177/3000 [==========>...................] - ETA: 18:25 - loss: 2.3741 - regression_loss: 1.8159 - classification_loss: 0.5583
1178/3000 [==========>...................] - ETA: 18:24 - loss: 2.3735 - regression_loss: 1.8153 - classification_loss: 0.5582
1179/3000 [==========>...................] - ETA: 18:23 - loss: 2.3723 - regression_loss: 1.8143 - classification_loss: 0.5580
1180/3000 [==========>...................] - ETA: 18:22 - loss: 2.3720 - regression_loss: 1.8141 - classification_loss: 0.5579
1181/3000 [==========>...................] - ETA: 18:21 - loss: 2.3717 - regression_loss: 1.8139 - classification_loss: 0.5578
1182/3000 [==========>...................] - ETA: 18:20 - loss: 2.3706 - regression_loss: 1.8129 - classification_loss: 0.5577
1183/3000 [==========>...................] - ETA: 18:19 - loss: 2.3693 - regression_loss: 1.8118 - classification_loss: 0.5576
1184/3000 [==========>...................] - ETA: 18:18 - loss: 2.3683 - regression_loss: 1.8109 - classification_loss: 0.5574
1185/3000 [==========>...................] - ETA: 18:17 - loss: 2.3675 - regression_loss: 1.8102 - classification_loss: 0.5573
1186/3000 [==========>...................] - ETA: 18:17 - loss: 2.3667 - regression_loss: 1.8096 - classification_loss: 0.5571
1187/3000 [==========>...................] - ETA: 18:16 - loss: 2.3658 - regression_loss: 1.8087 - classification_loss: 0.5571
1188/3000 [==========>...................] - ETA: 18:15 - loss: 2.3647 - regression_loss: 1.8077 - classification_loss: 0.5570
1189/3000 [==========>...................] - ETA: 18:14 - loss: 2.3641 - regression_loss: 1.8071 - classification_loss: 0.5570
1190/3000 [==========>...................] - ETA: 18:13 - loss: 2.3631 - regression_loss: 1.8062 - classification_loss: 0.5569
1191/3000 [==========>...................] - ETA: 18:12 - loss: 2.3622 - regression_loss: 1.8055 - classification_loss: 0.5567
1192/3000 [==========>...................] - ETA: 18:11 - loss: 2.3614 - regression_loss: 1.8048 - classification_loss: 0.5565
1193/3000 [==========>...................] - ETA: 18:10 - loss: 2.3605 - regression_loss: 1.8040 - classification_loss: 0.5565
1194/3000 [==========>...................] - ETA: 18:09 - loss: 2.3594 - regression_loss: 1.8030 - classification_loss: 0.5564
1195/3000 [==========>...................] - ETA: 18:09 - loss: 2.3584 - regression_loss: 1.8022 - classification_loss: 0.5562
1196/3000 [==========>...................] - ETA: 18:08 - loss: 2.3577 - regression_loss: 1.8017 - classification_loss: 0.5561
1197/3000 [==========>...................] - ETA: 18:07 - loss: 2.3569 - regression_loss: 1.8010 - classification_loss: 0.5560
1198/3000 [==========>...................] - ETA: 18:06 - loss: 2.3564 - regression_loss: 1.8006 - classification_loss: 0.5559
1199/3000 [==========>...................] - ETA: 18:05 - loss: 2.3555 - regression_loss: 1.7997 - classification_loss: 0.5557
1200/3000 [===========>..................] - ETA: 18:04 - loss: 2.3545 - regression_loss: 1.7988 - classification_loss: 0.5557
1201/3000 [===========>..................] - ETA: 18:03 - loss: 2.3538 - regression_loss: 1.7982 - classification_loss: 0.5555
1202/3000 [===========>..................] - ETA: 18:02 - loss: 2.3526 - regression_loss: 1.7972 - classification_loss: 0.5554
1203/3000 [===========>..................] - ETA: 18:01 - loss: 2.3521 - regression_loss: 1.7969 - classification_loss: 0.5552
1204/3000 [===========>..................] - ETA: 18:01 - loss: 2.3519 - regression_loss: 1.7967 - classification_loss: 0.5552
1205/3000 [===========>..................] - ETA: 18:00 - loss: 2.3511 - regression_loss: 1.7959 - classification_loss: 0.5552
1206/3000 [===========>..................] - ETA: 17:59 - loss: 2.3502 - regression_loss: 1.7951 - classification_loss: 0.5551
1207/3000 [===========>..................] - ETA: 17:58 - loss: 2.3494 - regression_loss: 1.7944 - classification_loss: 0.5550
1208/3000 [===========>..................] - ETA: 17:57 - loss: 2.3485 - regression_loss: 1.7936 - classification_loss: 0.5548
1209/3000 [===========>..................] - ETA: 17:56 - loss: 2.3478 - regression_loss: 1.7931 - classification_loss: 0.5547
1210/3000 [===========>..................] - ETA: 17:55 - loss: 2.3466 - regression_loss: 1.7921 - classification_loss: 0.5545
1211/3000 [===========>..................] - ETA: 17:54 - loss: 2.3461 - regression_loss: 1.7917 - classification_loss: 0.5544
1212/3000 [===========>..................] - ETA: 17:53 - loss: 2.3454 - regression_loss: 1.7912 - classification_loss: 0.5543
1213/3000 [===========>..................] - ETA: 17:52 - loss: 2.3445 - regression_loss: 1.7904 - classification_loss: 0.5542
1214/3000 [===========>..................] - ETA: 17:52 - loss: 2.3435 - regression_loss: 1.7896 - classification_loss: 0.5540
1215/3000 [===========>..................] - ETA: 17:51 - loss: 2.3425 - regression_loss: 1.7886 - classification_loss: 0.5539
1216/3000 [===========>..................] - ETA: 17:50 - loss: 2.3416 - regression_loss: 1.7878 - classification_loss: 0.5538
1217/3000 [===========>..................] - ETA: 17:49 - loss: 2.3407 - regression_loss: 1.7871 - classification_loss: 0.5536
1218/3000 [===========>..................] - ETA: 17:48 - loss: 2.3402 - regression_loss: 1.7867 - classification_loss: 0.5535
1219/3000 [===========>..................] - ETA: 17:47 - loss: 2.3393 - regression_loss: 1.7859 - classification_loss: 0.5534
1220/3000 [===========>..................] - ETA: 17:46 - loss: 2.3384 - regression_loss: 1.7851 - classification_loss: 0.5533
1221/3000 [===========>..................] - ETA: 17:46 - loss: 2.3378 - regression_loss: 1.7846 - classification_loss: 0.5532
1222/3000 [===========>..................] - ETA: 17:45 - loss: 2.3371 - regression_loss: 1.7840 - classification_loss: 0.5531
1223/3000 [===========>..................] - ETA: 17:44 - loss: 2.3361 - regression_loss: 1.7831 - classification_loss: 0.5530
1224/3000 [===========>..................] - ETA: 17:43 - loss: 2.3359 - regression_loss: 1.7830 - classification_loss: 0.5529
1225/3000 [===========>..................] - ETA: 17:42 - loss: 2.3349 - regression_loss: 1.7820 - classification_loss: 0.5528
1226/3000 [===========>..................] - ETA: 17:41 - loss: 2.3342 - regression_loss: 1.7815 - classification_loss: 0.5527
1227/3000 [===========>..................] - ETA: 17:40 - loss: 2.3331 - regression_loss: 1.7806 - classification_loss: 0.5525
1228/3000 [===========>..................] - ETA: 17:39 - loss: 2.3327 - regression_loss: 1.7803 - classification_loss: 0.5524
1229/3000 [===========>..................] - ETA: 17:39 - loss: 2.3319 - regression_loss: 1.7796 - classification_loss: 0.5523
1230/3000 [===========>..................] - ETA: 17:38 - loss: 2.3309 - regression_loss: 1.7787 - classification_loss: 0.5522
1231/3000 [===========>..................] - ETA: 17:37 - loss: 2.3300 - regression_loss: 1.7779 - classification_loss: 0.5521
1232/3000 [===========>..................] - ETA: 17:36 - loss: 2.3293 - regression_loss: 1.7773 - classification_loss: 0.5521
1233/3000 [===========>..................] - ETA: 17:35 - loss: 2.3284 - regression_loss: 1.7764 - classification_loss: 0.5520
1234/3000 [===========>..................] - ETA: 17:34 - loss: 2.3274 - regression_loss: 1.7755 - classification_loss: 0.5519
1235/3000 [===========>..................] - ETA: 17:33 - loss: 2.3268 - regression_loss: 1.7750 - classification_loss: 0.5518
1236/3000 [===========>..................] - ETA: 17:32 - loss: 2.3258 - regression_loss: 1.7741 - classification_loss: 0.5517
1237/3000 [===========>..................] - ETA: 17:32 - loss: 2.3247 - regression_loss: 1.7732 - classification_loss: 0.5515
1238/3000 [===========>..................] - ETA: 17:31 - loss: 2.3237 - regression_loss: 1.7723 - classification_loss: 0.5514
1239/3000 [===========>..................] - ETA: 17:30 - loss: 2.3230 - regression_loss: 1.7717 - classification_loss: 0.5513
1240/3000 [===========>..................] - ETA: 17:29 - loss: 2.3220 - regression_loss: 1.7708 - classification_loss: 0.5512
1241/3000 [===========>..................] - ETA: 17:28 - loss: 2.3214 - regression_loss: 1.7703 - classification_loss: 0.5510
1242/3000 [===========>..................] - ETA: 17:27 - loss: 2.3204 - regression_loss: 1.7695 - classification_loss: 0.5509
1243/3000 [===========>..................] - ETA: 17:26 - loss: 2.3195 - regression_loss: 1.7688 - classification_loss: 0.5508
1244/3000 [===========>..................] - ETA: 17:26 - loss: 2.3188 - regression_loss: 1.7682 - classification_loss: 0.5506
1245/3000 [===========>..................] - ETA: 17:25 - loss: 2.3176 - regression_loss: 1.7671 - classification_loss: 0.5505
1246/3000 [===========>..................] - ETA: 17:24 - loss: 2.3167 - regression_loss: 1.7663 - classification_loss: 0.5504
1247/3000 [===========>..................] - ETA: 17:23 - loss: 2.3161 - regression_loss: 1.7659 - classification_loss: 0.5502
1248/3000 [===========>..................] - ETA: 17:22 - loss: 2.3152 - regression_loss: 1.7651 - classification_loss: 0.5501
1249/3000 [===========>..................] - ETA: 17:21 - loss: 2.3146 - regression_loss: 1.7646 - classification_loss: 0.5500
1250/3000 [===========>..................] - ETA: 17:20 - loss: 2.3138 - regression_loss: 1.7639 - classification_loss: 0.5499
1251/3000 [===========>..................] - ETA: 17:20 - loss: 2.3134 - regression_loss: 1.7637 - classification_loss: 0.5498
1252/3000 [===========>..................] - ETA: 17:19 - loss: 2.3125 - regression_loss: 1.7628 - classification_loss: 0.5496
1253/3000 [===========>..................] - ETA: 17:18 - loss: 2.3117 - regression_loss: 1.7621 - classification_loss: 0.5495
1254/3000 [===========>..................] - ETA: 17:17 - loss: 2.3106 - regression_loss: 1.7612 - classification_loss: 0.5494
1255/3000 [===========>..................] - ETA: 17:16 - loss: 2.3097 - regression_loss: 1.7604 - classification_loss: 0.5493
1256/3000 [===========>..................] - ETA: 17:15 - loss: 2.3090 - regression_loss: 1.7598 - classification_loss: 0.5491
1257/3000 [===========>..................] - ETA: 17:14 - loss: 2.3079 - regression_loss: 1.7589 - classification_loss: 0.5490
1258/3000 [===========>..................] - ETA: 17:14 - loss: 2.3074 - regression_loss: 1.7584 - classification_loss: 0.5489
1259/3000 [===========>..................] - ETA: 17:13 - loss: 2.3067 - regression_loss: 1.7579 - classification_loss: 0.5488
1260/3000 [===========>..................] - ETA: 17:12 - loss: 2.3060 - regression_loss: 1.7573 - classification_loss: 0.5487
1261/3000 [===========>..................] - ETA: 17:11 - loss: 2.3049 - regression_loss: 1.7564 - classification_loss: 0.5485
1262/3000 [===========>..................] - ETA: 17:10 - loss: 2.3039 - regression_loss: 1.7554 - classification_loss: 0.5484
1263/3000 [===========>..................] - ETA: 17:09 - loss: 2.3030 - regression_loss: 1.7546 - classification_loss: 0.5484
1264/3000 [===========>..................] - ETA: 17:09 - loss: 2.3028 - regression_loss: 1.7546 - classification_loss: 0.5483
1265/3000 [===========>..................] - ETA: 17:08 - loss: 2.3021 - regression_loss: 1.7539 - classification_loss: 0.5482
1266/3000 [===========>..................] - ETA: 17:07 - loss: 2.3011 - regression_loss: 1.7531 - classification_loss: 0.5480
1267/3000 [===========>..................] - ETA: 17:06 - loss: 2.3005 - regression_loss: 1.7526 - classification_loss: 0.5480
1268/3000 [===========>..................] - ETA: 17:05 - loss: 2.2996 - regression_loss: 1.7518 - classification_loss: 0.5479
1269/3000 [===========>..................] - ETA: 17:04 - loss: 2.2986 - regression_loss: 1.7508 - classification_loss: 0.5477
1270/3000 [===========>..................] - ETA: 17:04 - loss: 2.2977 - regression_loss: 1.7500 - classification_loss: 0.5476
1271/3000 [===========>..................] - ETA: 17:03 - loss: 2.2972 - regression_loss: 1.7496 - classification_loss: 0.5476
1272/3000 [===========>..................] - ETA: 17:02 - loss: 2.2961 - regression_loss: 1.7487 - classification_loss: 0.5474
1273/3000 [===========>..................] - ETA: 17:01 - loss: 2.2950 - regression_loss: 1.7477 - classification_loss: 0.5473
1274/3000 [===========>..................] - ETA: 17:00 - loss: 2.2943 - regression_loss: 1.7470 - classification_loss: 0.5473
1275/3000 [===========>..................] - ETA: 16:59 - loss: 2.2933 - regression_loss: 1.7462 - classification_loss: 0.5471
1276/3000 [===========>..................] - ETA: 16:59 - loss: 2.2922 - regression_loss: 1.7453 - classification_loss: 0.5469
1277/3000 [===========>..................] - ETA: 16:58 - loss: 2.2920 - regression_loss: 1.7452 - classification_loss: 0.5468
1278/3000 [===========>..................] - ETA: 16:57 - loss: 2.2910 - regression_loss: 1.7443 - classification_loss: 0.5467
1279/3000 [===========>..................] - ETA: 16:56 - loss: 2.2902 - regression_loss: 1.7436 - classification_loss: 0.5466
1280/3000 [===========>..................] - ETA: 16:55 - loss: 2.2893 - regression_loss: 1.7428 - classification_loss: 0.5465
1281/3000 [===========>..................] - ETA: 16:54 - loss: 2.2885 - regression_loss: 1.7421 - classification_loss: 0.5464
1282/3000 [===========>..................] - ETA: 16:54 - loss: 2.2878 - regression_loss: 1.7415 - classification_loss: 0.5463
1283/3000 [===========>..................] - ETA: 16:53 - loss: 2.2867 - regression_loss: 1.7406 - classification_loss: 0.5462
1284/3000 [===========>..................] - ETA: 16:52 - loss: 2.2857 - regression_loss: 1.7396 - classification_loss: 0.5460
1285/3000 [===========>..................] - ETA: 16:51 - loss: 2.2850 - regression_loss: 1.7392 - classification_loss: 0.5459
1286/3000 [===========>..................] - ETA: 16:50 - loss: 2.2842 - regression_loss: 1.7384 - classification_loss: 0.5458
1287/3000 [===========>..................] - ETA: 16:50 - loss: 2.2836 - regression_loss: 1.7379 - classification_loss: 0.5457
1288/3000 [===========>..................] - ETA: 16:49 - loss: 2.2825 - regression_loss: 1.7370 - classification_loss: 0.5456
1289/3000 [===========>..................] - ETA: 16:48 - loss: 2.2818 - regression_loss: 1.7364 - classification_loss: 0.5455
1290/3000 [===========>..................] - ETA: 16:47 - loss: 2.2809 - regression_loss: 1.7356 - classification_loss: 0.5453
1291/3000 [===========>..................] - ETA: 16:46 - loss: 2.2801 - regression_loss: 1.7348 - classification_loss: 0.5453
1292/3000 [===========>..................] - ETA: 16:45 - loss: 2.2792 - regression_loss: 1.7340 - classification_loss: 0.5452
1293/3000 [===========>..................] - ETA: 16:45 - loss: 2.2782 - regression_loss: 1.7331 - classification_loss: 0.5451
1294/3000 [===========>..................] - ETA: 16:44 - loss: 2.2772 - regression_loss: 1.7322 - classification_loss: 0.5450
1295/3000 [===========>..................] - ETA: 16:43 - loss: 2.2761 - regression_loss: 1.7313 - classification_loss: 0.5448
1296/3000 [===========>..................] - ETA: 16:42 - loss: 2.2750 - regression_loss: 1.7304 - classification_loss: 0.5446
1297/3000 [===========>..................] - ETA: 16:41 - loss: 2.2744 - regression_loss: 1.7298 - classification_loss: 0.5445
1298/3000 [===========>..................] - ETA: 16:40 - loss: 2.2738 - regression_loss: 1.7293 - classification_loss: 0.5445
1299/3000 [===========>..................] - ETA: 16:40 - loss: 2.2729 - regression_loss: 1.7286 - classification_loss: 0.5444
1300/3000 [============>.................] - ETA: 16:39 - loss: 2.2719 - regression_loss: 1.7277 - classification_loss: 0.5442
1301/3000 [============>.................] - ETA: 16:38 - loss: 2.2711 - regression_loss: 1.7269 - classification_loss: 0.5441
1302/3000 [============>.................] - ETA: 16:37 - loss: 2.2703 - regression_loss: 1.7262 - classification_loss: 0.5440
1303/3000 [============>.................] - ETA: 16:36 - loss: 2.2693 - regression_loss: 1.7254 - classification_loss: 0.5439
1304/3000 [============>.................] - ETA: 16:36 - loss: 2.2684 - regression_loss: 1.7246 - classification_loss: 0.5438
1305/3000 [============>.................] - ETA: 16:35 - loss: 2.2674 - regression_loss: 1.7237 - classification_loss: 0.5437
1306/3000 [============>.................] - ETA: 16:34 - loss: 2.2670 - regression_loss: 1.7234 - classification_loss: 0.5436
1307/3000 [============>.................] - ETA: 16:33 - loss: 2.2663 - regression_loss: 1.7228 - classification_loss: 0.5435
1308/3000 [============>.................] - ETA: 16:32 - loss: 2.2654 - regression_loss: 1.7220 - classification_loss: 0.5434
1309/3000 [============>.................] - ETA: 16:31 - loss: 2.2649 - regression_loss: 1.7216 - classification_loss: 0.5433
1310/3000 [============>.................] - ETA: 16:31 - loss: 2.2643 - regression_loss: 1.7211 - classification_loss: 0.5432
1311/3000 [============>.................] - ETA: 16:30 - loss: 2.2636 - regression_loss: 1.7206 - classification_loss: 0.5430
1312/3000 [============>.................] - ETA: 16:29 - loss: 2.2628 - regression_loss: 1.7199 - classification_loss: 0.5429
1313/3000 [============>.................] - ETA: 16:28 - loss: 2.2625 - regression_loss: 1.7196 - classification_loss: 0.5428
1314/3000 [============>.................] - ETA: 16:27 - loss: 2.2615 - regression_loss: 1.7188 - classification_loss: 0.5427
1315/3000 [============>.................] - ETA: 16:27 - loss: 2.2605 - regression_loss: 1.7179 - classification_loss: 0.5425
1316/3000 [============>.................] - ETA: 16:26 - loss: 2.2597 - regression_loss: 1.7173 - classification_loss: 0.5424
1317/3000 [============>.................] - ETA: 16:25 - loss: 2.2588 - regression_loss: 1.7165 - classification_loss: 0.5423
1318/3000 [============>.................] - ETA: 16:24 - loss: 2.2580 - regression_loss: 1.7159 - classification_loss: 0.5422
1319/3000 [============>.................] - ETA: 16:23 - loss: 2.2571 - regression_loss: 1.7150 - classification_loss: 0.5421
1320/3000 [============>.................] - ETA: 16:23 - loss: 2.2560 - regression_loss: 1.7141 - classification_loss: 0.5419
1321/3000 [============>.................] - ETA: 16:22 - loss: 2.2551 - regression_loss: 1.7133 - classification_loss: 0.5418
1322/3000 [============>.................] - ETA: 16:21 - loss: 2.2541 - regression_loss: 1.7124 - classification_loss: 0.5417
1323/3000 [============>.................] - ETA: 16:20 - loss: 2.2534 - regression_loss: 1.7118 - classification_loss: 0.5415
1324/3000 [============>.................] - ETA: 16:19 - loss: 2.2524 - regression_loss: 1.7110 - classification_loss: 0.5414
1325/3000 [============>.................] - ETA: 16:19 - loss: 2.2515 - regression_loss: 1.7102 - classification_loss: 0.5413
1326/3000 [============>.................] - ETA: 16:18 - loss: 2.2506 - regression_loss: 1.7095 - classification_loss: 0.5411
1327/3000 [============>.................] - ETA: 16:17 - loss: 2.2499 - regression_loss: 1.7088 - classification_loss: 0.5411
1328/3000 [============>.................] - ETA: 16:16 - loss: 2.2493 - regression_loss: 1.7083 - classification_loss: 0.5410
1329/3000 [============>.................] - ETA: 16:15 - loss: 2.2483 - regression_loss: 1.7074 - classification_loss: 0.5409
1330/3000 [============>.................] - ETA: 16:15 - loss: 2.2476 - regression_loss: 1.7068 - classification_loss: 0.5408
1331/3000 [============>.................] - ETA: 16:14 - loss: 2.2466 - regression_loss: 1.7060 - classification_loss: 0.5406
1332/3000 [============>.................] - ETA: 16:13 - loss: 2.2460 - regression_loss: 1.7056 - classification_loss: 0.5405
1333/3000 [============>.................] - ETA: 16:12 - loss: 2.2451 - regression_loss: 1.7049 - classification_loss: 0.5403
1334/3000 [============>.................] - ETA: 16:11 - loss: 2.2447 - regression_loss: 1.7046 - classification_loss: 0.5401
1335/3000 [============>.................] - ETA: 16:10 - loss: 2.2447 - regression_loss: 1.7046 - classification_loss: 0.5401
1336/3000 [============>.................] - ETA: 16:10 - loss: 2.2439 - regression_loss: 1.7040 - classification_loss: 0.5400
1337/3000 [============>.................] - ETA: 16:09 - loss: 2.2435 - regression_loss: 1.7036 - classification_loss: 0.5399
1338/3000 [============>.................] - ETA: 16:08 - loss: 2.2426 - regression_loss: 1.7028 - classification_loss: 0.5397
1339/3000 [============>.................] - ETA: 16:07 - loss: 2.2423 - regression_loss: 1.7027 - classification_loss: 0.5397
1340/3000 [============>.................] - ETA: 16:06 - loss: 2.2416 - regression_loss: 1.7019 - classification_loss: 0.5396
1341/3000 [============>.................] - ETA: 16:06 - loss: 2.2413 - regression_loss: 1.7017 - classification_loss: 0.5396
1342/3000 [============>.................] - ETA: 16:05 - loss: 2.2405 - regression_loss: 1.7011 - classification_loss: 0.5395
1343/3000 [============>.................] - ETA: 16:04 - loss: 2.2398 - regression_loss: 1.7004 - classification_loss: 0.5393
1344/3000 [============>.................] - ETA: 16:03 - loss: 2.2394 - regression_loss: 1.7001 - classification_loss: 0.5393
1345/3000 [============>.................] - ETA: 16:03 - loss: 2.2384 - regression_loss: 1.6993 - classification_loss: 0.5391
1346/3000 [============>.................] - ETA: 16:02 - loss: 2.2377 - regression_loss: 1.6985 - classification_loss: 0.5392
1347/3000 [============>.................] - ETA: 16:01 - loss: 2.2373 - regression_loss: 1.6982 - classification_loss: 0.5391
1348/3000 [============>.................] - ETA: 16:00 - loss: 2.2370 - regression_loss: 1.6980 - classification_loss: 0.5390
1349/3000 [============>.................] - ETA: 15:59 - loss: 2.2364 - regression_loss: 1.6976 - classification_loss: 0.5389
1350/3000 [============>.................] - ETA: 15:59 - loss: 2.2358 - regression_loss: 1.6970 - classification_loss: 0.5387
1351/3000 [============>.................] - ETA: 15:58 - loss: 2.2353 - regression_loss: 1.6966 - classification_loss: 0.5387
1352/3000 [============>.................] - ETA: 15:57 - loss: 2.2345 - regression_loss: 1.6959 - classification_loss: 0.5386
1353/3000 [============>.................] - ETA: 15:56 - loss: 2.2336 - regression_loss: 1.6951 - classification_loss: 0.5385
1354/3000 [============>.................] - ETA: 15:55 - loss: 2.2333 - regression_loss: 1.6949 - classification_loss: 0.5383
1355/3000 [============>.................] - ETA: 15:55 - loss: 2.2323 - regression_loss: 1.6941 - classification_loss: 0.5382
1356/3000 [============>.................] - ETA: 15:54 - loss: 2.2315 - regression_loss: 1.6934 - classification_loss: 0.5381
1357/3000 [============>.................] - ETA: 15:53 - loss: 2.2309 - regression_loss: 1.6929 - classification_loss: 0.5380
1358/3000 [============>.................] - ETA: 15:52 - loss: 2.2300 - regression_loss: 1.6922 - classification_loss: 0.5379
1359/3000 [============>.................] - ETA: 15:52 - loss: 2.2293 - regression_loss: 1.6915 - classification_loss: 0.5378
1360/3000 [============>.................] - ETA: 15:51 - loss: 2.2288 - regression_loss: 1.6911 - classification_loss: 0.5378
1361/3000 [============>.................] - ETA: 15:50 - loss: 2.2279 - regression_loss: 1.6902 - classification_loss: 0.5377
1362/3000 [============>.................] - ETA: 15:49 - loss: 2.2270 - regression_loss: 1.6894 - classification_loss: 0.5376
1363/3000 [============>.................] - ETA: 15:48 - loss: 2.2267 - regression_loss: 1.6892 - classification_loss: 0.5375
1364/3000 [============>.................] - ETA: 15:48 - loss: 2.2264 - regression_loss: 1.6887 - classification_loss: 0.5376
1365/3000 [============>.................] - ETA: 15:47 - loss: 2.2256 - regression_loss: 1.6881 - classification_loss: 0.5375
1366/3000 [============>.................] - ETA: 15:46 - loss: 2.2250 - regression_loss: 1.6876 - classification_loss: 0.5374
1367/3000 [============>.................] - ETA: 15:45 - loss: 2.2243 - regression_loss: 1.6870 - classification_loss: 0.5373
1368/3000 [============>.................] - ETA: 15:45 - loss: 2.2236 - regression_loss: 1.6864 - classification_loss: 0.5372
1369/3000 [============>.................] - ETA: 15:44 - loss: 2.2226 - regression_loss: 1.6856 - classification_loss: 0.5370
1370/3000 [============>.................] - ETA: 15:43 - loss: 2.2218 - regression_loss: 1.6848 - classification_loss: 0.5370
1371/3000 [============>.................] - ETA: 15:42 - loss: 2.2212 - regression_loss: 1.6843 - classification_loss: 0.5369
1372/3000 [============>.................] - ETA: 15:41 - loss: 2.2208 - regression_loss: 1.6840 - classification_loss: 0.5368
1373/3000 [============>.................] - ETA: 15:41 - loss: 2.2200 - regression_loss: 1.6833 - classification_loss: 0.5367
1374/3000 [============>.................] - ETA: 15:40 - loss: 2.2197 - regression_loss: 1.6830 - classification_loss: 0.5367
1375/3000 [============>.................] - ETA: 15:39 - loss: 2.2186 - regression_loss: 1.6821 - classification_loss: 0.5365
1376/3000 [============>.................] - ETA: 15:38 - loss: 2.2181 - regression_loss: 1.6817 - classification_loss: 0.5365
1377/3000 [============>.................] - ETA: 15:37 - loss: 2.2177 - regression_loss: 1.6814 - classification_loss: 0.5363
1378/3000 [============>.................] - ETA: 15:37 - loss: 2.2170 - regression_loss: 1.6807 - classification_loss: 0.5363
1379/3000 [============>.................] - ETA: 15:36 - loss: 2.2162 - regression_loss: 1.6800 - classification_loss: 0.5362
1380/3000 [============>.................] - ETA: 15:35 - loss: 2.2155 - regression_loss: 1.6794 - classification_loss: 0.5361
1381/3000 [============>.................] - ETA: 15:34 - loss: 2.2147 - regression_loss: 1.6787 - classification_loss: 0.5360
1382/3000 [============>.................] - ETA: 15:34 - loss: 2.2145 - regression_loss: 1.6785 - classification_loss: 0.5359
1383/3000 [============>.................] - ETA: 15:33 - loss: 2.2142 - regression_loss: 1.6783 - classification_loss: 0.5359
1384/3000 [============>.................] - ETA: 15:32 - loss: 2.2135 - regression_loss: 1.6778 - classification_loss: 0.5357
1385/3000 [============>.................] - ETA: 15:31 - loss: 2.2129 - regression_loss: 1.6773 - classification_loss: 0.5356
1386/3000 [============>.................] - ETA: 15:31 - loss: 2.2121 - regression_loss: 1.6766 - classification_loss: 0.5355
1387/3000 [============>.................] - ETA: 15:30 - loss: 2.2113 - regression_loss: 1.6759 - classification_loss: 0.5354
1388/3000 [============>.................] - ETA: 15:29 - loss: 2.2105 - regression_loss: 1.6753 - classification_loss: 0.5352
1389/3000 [============>.................] - ETA: 15:28 - loss: 2.2099 - regression_loss: 1.6748 - classification_loss: 0.5351
1390/3000 [============>.................] - ETA: 15:28 - loss: 2.2093 - regression_loss: 1.6743 - classification_loss: 0.5350
1391/3000 [============>.................] - ETA: 15:27 - loss: 2.2085 - regression_loss: 1.6736 - classification_loss: 0.5349
1392/3000 [============>.................] - ETA: 15:26 - loss: 2.2080 - regression_loss: 1.6732 - classification_loss: 0.5348
1393/3000 [============>.................] - ETA: 15:25 - loss: 2.2073 - regression_loss: 1.6725 - classification_loss: 0.5347
1394/3000 [============>.................] - ETA: 15:24 - loss: 2.2068 - regression_loss: 1.6721 - classification_loss: 0.5347
1395/3000 [============>.................] - ETA: 15:24 - loss: 2.2060 - regression_loss: 1.6714 - classification_loss: 0.5346
1396/3000 [============>.................] - ETA: 15:23 - loss: 2.2052 - regression_loss: 1.6707 - classification_loss: 0.5346
1397/3000 [============>.................] - ETA: 15:22 - loss: 2.2043 - regression_loss: 1.6699 - classification_loss: 0.5345
1398/3000 [============>.................] - ETA: 15:21 - loss: 2.2035 - regression_loss: 1.6691 - classification_loss: 0.5344
1399/3000 [============>.................] - ETA: 15:21 - loss: 2.2029 - regression_loss: 1.6686 - classification_loss: 0.5343
1400/3000 [=============>................] - ETA: 15:20 - loss: 2.2027 - regression_loss: 1.6685 - classification_loss: 0.5343
1401/3000 [=============>................] - ETA: 15:19 - loss: 2.2021 - regression_loss: 1.6680 - classification_loss: 0.5341
1402/3000 [=============>................] - ETA: 15:18 - loss: 2.2018 - regression_loss: 1.6677 - classification_loss: 0.5341
1403/3000 [=============>................] - ETA: 15:18 - loss: 2.2010 - regression_loss: 1.6669 - classification_loss: 0.5340
1404/3000 [=============>................] - ETA: 15:17 - loss: 2.2003 - regression_loss: 1.6663 - classification_loss: 0.5340
1405/3000 [=============>................] - ETA: 15:16 - loss: 2.1996 - regression_loss: 1.6657 - classification_loss: 0.5339
1406/3000 [=============>................] - ETA: 15:15 - loss: 2.1988 - regression_loss: 1.6650 - classification_loss: 0.5338
1407/3000 [=============>................] - ETA: 15:15 - loss: 2.1981 - regression_loss: 1.6643 - classification_loss: 0.5338
1408/3000 [=============>................] - ETA: 15:14 - loss: 2.1972 - regression_loss: 1.6636 - classification_loss: 0.5337
1409/3000 [=============>................] - ETA: 15:13 - loss: 2.1967 - regression_loss: 1.6632 - classification_loss: 0.5336
1410/3000 [=============>................] - ETA: 15:12 - loss: 2.1965 - regression_loss: 1.6631 - classification_loss: 0.5335
1411/3000 [=============>................] - ETA: 15:12 - loss: 2.1957 - regression_loss: 1.6623 - classification_loss: 0.5334
1412/3000 [=============>................] - ETA: 15:11 - loss: 2.1952 - regression_loss: 1.6619 - classification_loss: 0.5332
1413/3000 [=============>................] - ETA: 15:10 - loss: 2.1947 - regression_loss: 1.6614 - classification_loss: 0.5332
1414/3000 [=============>................] - ETA: 15:09 - loss: 2.1940 - regression_loss: 1.6609 - classification_loss: 0.5331
1415/3000 [=============>................] - ETA: 15:09 - loss: 2.1938 - regression_loss: 1.6608 - classification_loss: 0.5330
1416/3000 [=============>................] - ETA: 15:08 - loss: 2.1931 - regression_loss: 1.6602 - classification_loss: 0.5329
1417/3000 [=============>................] - ETA: 15:07 - loss: 2.1923 - regression_loss: 1.6595 - classification_loss: 0.5328
1418/3000 [=============>................] - ETA: 15:06 - loss: 2.1913 - regression_loss: 1.6586 - classification_loss: 0.5327
1419/3000 [=============>................] - ETA: 15:05 - loss: 2.1907 - regression_loss: 1.6581 - classification_loss: 0.5326
1420/3000 [=============>................] - ETA: 15:05 - loss: 2.1900 - regression_loss: 1.6575 - classification_loss: 0.5324
1421/3000 [=============>................] - ETA: 15:04 - loss: 2.1896 - regression_loss: 1.6572 - classification_loss: 0.5324
1422/3000 [=============>................] - ETA: 15:03 - loss: 2.1888 - regression_loss: 1.6565 - classification_loss: 0.5323
1423/3000 [=============>................] - ETA: 15:02 - loss: 2.1883 - regression_loss: 1.6561 - classification_loss: 0.5322
1424/3000 [=============>................] - ETA: 15:02 - loss: 2.1876 - regression_loss: 1.6555 - classification_loss: 0.5321
1425/3000 [=============>................] - ETA: 15:01 - loss: 2.1871 - regression_loss: 1.6550 - classification_loss: 0.5320
1426/3000 [=============>................] - ETA: 15:00 - loss: 2.1863 - regression_loss: 1.6543 - classification_loss: 0.5319
1427/3000 [=============>................] - ETA: 14:59 - loss: 2.1855 - regression_loss: 1.6536 - classification_loss: 0.5318
1428/3000 [=============>................] - ETA: 14:59 - loss: 2.1848 - regression_loss: 1.6531 - classification_loss: 0.5317
1429/3000 [=============>................] - ETA: 14:58 - loss: 2.1840 - regression_loss: 1.6524 - classification_loss: 0.5316
1430/3000 [=============>................] - ETA: 14:57 - loss: 2.1833 - regression_loss: 1.6517 - classification_loss: 0.5315
1431/3000 [=============>................] - ETA: 14:57 - loss: 2.1827 - regression_loss: 1.6513 - classification_loss: 0.5314
1432/3000 [=============>................] - ETA: 14:56 - loss: 2.1821 - regression_loss: 1.6507 - classification_loss: 0.5314
1433/3000 [=============>................] - ETA: 14:55 - loss: 2.1818 - regression_loss: 1.6505 - classification_loss: 0.5313
1434/3000 [=============>................] - ETA: 14:54 - loss: 2.1812 - regression_loss: 1.6500 - classification_loss: 0.5312
1435/3000 [=============>................] - ETA: 14:54 - loss: 2.1803 - regression_loss: 1.6493 - classification_loss: 0.5310
1436/3000 [=============>................] - ETA: 14:53 - loss: 2.1802 - regression_loss: 1.6492 - classification_loss: 0.5310
1437/3000 [=============>................] - ETA: 14:52 - loss: 2.1796 - regression_loss: 1.6487 - classification_loss: 0.5308
1438/3000 [=============>................] - ETA: 14:51 - loss: 2.1791 - regression_loss: 1.6483 - classification_loss: 0.5308
1439/3000 [=============>................] - ETA: 14:51 - loss: 2.1786 - regression_loss: 1.6478 - classification_loss: 0.5307
1440/3000 [=============>................] - ETA: 14:50 - loss: 2.1781 - regression_loss: 1.6475 - classification_loss: 0.5307
1441/3000 [=============>................] - ETA: 14:49 - loss: 2.1773 - regression_loss: 1.6467 - classification_loss: 0.5306
1442/3000 [=============>................] - ETA: 14:48 - loss: 2.1767 - regression_loss: 1.6462 - classification_loss: 0.5305
1443/3000 [=============>................] - ETA: 14:48 - loss: 2.1762 - regression_loss: 1.6459 - classification_loss: 0.5303
1444/3000 [=============>................] - ETA: 14:47 - loss: 2.1757 - regression_loss: 1.6455 - classification_loss: 0.5302
1445/3000 [=============>................] - ETA: 14:46 - loss: 2.1748 - regression_loss: 1.6447 - classification_loss: 0.5301
1446/3000 [=============>................] - ETA: 14:45 - loss: 2.1744 - regression_loss: 1.6444 - classification_loss: 0.5300
1447/3000 [=============>................] - ETA: 14:45 - loss: 2.1737 - regression_loss: 1.6437 - classification_loss: 0.5300
1448/3000 [=============>................] - ETA: 14:44 - loss: 2.1730 - regression_loss: 1.6431 - classification_loss: 0.5299
1449/3000 [=============>................] - ETA: 14:43 - loss: 2.1722 - regression_loss: 1.6424 - classification_loss: 0.5298
1450/3000 [=============>................] - ETA: 14:42 - loss: 2.1715 - regression_loss: 1.6419 - classification_loss: 0.5297
1451/3000 [=============>................] - ETA: 14:42 - loss: 2.1707 - regression_loss: 1.6411 - classification_loss: 0.5296
1452/3000 [=============>................] - ETA: 14:41 - loss: 2.1698 - regression_loss: 1.6403 - classification_loss: 0.5295
1453/3000 [=============>................] - ETA: 14:40 - loss: 2.1691 - regression_loss: 1.6397 - classification_loss: 0.5295
1454/3000 [=============>................] - ETA: 14:39 - loss: 2.1685 - regression_loss: 1.6391 - classification_loss: 0.5294
1455/3000 [=============>................] - ETA: 14:39 - loss: 2.1679 - regression_loss: 1.6386 - classification_loss: 0.5293
1456/3000 [=============>................] - ETA: 14:38 - loss: 2.1671 - regression_loss: 1.6380 - classification_loss: 0.5291
1457/3000 [=============>................] - ETA: 14:37 - loss: 2.1664 - regression_loss: 1.6373 - classification_loss: 0.5291
1458/3000 [=============>................] - ETA: 14:37 - loss: 2.1661 - regression_loss: 1.6371 - classification_loss: 0.5290
1459/3000 [=============>................] - ETA: 14:36 - loss: 2.1652 - regression_loss: 1.6363 - classification_loss: 0.5289
1460/3000 [=============>................] - ETA: 14:35 - loss: 2.1645 - regression_loss: 1.6357 - classification_loss: 0.5288
1461/3000 [=============>................] - ETA: 14:34 - loss: 2.1641 - regression_loss: 1.6353 - classification_loss: 0.5288
1462/3000 [=============>................] - ETA: 14:34 - loss: 2.1634 - regression_loss: 1.6347 - classification_loss: 0.5287
1463/3000 [=============>................] - ETA: 14:33 - loss: 2.1629 - regression_loss: 1.6342 - classification_loss: 0.5287
1464/3000 [=============>................] - ETA: 14:32 - loss: 2.1622 - regression_loss: 1.6336 - classification_loss: 0.5286
1465/3000 [=============>................] - ETA: 14:31 - loss: 2.1614 - regression_loss: 1.6329 - classification_loss: 0.5285
1466/3000 [=============>................] - ETA: 14:31 - loss: 2.1606 - regression_loss: 1.6322 - classification_loss: 0.5284
1467/3000 [=============>................] - ETA: 14:30 - loss: 2.1599 - regression_loss: 1.6316 - classification_loss: 0.5284
1468/3000 [=============>................] - ETA: 14:29 - loss: 2.1591 - regression_loss: 1.6308 - classification_loss: 0.5283
1469/3000 [=============>................] - ETA: 14:28 - loss: 2.1583 - regression_loss: 1.6301 - classification_loss: 0.5282
1470/3000 [=============>................] - ETA: 14:28 - loss: 2.1574 - regression_loss: 1.6293 - classification_loss: 0.5281
1471/3000 [=============>................] - ETA: 14:27 - loss: 2.1565 - regression_loss: 1.6285 - classification_loss: 0.5279
1472/3000 [=============>................] - ETA: 14:26 - loss: 2.1559 - regression_loss: 1.6280 - classification_loss: 0.5279
1473/3000 [=============>................] - ETA: 14:26 - loss: 2.1551 - regression_loss: 1.6273 - classification_loss: 0.5278
1474/3000 [=============>................] - ETA: 14:25 - loss: 2.1548 - regression_loss: 1.6270 - classification_loss: 0.5277
1475/3000 [=============>................] - ETA: 14:24 - loss: 2.1539 - regression_loss: 1.6263 - classification_loss: 0.5276
1476/3000 [=============>................] - ETA: 14:23 - loss: 2.1533 - regression_loss: 1.6258 - classification_loss: 0.5275
1477/3000 [=============>................] - ETA: 14:23 - loss: 2.1525 - regression_loss: 1.6252 - classification_loss: 0.5274
1478/3000 [=============>................] - ETA: 14:22 - loss: 2.1518 - regression_loss: 1.6245 - classification_loss: 0.5273
1479/3000 [=============>................] - ETA: 14:21 - loss: 2.1514 - regression_loss: 1.6242 - classification_loss: 0.5272
1480/3000 [=============>................] - ETA: 14:20 - loss: 2.1506 - regression_loss: 1.6235 - classification_loss: 0.5272
1481/3000 [=============>................] - ETA: 14:20 - loss: 2.1497 - regression_loss: 1.6226 - classification_loss: 0.5271
1482/3000 [=============>................] - ETA: 14:19 - loss: 2.1491 - regression_loss: 1.6221 - classification_loss: 0.5269
1483/3000 [=============>................] - ETA: 14:18 - loss: 2.1485 - regression_loss: 1.6216 - classification_loss: 0.5268
1484/3000 [=============>................] - ETA: 14:18 - loss: 2.1477 - regression_loss: 1.6209 - classification_loss: 0.5267
1485/3000 [=============>................] - ETA: 14:17 - loss: 2.1473 - regression_loss: 1.6206 - classification_loss: 0.5267
1486/3000 [=============>................] - ETA: 14:16 - loss: 2.1467 - regression_loss: 1.6201 - classification_loss: 0.5267
1487/3000 [=============>................] - ETA: 14:15 - loss: 2.1462 - regression_loss: 1.6196 - classification_loss: 0.5266
1488/3000 [=============>................] - ETA: 14:15 - loss: 2.1456 - regression_loss: 1.6191 - classification_loss: 0.5265
1489/3000 [=============>................] - ETA: 14:14 - loss: 2.1451 - regression_loss: 1.6186 - classification_loss: 0.5265
1490/3000 [=============>................] - ETA: 14:13 - loss: 2.1450 - regression_loss: 1.6186 - classification_loss: 0.5264
1491/3000 [=============>................] - ETA: 14:12 - loss: 2.1442 - regression_loss: 1.6179 - classification_loss: 0.5263
1492/3000 [=============>................] - ETA: 14:12 - loss: 2.1434 - regression_loss: 1.6172 - classification_loss: 0.5262
1493/3000 [=============>................] - ETA: 14:11 - loss: 2.1428 - regression_loss: 1.6166 - classification_loss: 0.5262
1494/3000 [=============>................] - ETA: 14:10 - loss: 2.1420 - regression_loss: 1.6160 - classification_loss: 0.5261
1495/3000 [=============>................] - ETA: 14:10 - loss: 2.1416 - regression_loss: 1.6156 - classification_loss: 0.5260
1496/3000 [=============>................] - ETA: 14:09 - loss: 2.1407 - regression_loss: 1.6148 - classification_loss: 0.5259
1497/3000 [=============>................] - ETA: 14:08 - loss: 2.1403 - regression_loss: 1.6145 - classification_loss: 0.5258
1498/3000 [=============>................] - ETA: 14:07 - loss: 2.1401 - regression_loss: 1.6143 - classification_loss: 0.5258
1499/3000 [=============>................] - ETA: 14:07 - loss: 2.1395 - regression_loss: 1.6138 - classification_loss: 0.5256
1500/3000 [==============>...............] - ETA: 14:06 - loss: 2.1386 - regression_loss: 1.6131 - classification_loss: 0.5255
1501/3000 [==============>...............] - ETA: 14:05 - loss: 2.1382 - regression_loss: 1.6128 - classification_loss: 0.5254
1502/3000 [==============>...............] - ETA: 14:05 - loss: 2.1376 - regression_loss: 1.6124 - classification_loss: 0.5253
1503/3000 [==============>...............] - ETA: 14:04 - loss: 2.1375 - regression_loss: 1.6123 - classification_loss: 0.5252
1504/3000 [==============>...............] - ETA: 14:03 - loss: 2.1375 - regression_loss: 1.6123 - classification_loss: 0.5252
1505/3000 [==============>...............] - ETA: 14:02 - loss: 2.1367 - regression_loss: 1.6116 - classification_loss: 0.5251
1506/3000 [==============>...............] - ETA: 14:02 - loss: 2.1362 - regression_loss: 1.6111 - classification_loss: 0.5251
1507/3000 [==============>...............] - ETA: 14:01 - loss: 2.1358 - regression_loss: 1.6108 - classification_loss: 0.5250
1508/3000 [==============>...............] - ETA: 14:00 - loss: 2.1350 - regression_loss: 1.6101 - classification_loss: 0.5249
1509/3000 [==============>...............] - ETA: 14:00 - loss: 2.1343 - regression_loss: 1.6095 - classification_loss: 0.5248
1510/3000 [==============>...............] - ETA: 13:59 - loss: 2.1338 - regression_loss: 1.6091 - classification_loss: 0.5247
1511/3000 [==============>...............] - ETA: 13:58 - loss: 2.1337 - regression_loss: 1.6090 - classification_loss: 0.5247
1512/3000 [==============>...............] - ETA: 13:57 - loss: 2.1328 - regression_loss: 1.6083 - classification_loss: 0.5245
1513/3000 [==============>...............] - ETA: 13:57 - loss: 2.1319 - regression_loss: 1.6075 - classification_loss: 0.5244
1514/3000 [==============>...............] - ETA: 13:56 - loss: 2.1310 - regression_loss: 1.6067 - classification_loss: 0.5243
1515/3000 [==============>...............] - ETA: 13:55 - loss: 2.1302 - regression_loss: 1.6059 - classification_loss: 0.5243
1516/3000 [==============>...............] - ETA: 13:55 - loss: 2.1297 - regression_loss: 1.6055 - classification_loss: 0.5242
1517/3000 [==============>...............] - ETA: 13:54 - loss: 2.1290 - regression_loss: 1.6049 - classification_loss: 0.5241
1518/3000 [==============>...............] - ETA: 13:53 - loss: 2.1285 - regression_loss: 1.6045 - classification_loss: 0.5240
1519/3000 [==============>...............] - ETA: 13:52 - loss: 2.1279 - regression_loss: 1.6040 - classification_loss: 0.5239
1520/3000 [==============>...............] - ETA: 13:52 - loss: 2.1273 - regression_loss: 1.6035 - classification_loss: 0.5238
1521/3000 [==============>...............] - ETA: 13:51 - loss: 2.1271 - regression_loss: 1.6034 - classification_loss: 0.5237
1522/3000 [==============>...............] - ETA: 13:50 - loss: 2.1263 - regression_loss: 1.6027 - classification_loss: 0.5236
1523/3000 [==============>...............] - ETA: 13:50 - loss: 2.1257 - regression_loss: 1.6020 - classification_loss: 0.5236
1524/3000 [==============>...............] - ETA: 13:49 - loss: 2.1249 - regression_loss: 1.6014 - classification_loss: 0.5235
1525/3000 [==============>...............] - ETA: 13:48 - loss: 2.1244 - regression_loss: 1.6010 - classification_loss: 0.5234
1526/3000 [==============>...............] - ETA: 13:47 - loss: 2.1237 - regression_loss: 1.6003 - classification_loss: 0.5234
1527/3000 [==============>...............] - ETA: 13:47 - loss: 2.1230 - regression_loss: 1.5997 - classification_loss: 0.5233
1528/3000 [==============>...............] - ETA: 13:46 - loss: 2.1223 - regression_loss: 1.5991 - classification_loss: 0.5232
1529/3000 [==============>...............] - ETA: 13:45 - loss: 2.1216 - regression_loss: 1.5986 - classification_loss: 0.5230
1530/3000 [==============>...............] - ETA: 13:45 - loss: 2.1213 - regression_loss: 1.5983 - classification_loss: 0.5229
1531/3000 [==============>...............] - ETA: 13:44 - loss: 2.1206 - regression_loss: 1.5977 - classification_loss: 0.5229
1532/3000 [==============>...............] - ETA: 13:43 - loss: 2.1199 - regression_loss: 1.5971 - classification_loss: 0.5228
1533/3000 [==============>...............] - ETA: 13:42 - loss: 2.1193 - regression_loss: 1.5966 - classification_loss: 0.5227
1534/3000 [==============>...............] - ETA: 13:42 - loss: 2.1187 - regression_loss: 1.5960 - classification_loss: 0.5226
1535/3000 [==============>...............] - ETA: 13:41 - loss: 2.1182 - regression_loss: 1.5956 - classification_loss: 0.5226
1536/3000 [==============>...............] - ETA: 13:40 - loss: 2.1177 - regression_loss: 1.5951 - classification_loss: 0.5226
1537/3000 [==============>...............] - ETA: 13:40 - loss: 2.1173 - regression_loss: 1.5949 - classification_loss: 0.5225
1538/3000 [==============>...............] - ETA: 13:39 - loss: 2.1170 - regression_loss: 1.5946 - classification_loss: 0.5224
1539/3000 [==============>...............] - ETA: 13:38 - loss: 2.1165 - regression_loss: 1.5941 - classification_loss: 0.5224
1540/3000 [==============>...............] - ETA: 13:38 - loss: 2.1158 - regression_loss: 1.5934 - classification_loss: 0.5223
1541/3000 [==============>...............] - ETA: 13:37 - loss: 2.1152 - regression_loss: 1.5929 - classification_loss: 0.5222
1542/3000 [==============>...............] - ETA: 13:36 - loss: 2.1151 - regression_loss: 1.5929 - classification_loss: 0.5222
1543/3000 [==============>...............] - ETA: 13:35 - loss: 2.1145 - regression_loss: 1.5924 - classification_loss: 0.5221
1544/3000 [==============>...............] - ETA: 13:35 - loss: 2.1145 - regression_loss: 1.5924 - classification_loss: 0.5221
1545/3000 [==============>...............] - ETA: 13:34 - loss: 2.1140 - regression_loss: 1.5920 - classification_loss: 0.5220
1546/3000 [==============>...............] - ETA: 13:33 - loss: 2.1138 - regression_loss: 1.5918 - classification_loss: 0.5220
1547/3000 [==============>...............] - ETA: 13:33 - loss: 2.1132 - regression_loss: 1.5913 - classification_loss: 0.5219
1548/3000 [==============>...............] - ETA: 13:32 - loss: 2.1130 - regression_loss: 1.5912 - classification_loss: 0.5218
1549/3000 [==============>...............] - ETA: 13:31 - loss: 2.1124 - regression_loss: 1.5906 - classification_loss: 0.5217
1550/3000 [==============>...............] - ETA: 13:31 - loss: 2.1118 - regression_loss: 1.5901 - classification_loss: 0.5217
1551/3000 [==============>...............] - ETA: 13:30 - loss: 2.1110 - regression_loss: 1.5895 - classification_loss: 0.5215
1552/3000 [==============>...............] - ETA: 13:29 - loss: 2.1102 - regression_loss: 1.5888 - classification_loss: 0.5214
1553/3000 [==============>...............] - ETA: 13:29 - loss: 2.1096 - regression_loss: 1.5883 - classification_loss: 0.5213
1554/3000 [==============>...............] - ETA: 13:28 - loss: 2.1092 - regression_loss: 1.5880 - classification_loss: 0.5213
1555/3000 [==============>...............] - ETA: 13:27 - loss: 2.1086 - regression_loss: 1.5874 - classification_loss: 0.5212
1556/3000 [==============>...............] - ETA: 13:26 - loss: 2.1079 - regression_loss: 1.5868 - classification_loss: 0.5211
1557/3000 [==============>...............] - ETA: 13:26 - loss: 2.1072 - regression_loss: 1.5862 - classification_loss: 0.5210
1558/3000 [==============>...............] - ETA: 13:25 - loss: 2.1072 - regression_loss: 1.5862 - classification_loss: 0.5210
1559/3000 [==============>...............] - ETA: 13:24 - loss: 2.1065 - regression_loss: 1.5857 - classification_loss: 0.5208
1560/3000 [==============>...............] - ETA: 13:24 - loss: 2.1058 - regression_loss: 1.5851 - classification_loss: 0.5207
1561/3000 [==============>...............] - ETA: 13:23 - loss: 2.1051 - regression_loss: 1.5846 - classification_loss: 0.5205
1562/3000 [==============>...............] - ETA: 13:22 - loss: 2.1046 - regression_loss: 1.5843 - classification_loss: 0.5203
1563/3000 [==============>...............] - ETA: 13:22 - loss: 2.1043 - regression_loss: 1.5840 - classification_loss: 0.5202
1564/3000 [==============>...............] - ETA: 13:21 - loss: 2.1039 - regression_loss: 1.5837 - classification_loss: 0.5202
1565/3000 [==============>...............] - ETA: 13:20 - loss: 2.1034 - regression_loss: 1.5832 - classification_loss: 0.5202
1566/3000 [==============>...............] - ETA: 13:19 - loss: 2.1027 - regression_loss: 1.5826 - classification_loss: 0.5201
1567/3000 [==============>...............] - ETA: 13:19 - loss: 2.1019 - regression_loss: 1.5819 - classification_loss: 0.5200
1568/3000 [==============>...............] - ETA: 13:18 - loss: 2.1012 - regression_loss: 1.5812 - classification_loss: 0.5200
1569/3000 [==============>...............] - ETA: 13:17 - loss: 2.1005 - regression_loss: 1.5806 - classification_loss: 0.5199
1570/3000 [==============>...............] - ETA: 13:17 - loss: 2.1002 - regression_loss: 1.5803 - classification_loss: 0.5198
1571/3000 [==============>...............] - ETA: 13:16 - loss: 2.0999 - regression_loss: 1.5800 - classification_loss: 0.5198
1572/3000 [==============>...............] - ETA: 13:15 - loss: 2.0991 - regression_loss: 1.5794 - classification_loss: 0.5197
1573/3000 [==============>...............] - ETA: 13:15 - loss: 2.0987 - regression_loss: 1.5789 - classification_loss: 0.5197
1574/3000 [==============>...............] - ETA: 13:14 - loss: 2.0980 - regression_loss: 1.5784 - classification_loss: 0.5196
1575/3000 [==============>...............] - ETA: 13:13 - loss: 2.0973 - regression_loss: 1.5778 - classification_loss: 0.5195
1576/3000 [==============>...............] - ETA: 13:13 - loss: 2.0966 - regression_loss: 1.5772 - classification_loss: 0.5194
1577/3000 [==============>...............] - ETA: 13:12 - loss: 2.0958 - regression_loss: 1.5765 - classification_loss: 0.5193
1578/3000 [==============>...............] - ETA: 13:11 - loss: 2.0952 - regression_loss: 1.5760 - classification_loss: 0.5192
1579/3000 [==============>...............] - ETA: 13:10 - loss: 2.0947 - regression_loss: 1.5756 - classification_loss: 0.5191
1580/3000 [==============>...............] - ETA: 13:10 - loss: 2.0939 - regression_loss: 1.5749 - classification_loss: 0.5190
1581/3000 [==============>...............] - ETA: 13:09 - loss: 2.0937 - regression_loss: 1.5748 - classification_loss: 0.5189
1582/3000 [==============>...............] - ETA: 13:08 - loss: 2.0931 - regression_loss: 1.5743 - classification_loss: 0.5189
1583/3000 [==============>...............] - ETA: 13:08 - loss: 2.0926 - regression_loss: 1.5738 - classification_loss: 0.5188
1584/3000 [==============>...............] - ETA: 13:07 - loss: 2.0923 - regression_loss: 1.5735 - classification_loss: 0.5187
1585/3000 [==============>...............] - ETA: 13:06 - loss: 2.0918 - regression_loss: 1.5732 - classification_loss: 0.5186
1586/3000 [==============>...............] - ETA: 13:06 - loss: 2.0917 - regression_loss: 1.5730 - classification_loss: 0.5186
1587/3000 [==============>...............] - ETA: 13:05 - loss: 2.0913 - regression_loss: 1.5727 - classification_loss: 0.5185
1588/3000 [==============>...............] - ETA: 13:04 - loss: 2.0909 - regression_loss: 1.5725 - classification_loss: 0.5184
1589/3000 [==============>...............] - ETA: 13:04 - loss: 2.0905 - regression_loss: 1.5722 - classification_loss: 0.5183
1590/3000 [==============>...............] - ETA: 13:03 - loss: 2.0898 - regression_loss: 1.5717 - classification_loss: 0.5182
1591/3000 [==============>...............] - ETA: 13:02 - loss: 2.0895 - regression_loss: 1.5714 - classification_loss: 0.5181
1592/3000 [==============>...............] - ETA: 13:02 - loss: 2.0892 - regression_loss: 1.5712 - classification_loss: 0.5180
1593/3000 [==============>...............] - ETA: 13:01 - loss: 2.0887 - regression_loss: 1.5708 - classification_loss: 0.5179
1594/3000 [==============>...............] - ETA: 13:00 - loss: 2.0880 - regression_loss: 1.5702 - classification_loss: 0.5179
1595/3000 [==============>...............] - ETA: 13:00 - loss: 2.0874 - regression_loss: 1.5696 - classification_loss: 0.5178
1596/3000 [==============>...............] - ETA: 12:59 - loss: 2.0874 - regression_loss: 1.5696 - classification_loss: 0.5178
1597/3000 [==============>...............] - ETA: 12:58 - loss: 2.0871 - regression_loss: 1.5694 - classification_loss: 0.5177
1598/3000 [==============>...............] - ETA: 12:58 - loss: 2.0868 - regression_loss: 1.5692 - classification_loss: 0.5177
1599/3000 [==============>...............] - ETA: 12:57 - loss: 2.0865 - regression_loss: 1.5688 - classification_loss: 0.5176
1600/3000 [===============>..............] - ETA: 12:56 - loss: 2.0860 - regression_loss: 1.5684 - classification_loss: 0.5175
1601/3000 [===============>..............] - ETA: 12:55 - loss: 2.0853 - regression_loss: 1.5679 - classification_loss: 0.5174
1602/3000 [===============>..............] - ETA: 12:55 - loss: 2.0852 - regression_loss: 1.5678 - classification_loss: 0.5174
1603/3000 [===============>..............] - ETA: 12:54 - loss: 2.0848 - regression_loss: 1.5675 - classification_loss: 0.5173
1604/3000 [===============>..............] - ETA: 12:53 - loss: 2.0841 - regression_loss: 1.5669 - classification_loss: 0.5172
1605/3000 [===============>..............] - ETA: 12:53 - loss: 2.0837 - regression_loss: 1.5665 - classification_loss: 0.5171
1606/3000 [===============>..............] - ETA: 12:52 - loss: 2.0831 - regression_loss: 1.5661 - classification_loss: 0.5170
1607/3000 [===============>..............] - ETA: 12:51 - loss: 2.0824 - regression_loss: 1.5655 - classification_loss: 0.5169
1608/3000 [===============>..............] - ETA: 12:51 - loss: 2.0819 - regression_loss: 1.5650 - classification_loss: 0.5169
1609/3000 [===============>..............] - ETA: 12:50 - loss: 2.0814 - regression_loss: 1.5646 - classification_loss: 0.5168
1610/3000 [===============>..............] - ETA: 12:49 - loss: 2.0807 - regression_loss: 1.5641 - classification_loss: 0.5167
1611/3000 [===============>..............] - ETA: 12:49 - loss: 2.0803 - regression_loss: 1.5636 - classification_loss: 0.5167
1612/3000 [===============>..............] - ETA: 12:48 - loss: 2.0795 - regression_loss: 1.5629 - classification_loss: 0.5166
1613/3000 [===============>..............] - ETA: 12:47 - loss: 2.0790 - regression_loss: 1.5625 - classification_loss: 0.5165
1614/3000 [===============>..............] - ETA: 12:47 - loss: 2.0785 - regression_loss: 1.5620 - classification_loss: 0.5165
1615/3000 [===============>..............] - ETA: 12:46 - loss: 2.0778 - regression_loss: 1.5614 - classification_loss: 0.5164
1616/3000 [===============>..............] - ETA: 12:45 - loss: 2.0775 - regression_loss: 1.5612 - classification_loss: 0.5163
1617/3000 [===============>..............] - ETA: 12:45 - loss: 2.0769 - regression_loss: 1.5607 - classification_loss: 0.5162
1618/3000 [===============>..............] - ETA: 12:44 - loss: 2.0763 - regression_loss: 1.5601 - classification_loss: 0.5161
1619/3000 [===============>..............] - ETA: 12:43 - loss: 2.0757 - regression_loss: 1.5597 - classification_loss: 0.5160
1620/3000 [===============>..............] - ETA: 12:43 - loss: 2.0751 - regression_loss: 1.5592 - classification_loss: 0.5159
1621/3000 [===============>..............] - ETA: 12:42 - loss: 2.0746 - regression_loss: 1.5587 - classification_loss: 0.5159
1622/3000 [===============>..............] - ETA: 12:41 - loss: 2.0741 - regression_loss: 1.5583 - classification_loss: 0.5158
1623/3000 [===============>..............] - ETA: 12:41 - loss: 2.0739 - regression_loss: 1.5581 - classification_loss: 0.5157
1624/3000 [===============>..............] - ETA: 12:40 - loss: 2.0730 - regression_loss: 1.5574 - classification_loss: 0.5156
1625/3000 [===============>..............] - ETA: 12:39 - loss: 2.0723 - regression_loss: 1.5568 - classification_loss: 0.5156
1626/3000 [===============>..............] - ETA: 12:39 - loss: 2.0716 - regression_loss: 1.5562 - classification_loss: 0.5154
1627/3000 [===============>..............] - ETA: 12:38 - loss: 2.0711 - regression_loss: 1.5557 - classification_loss: 0.5154
1628/3000 [===============>..............] - ETA: 12:37 - loss: 2.0703 - regression_loss: 1.5550 - classification_loss: 0.5153
1629/3000 [===============>..............] - ETA: 12:37 - loss: 2.0696 - regression_loss: 1.5544 - classification_loss: 0.5152
1630/3000 [===============>..............] - ETA: 12:36 - loss: 2.0688 - regression_loss: 1.5537 - classification_loss: 0.5151
1631/3000 [===============>..............] - ETA: 12:35 - loss: 2.0687 - regression_loss: 1.5536 - classification_loss: 0.5151
1632/3000 [===============>..............] - ETA: 12:34 - loss: 2.0681 - regression_loss: 1.5531 - classification_loss: 0.5150
1633/3000 [===============>..............] - ETA: 12:34 - loss: 2.0677 - regression_loss: 1.5528 - classification_loss: 0.5149
1634/3000 [===============>..............] - ETA: 12:33 - loss: 2.0670 - regression_loss: 1.5522 - classification_loss: 0.5149
1635/3000 [===============>..............] - ETA: 12:32 - loss: 2.0666 - regression_loss: 1.5518 - classification_loss: 0.5148
1636/3000 [===============>..............] - ETA: 12:32 - loss: 2.0659 - regression_loss: 1.5511 - classification_loss: 0.5147
1637/3000 [===============>..............] - ETA: 12:31 - loss: 2.0652 - regression_loss: 1.5505 - classification_loss: 0.5147
1638/3000 [===============>..............] - ETA: 12:30 - loss: 2.0650 - regression_loss: 1.5504 - classification_loss: 0.5146
1639/3000 [===============>..............] - ETA: 12:30 - loss: 2.0648 - regression_loss: 1.5502 - classification_loss: 0.5146
1640/3000 [===============>..............] - ETA: 12:29 - loss: 2.0642 - regression_loss: 1.5498 - classification_loss: 0.5145
1641/3000 [===============>..............] - ETA: 12:28 - loss: 2.0635 - regression_loss: 1.5492 - classification_loss: 0.5143
1642/3000 [===============>..............] - ETA: 12:28 - loss: 2.0628 - regression_loss: 1.5485 - classification_loss: 0.5143
1643/3000 [===============>..............] - ETA: 12:27 - loss: 2.0622 - regression_loss: 1.5480 - classification_loss: 0.5142
1644/3000 [===============>..............] - ETA: 12:26 - loss: 2.0619 - regression_loss: 1.5477 - classification_loss: 0.5142
1645/3000 [===============>..............] - ETA: 12:26 - loss: 2.0612 - regression_loss: 1.5472 - classification_loss: 0.5141
1646/3000 [===============>..............] - ETA: 12:25 - loss: 2.0607 - regression_loss: 1.5467 - classification_loss: 0.5140
1647/3000 [===============>..............] - ETA: 12:24 - loss: 2.0601 - regression_loss: 1.5461 - classification_loss: 0.5140
1648/3000 [===============>..............] - ETA: 12:24 - loss: 2.0593 - regression_loss: 1.5455 - classification_loss: 0.5139
1649/3000 [===============>..............] - ETA: 12:23 - loss: 2.0590 - regression_loss: 1.5452 - classification_loss: 0.5138
1650/3000 [===============>..............] - ETA: 12:22 - loss: 2.0583 - regression_loss: 1.5446 - classification_loss: 0.5137
1651/3000 [===============>..............] - ETA: 12:22 - loss: 2.0577 - regression_loss: 1.5441 - classification_loss: 0.5136
1652/3000 [===============>..............] - ETA: 12:21 - loss: 2.0570 - regression_loss: 1.5435 - classification_loss: 0.5135
1653/3000 [===============>..............] - ETA: 12:20 - loss: 2.0563 - regression_loss: 1.5429 - classification_loss: 0.5134
1654/3000 [===============>..............] - ETA: 12:20 - loss: 2.0561 - regression_loss: 1.5428 - classification_loss: 0.5133
1655/3000 [===============>..............] - ETA: 12:19 - loss: 2.0556 - regression_loss: 1.5424 - classification_loss: 0.5132
1656/3000 [===============>..............] - ETA: 12:18 - loss: 2.0551 - regression_loss: 1.5420 - classification_loss: 0.5131
1657/3000 [===============>..............] - ETA: 12:18 - loss: 2.0549 - regression_loss: 1.5418 - classification_loss: 0.5131
1658/3000 [===============>..............] - ETA: 12:17 - loss: 2.0543 - regression_loss: 1.5412 - classification_loss: 0.5131
1659/3000 [===============>..............] - ETA: 12:16 - loss: 2.0536 - regression_loss: 1.5406 - classification_loss: 0.5130
1660/3000 [===============>..............] - ETA: 12:16 - loss: 2.0529 - regression_loss: 1.5400 - classification_loss: 0.5129
1661/3000 [===============>..............] - ETA: 12:15 - loss: 2.0523 - regression_loss: 1.5396 - classification_loss: 0.5128
1662/3000 [===============>..............] - ETA: 12:14 - loss: 2.0518 - regression_loss: 1.5390 - classification_loss: 0.5127
1663/3000 [===============>..............] - ETA: 12:14 - loss: 2.0514 - regression_loss: 1.5387 - classification_loss: 0.5127
1664/3000 [===============>..............] - ETA: 12:13 - loss: 2.0508 - regression_loss: 1.5382 - classification_loss: 0.5127
1665/3000 [===============>..............] - ETA: 12:12 - loss: 2.0503 - regression_loss: 1.5377 - classification_loss: 0.5126
1666/3000 [===============>..............] - ETA: 12:12 - loss: 2.0496 - regression_loss: 1.5371 - classification_loss: 0.5125
1667/3000 [===============>..............] - ETA: 12:11 - loss: 2.0495 - regression_loss: 1.5370 - classification_loss: 0.5125
1668/3000 [===============>..............] - ETA: 12:10 - loss: 2.0489 - regression_loss: 1.5364 - classification_loss: 0.5125
1669/3000 [===============>..............] - ETA: 12:10 - loss: 2.0483 - regression_loss: 1.5358 - classification_loss: 0.5125
1670/3000 [===============>..............] - ETA: 12:09 - loss: 2.0476 - regression_loss: 1.5352 - classification_loss: 0.5124
1671/3000 [===============>..............] - ETA: 12:09 - loss: 2.0470 - regression_loss: 1.5347 - classification_loss: 0.5123
1672/3000 [===============>..............] - ETA: 12:08 - loss: 2.0464 - regression_loss: 1.5341 - classification_loss: 0.5123
1673/3000 [===============>..............] - ETA: 12:07 - loss: 2.0460 - regression_loss: 1.5338 - classification_loss: 0.5123
1674/3000 [===============>..............] - ETA: 12:07 - loss: 2.0456 - regression_loss: 1.5334 - classification_loss: 0.5122
1675/3000 [===============>..............] - ETA: 12:06 - loss: 2.0452 - regression_loss: 1.5331 - classification_loss: 0.5121
1676/3000 [===============>..............] - ETA: 12:05 - loss: 2.0447 - regression_loss: 1.5326 - classification_loss: 0.5121
1677/3000 [===============>..............] - ETA: 12:05 - loss: 2.0440 - regression_loss: 1.5320 - classification_loss: 0.5120
1678/3000 [===============>..............] - ETA: 12:04 - loss: 2.0438 - regression_loss: 1.5318 - classification_loss: 0.5120
1679/3000 [===============>..............] - ETA: 12:03 - loss: 2.0432 - regression_loss: 1.5313 - classification_loss: 0.5119
1680/3000 [===============>..............] - ETA: 12:03 - loss: 2.0430 - regression_loss: 1.5311 - classification_loss: 0.5119
1681/3000 [===============>..............] - ETA: 12:02 - loss: 2.0424 - regression_loss: 1.5306 - classification_loss: 0.5118
1682/3000 [===============>..............] - ETA: 12:01 - loss: 2.0417 - regression_loss: 1.5300 - classification_loss: 0.5117
1683/3000 [===============>..............] - ETA: 12:01 - loss: 2.0413 - regression_loss: 1.5296 - classification_loss: 0.5117
1684/3000 [===============>..............] - ETA: 12:00 - loss: 2.0407 - regression_loss: 1.5291 - classification_loss: 0.5116
1685/3000 [===============>..............] - ETA: 11:59 - loss: 2.0401 - regression_loss: 1.5285 - classification_loss: 0.5116
1686/3000 [===============>..............] - ETA: 11:59 - loss: 2.0396 - regression_loss: 1.5281 - classification_loss: 0.5115
1687/3000 [===============>..............] - ETA: 11:58 - loss: 2.0391 - regression_loss: 1.5276 - classification_loss: 0.5115
1688/3000 [===============>..............] - ETA: 11:57 - loss: 2.0385 - regression_loss: 1.5271 - classification_loss: 0.5114
1689/3000 [===============>..............] - ETA: 11:57 - loss: 2.0380 - regression_loss: 1.5268 - classification_loss: 0.5113
1690/3000 [===============>..............] - ETA: 11:56 - loss: 2.0375 - regression_loss: 1.5263 - classification_loss: 0.5112
1691/3000 [===============>..............] - ETA: 11:55 - loss: 2.0373 - regression_loss: 1.5261 - classification_loss: 0.5111
1692/3000 [===============>..............] - ETA: 11:55 - loss: 2.0368 - regression_loss: 1.5257 - classification_loss: 0.5110
1693/3000 [===============>..............] - ETA: 11:54 - loss: 2.0364 - regression_loss: 1.5254 - classification_loss: 0.5110
1694/3000 [===============>..............] - ETA: 11:53 - loss: 2.0356 - regression_loss: 1.5248 - classification_loss: 0.5108
1695/3000 [===============>..............] - ETA: 11:53 - loss: 2.0349 - regression_loss: 1.5242 - classification_loss: 0.5108
1696/3000 [===============>..............] - ETA: 11:52 - loss: 2.0344 - regression_loss: 1.5238 - classification_loss: 0.5106
1697/3000 [===============>..............] - ETA: 11:51 - loss: 2.0338 - regression_loss: 1.5233 - classification_loss: 0.5106
1698/3000 [===============>..............] - ETA: 11:51 - loss: 2.0331 - regression_loss: 1.5226 - classification_loss: 0.5105
1699/3000 [===============>..............] - ETA: 11:50 - loss: 2.0326 - regression_loss: 1.5222 - classification_loss: 0.5104
1700/3000 [================>.............] - ETA: 11:49 - loss: 2.0320 - regression_loss: 1.5216 - classification_loss: 0.5103
1701/3000 [================>.............] - ETA: 11:49 - loss: 2.0314 - regression_loss: 1.5211 - classification_loss: 0.5102
1702/3000 [================>.............] - ETA: 11:48 - loss: 2.0307 - regression_loss: 1.5205 - classification_loss: 0.5102
1703/3000 [================>.............] - ETA: 11:48 - loss: 2.0303 - regression_loss: 1.5202 - classification_loss: 0.5101
1704/3000 [================>.............] - ETA: 11:47 - loss: 2.0296 - regression_loss: 1.5196 - classification_loss: 0.5100
1705/3000 [================>.............] - ETA: 11:46 - loss: 2.0293 - regression_loss: 1.5194 - classification_loss: 0.5099
1706/3000 [================>.............] - ETA: 11:46 - loss: 2.0287 - regression_loss: 1.5188 - classification_loss: 0.5099
1707/3000 [================>.............] - ETA: 11:45 - loss: 2.0282 - regression_loss: 1.5185 - classification_loss: 0.5098
1708/3000 [================>.............] - ETA: 11:44 - loss: 2.0278 - regression_loss: 1.5181 - classification_loss: 0.5097
1709/3000 [================>.............] - ETA: 11:44 - loss: 2.0271 - regression_loss: 1.5175 - classification_loss: 0.5096
1710/3000 [================>.............] - ETA: 11:43 - loss: 2.0267 - regression_loss: 1.5172 - classification_loss: 0.5095
1711/3000 [================>.............] - ETA: 11:42 - loss: 2.0265 - regression_loss: 1.5171 - classification_loss: 0.5094
1712/3000 [================>.............] - ETA: 11:42 - loss: 2.0259 - regression_loss: 1.5165 - classification_loss: 0.5094
1713/3000 [================>.............] - ETA: 11:41 - loss: 2.0253 - regression_loss: 1.5160 - classification_loss: 0.5093
1714/3000 [================>.............] - ETA: 11:40 - loss: 2.0248 - regression_loss: 1.5155 - classification_loss: 0.5093
1715/3000 [================>.............] - ETA: 11:40 - loss: 2.0243 - regression_loss: 1.5151 - classification_loss: 0.5092
1716/3000 [================>.............] - ETA: 11:39 - loss: 2.0235 - regression_loss: 1.5144 - classification_loss: 0.5091
1717/3000 [================>.............] - ETA: 11:39 - loss: 2.0231 - regression_loss: 1.5141 - classification_loss: 0.5090
1718/3000 [================>.............] - ETA: 11:38 - loss: 2.0227 - regression_loss: 1.5137 - classification_loss: 0.5090
1719/3000 [================>.............] - ETA: 11:37 - loss: 2.0223 - regression_loss: 1.5135 - classification_loss: 0.5089
1720/3000 [================>.............] - ETA: 11:37 - loss: 2.0218 - regression_loss: 1.5130 - classification_loss: 0.5088
1721/3000 [================>.............] - ETA: 11:36 - loss: 2.0213 - regression_loss: 1.5126 - classification_loss: 0.5087
1722/3000 [================>.............] - ETA: 11:35 - loss: 2.0209 - regression_loss: 1.5123 - classification_loss: 0.5087
1723/3000 [================>.............] - ETA: 11:35 - loss: 2.0203 - regression_loss: 1.5117 - classification_loss: 0.5086
1724/3000 [================>.............] - ETA: 11:34 - loss: 2.0195 - regression_loss: 1.5111 - classification_loss: 0.5084
1725/3000 [================>.............] - ETA: 11:33 - loss: 2.0192 - regression_loss: 1.5108 - classification_loss: 0.5084
1726/3000 [================>.............] - ETA: 11:33 - loss: 2.0189 - regression_loss: 1.5106 - classification_loss: 0.5083
1727/3000 [================>.............] - ETA: 11:32 - loss: 2.0182 - regression_loss: 1.5100 - classification_loss: 0.5082
1728/3000 [================>.............] - ETA: 11:31 - loss: 2.0179 - regression_loss: 1.5098 - classification_loss: 0.5081
1729/3000 [================>.............] - ETA: 11:31 - loss: 2.0173 - regression_loss: 1.5093 - classification_loss: 0.5080
1730/3000 [================>.............] - ETA: 11:30 - loss: 2.0165 - regression_loss: 1.5086 - classification_loss: 0.5080
1731/3000 [================>.............] - ETA: 11:30 - loss: 2.0161 - regression_loss: 1.5082 - classification_loss: 0.5079
1732/3000 [================>.............] - ETA: 11:29 - loss: 2.0154 - regression_loss: 1.5075 - classification_loss: 0.5079
1733/3000 [================>.............] - ETA: 11:28 - loss: 2.0151 - regression_loss: 1.5073 - classification_loss: 0.5078
1734/3000 [================>.............] - ETA: 11:28 - loss: 2.0144 - regression_loss: 1.5067 - classification_loss: 0.5077
1735/3000 [================>.............] - ETA: 11:27 - loss: 2.0137 - regression_loss: 1.5061 - classification_loss: 0.5076
1736/3000 [================>.............] - ETA: 11:26 - loss: 2.0131 - regression_loss: 1.5055 - classification_loss: 0.5075
1737/3000 [================>.............] - ETA: 11:26 - loss: 2.0127 - regression_loss: 1.5052 - classification_loss: 0.5075
1738/3000 [================>.............] - ETA: 11:25 - loss: 2.0123 - regression_loss: 1.5048 - classification_loss: 0.5075
1739/3000 [================>.............] - ETA: 11:24 - loss: 2.0118 - regression_loss: 1.5044 - classification_loss: 0.5074
1740/3000 [================>.............] - ETA: 11:24 - loss: 2.0113 - regression_loss: 1.5040 - classification_loss: 0.5073
1741/3000 [================>.............] - ETA: 11:23 - loss: 2.0108 - regression_loss: 1.5035 - classification_loss: 0.5073
1742/3000 [================>.............] - ETA: 11:23 - loss: 2.0107 - regression_loss: 1.5035 - classification_loss: 0.5073
1743/3000 [================>.............] - ETA: 11:22 - loss: 2.0101 - regression_loss: 1.5029 - classification_loss: 0.5072
1744/3000 [================>.............] - ETA: 11:21 - loss: 2.0096 - regression_loss: 1.5025 - classification_loss: 0.5071
1745/3000 [================>.............] - ETA: 11:21 - loss: 2.0090 - regression_loss: 1.5019 - classification_loss: 0.5070
1746/3000 [================>.............] - ETA: 11:20 - loss: 2.0084 - regression_loss: 1.5014 - classification_loss: 0.5070
1747/3000 [================>.............] - ETA: 11:19 - loss: 2.0080 - regression_loss: 1.5011 - classification_loss: 0.5069
1748/3000 [================>.............] - ETA: 11:19 - loss: 2.0075 - regression_loss: 1.5006 - classification_loss: 0.5069
1749/3000 [================>.............] - ETA: 11:18 - loss: 2.0069 - regression_loss: 1.5000 - classification_loss: 0.5068
1750/3000 [================>.............] - ETA: 11:17 - loss: 2.0063 - regression_loss: 1.4995 - classification_loss: 0.5068
1751/3000 [================>.............] - ETA: 11:17 - loss: 2.0056 - regression_loss: 1.4990 - classification_loss: 0.5067
1752/3000 [================>.............] - ETA: 11:16 - loss: 2.0050 - regression_loss: 1.4984 - classification_loss: 0.5066
1753/3000 [================>.............] - ETA: 11:16 - loss: 2.0045 - regression_loss: 1.4980 - classification_loss: 0.5066
1754/3000 [================>.............] - ETA: 11:15 - loss: 2.0039 - regression_loss: 1.4974 - classification_loss: 0.5065
1755/3000 [================>.............] - ETA: 11:14 - loss: 2.0036 - regression_loss: 1.4972 - classification_loss: 0.5064
1756/3000 [================>.............] - ETA: 11:14 - loss: 2.0032 - regression_loss: 1.4967 - classification_loss: 0.5065
1757/3000 [================>.............] - ETA: 11:13 - loss: 2.0026 - regression_loss: 1.4962 - classification_loss: 0.5064
1758/3000 [================>.............] - ETA: 11:12 - loss: 2.0024 - regression_loss: 1.4960 - classification_loss: 0.5063
1759/3000 [================>.............] - ETA: 11:12 - loss: 2.0018 - regression_loss: 1.4955 - classification_loss: 0.5063
1760/3000 [================>.............] - ETA: 11:11 - loss: 2.0012 - regression_loss: 1.4950 - classification_loss: 0.5062
1761/3000 [================>.............] - ETA: 11:10 - loss: 2.0006 - regression_loss: 1.4945 - classification_loss: 0.5062
1762/3000 [================>.............] - ETA: 11:10 - loss: 1.9999 - regression_loss: 1.4938 - classification_loss: 0.5061
1763/3000 [================>.............] - ETA: 11:09 - loss: 1.9993 - regression_loss: 1.4934 - classification_loss: 0.5060
1764/3000 [================>.............] - ETA: 11:09 - loss: 1.9988 - regression_loss: 1.4929 - classification_loss: 0.5059
1765/3000 [================>.............] - ETA: 11:08 - loss: 1.9983 - regression_loss: 1.4925 - classification_loss: 0.5058
1766/3000 [================>.............] - ETA: 11:07 - loss: 1.9979 - regression_loss: 1.4922 - classification_loss: 0.5057
1767/3000 [================>.............] - ETA: 11:07 - loss: 1.9974 - regression_loss: 1.4917 - classification_loss: 0.5057
1768/3000 [================>.............] - ETA: 11:06 - loss: 1.9967 - regression_loss: 1.4911 - classification_loss: 0.5056
1769/3000 [================>.............] - ETA: 11:05 - loss: 1.9962 - regression_loss: 1.4907 - classification_loss: 0.5055
1770/3000 [================>.............] - ETA: 11:05 - loss: 1.9955 - regression_loss: 1.4901 - classification_loss: 0.5055
1771/3000 [================>.............] - ETA: 11:04 - loss: 1.9954 - regression_loss: 1.4900 - classification_loss: 0.5054
1772/3000 [================>.............] - ETA: 11:04 - loss: 1.9948 - regression_loss: 1.4895 - classification_loss: 0.5053
1773/3000 [================>.............] - ETA: 11:03 - loss: 1.9941 - regression_loss: 1.4889 - classification_loss: 0.5052
1774/3000 [================>.............] - ETA: 11:02 - loss: 1.9936 - regression_loss: 1.4884 - classification_loss: 0.5051
1775/3000 [================>.............] - ETA: 11:02 - loss: 1.9933 - regression_loss: 1.4881 - classification_loss: 0.5051
1776/3000 [================>.............] - ETA: 11:01 - loss: 1.9931 - regression_loss: 1.4880 - classification_loss: 0.5051
1777/3000 [================>.............] - ETA: 11:00 - loss: 1.9926 - regression_loss: 1.4876 - classification_loss: 0.5050
1778/3000 [================>.............] - ETA: 11:00 - loss: 1.9923 - regression_loss: 1.4874 - classification_loss: 0.5049
1779/3000 [================>.............] - ETA: 10:59 - loss: 1.9917 - regression_loss: 1.4869 - classification_loss: 0.5048
1780/3000 [================>.............] - ETA: 10:58 - loss: 1.9910 - regression_loss: 1.4863 - classification_loss: 0.5047
1781/3000 [================>.............] - ETA: 10:58 - loss: 1.9905 - regression_loss: 1.4858 - classification_loss: 0.5047
1782/3000 [================>.............] - ETA: 10:57 - loss: 1.9900 - regression_loss: 1.4854 - classification_loss: 0.5046
1783/3000 [================>.............] - ETA: 10:57 - loss: 1.9896 - regression_loss: 1.4851 - classification_loss: 0.5045
1784/3000 [================>.............] - ETA: 10:56 - loss: 1.9891 - regression_loss: 1.4845 - classification_loss: 0.5045
1785/3000 [================>.............] - ETA: 10:55 - loss: 1.9886 - regression_loss: 1.4842 - classification_loss: 0.5044
1786/3000 [================>.............] - ETA: 10:55 - loss: 1.9884 - regression_loss: 1.4840 - classification_loss: 0.5044
1787/3000 [================>.............] - ETA: 10:54 - loss: 1.9878 - regression_loss: 1.4834 - classification_loss: 0.5043
1788/3000 [================>.............] - ETA: 10:53 - loss: 1.9875 - regression_loss: 1.4831 - classification_loss: 0.5043
1789/3000 [================>.............] - ETA: 10:53 - loss: 1.9868 - regression_loss: 1.4825 - classification_loss: 0.5043
1790/3000 [================>.............] - ETA: 10:52 - loss: 1.9863 - regression_loss: 1.4820 - classification_loss: 0.5043
1791/3000 [================>.............] - ETA: 10:52 - loss: 1.9859 - regression_loss: 1.4816 - classification_loss: 0.5043
1792/3000 [================>.............] - ETA: 10:51 - loss: 1.9853 - regression_loss: 1.4811 - classification_loss: 0.5042
1793/3000 [================>.............] - ETA: 10:50 - loss: 1.9849 - regression_loss: 1.4808 - classification_loss: 0.5041
1794/3000 [================>.............] - ETA: 10:50 - loss: 1.9844 - regression_loss: 1.4804 - classification_loss: 0.5040
1795/3000 [================>.............] - ETA: 10:49 - loss: 1.9838 - regression_loss: 1.4798 - classification_loss: 0.5040
1796/3000 [================>.............] - ETA: 10:48 - loss: 1.9836 - regression_loss: 1.4797 - classification_loss: 0.5039
1797/3000 [================>.............] - ETA: 10:48 - loss: 1.9832 - regression_loss: 1.4794 - classification_loss: 0.5038
1798/3000 [================>.............] - ETA: 10:47 - loss: 1.9827 - regression_loss: 1.4789 - classification_loss: 0.5038
1799/3000 [================>.............] - ETA: 10:47 - loss: 1.9824 - regression_loss: 1.4786 - classification_loss: 0.5038
1800/3000 [=================>............] - ETA: 10:46 - loss: 1.9820 - regression_loss: 1.4782 - classification_loss: 0.5037
1801/3000 [=================>............] - ETA: 10:45 - loss: 1.9815 - regression_loss: 1.4778 - classification_loss: 0.5037
1802/3000 [=================>............] - ETA: 10:45 - loss: 1.9811 - regression_loss: 1.4775 - classification_loss: 0.5036
1803/3000 [=================>............] - ETA: 10:44 - loss: 1.9805 - regression_loss: 1.4770 - classification_loss: 0.5035
1804/3000 [=================>............] - ETA: 10:43 - loss: 1.9801 - regression_loss: 1.4767 - classification_loss: 0.5034
1805/3000 [=================>............] - ETA: 10:43 - loss: 1.9796 - regression_loss: 1.4763 - classification_loss: 0.5034
1806/3000 [=================>............] - ETA: 10:42 - loss: 1.9794 - regression_loss: 1.4760 - classification_loss: 0.5034
1807/3000 [=================>............] - ETA: 10:42 - loss: 1.9792 - regression_loss: 1.4759 - classification_loss: 0.5033
1808/3000 [=================>............] - ETA: 10:41 - loss: 1.9786 - regression_loss: 1.4754 - classification_loss: 0.5033
1809/3000 [=================>............] - ETA: 10:40 - loss: 1.9782 - regression_loss: 1.4749 - classification_loss: 0.5032
1810/3000 [=================>............] - ETA: 10:40 - loss: 1.9777 - regression_loss: 1.4744 - classification_loss: 0.5032
1811/3000 [=================>............] - ETA: 10:39 - loss: 1.9773 - regression_loss: 1.4741 - classification_loss: 0.5032
1812/3000 [=================>............] - ETA: 10:38 - loss: 1.9768 - regression_loss: 1.4736 - classification_loss: 0.5032
1813/3000 [=================>............] - ETA: 10:38 - loss: 1.9762 - regression_loss: 1.4731 - classification_loss: 0.5031
1814/3000 [=================>............] - ETA: 10:37 - loss: 1.9756 - regression_loss: 1.4726 - classification_loss: 0.5030
1815/3000 [=================>............] - ETA: 10:37 - loss: 1.9749 - regression_loss: 1.4720 - classification_loss: 0.5029
1816/3000 [=================>............] - ETA: 10:36 - loss: 1.9743 - regression_loss: 1.4715 - classification_loss: 0.5028
1817/3000 [=================>............] - ETA: 10:35 - loss: 1.9742 - regression_loss: 1.4715 - classification_loss: 0.5028
1818/3000 [=================>............] - ETA: 10:35 - loss: 1.9739 - regression_loss: 1.4712 - classification_loss: 0.5027
1819/3000 [=================>............] - ETA: 10:34 - loss: 1.9734 - regression_loss: 1.4708 - classification_loss: 0.5027
1820/3000 [=================>............] - ETA: 10:34 - loss: 1.9731 - regression_loss: 1.4704 - classification_loss: 0.5026
1821/3000 [=================>............] - ETA: 10:33 - loss: 1.9728 - regression_loss: 1.4702 - classification_loss: 0.5026
1822/3000 [=================>............] - ETA: 10:32 - loss: 1.9723 - regression_loss: 1.4698 - classification_loss: 0.5025
1823/3000 [=================>............] - ETA: 10:32 - loss: 1.9718 - regression_loss: 1.4693 - classification_loss: 0.5024
1824/3000 [=================>............] - ETA: 10:31 - loss: 1.9711 - regression_loss: 1.4688 - classification_loss: 0.5024
1825/3000 [=================>............] - ETA: 10:30 - loss: 1.9709 - regression_loss: 1.4687 - classification_loss: 0.5023
1826/3000 [=================>............] - ETA: 10:30 - loss: 1.9705 - regression_loss: 1.4683 - classification_loss: 0.5022
1827/3000 [=================>............] - ETA: 10:29 - loss: 1.9699 - regression_loss: 1.4677 - classification_loss: 0.5021
1828/3000 [=================>............] - ETA: 10:29 - loss: 1.9694 - regression_loss: 1.4674 - classification_loss: 0.5021
1829/3000 [=================>............] - ETA: 10:28 - loss: 1.9691 - regression_loss: 1.4670 - classification_loss: 0.5021
1830/3000 [=================>............] - ETA: 10:27 - loss: 1.9686 - regression_loss: 1.4666 - classification_loss: 0.5020
1831/3000 [=================>............] - ETA: 10:27 - loss: 1.9681 - regression_loss: 1.4661 - classification_loss: 0.5019
1832/3000 [=================>............] - ETA: 10:26 - loss: 1.9675 - regression_loss: 1.4656 - classification_loss: 0.5019
1833/3000 [=================>............] - ETA: 10:26 - loss: 1.9671 - regression_loss: 1.4653 - classification_loss: 0.5018
1834/3000 [=================>............] - ETA: 10:25 - loss: 1.9666 - regression_loss: 1.4649 - classification_loss: 0.5017
1835/3000 [=================>............] - ETA: 10:24 - loss: 1.9665 - regression_loss: 1.4648 - classification_loss: 0.5017
1836/3000 [=================>............] - ETA: 10:24 - loss: 1.9660 - regression_loss: 1.4644 - classification_loss: 0.5016
1837/3000 [=================>............] - ETA: 10:23 - loss: 1.9655 - regression_loss: 1.4640 - classification_loss: 0.5015
1838/3000 [=================>............] - ETA: 10:22 - loss: 1.9654 - regression_loss: 1.4639 - classification_loss: 0.5015
1839/3000 [=================>............] - ETA: 10:22 - loss: 1.9650 - regression_loss: 1.4636 - classification_loss: 0.5014
1840/3000 [=================>............] - ETA: 10:21 - loss: 1.9646 - regression_loss: 1.4632 - classification_loss: 0.5014
1841/3000 [=================>............] - ETA: 10:21 - loss: 1.9641 - regression_loss: 1.4628 - classification_loss: 0.5013
1842/3000 [=================>............] - ETA: 10:20 - loss: 1.9635 - regression_loss: 1.4623 - classification_loss: 0.5012
1843/3000 [=================>............] - ETA: 10:19 - loss: 1.9629 - regression_loss: 1.4618 - classification_loss: 0.5012
1844/3000 [=================>............] - ETA: 10:19 - loss: 1.9624 - regression_loss: 1.4614 - classification_loss: 0.5011
1845/3000 [=================>............] - ETA: 10:18 - loss: 1.9620 - regression_loss: 1.4610 - classification_loss: 0.5010
1846/3000 [=================>............] - ETA: 10:18 - loss: 1.9617 - regression_loss: 1.4608 - classification_loss: 0.5009
1847/3000 [=================>............] - ETA: 10:17 - loss: 1.9611 - regression_loss: 1.4603 - classification_loss: 0.5008
1848/3000 [=================>............] - ETA: 10:16 - loss: 1.9606 - regression_loss: 1.4598 - classification_loss: 0.5008
1849/3000 [=================>............] - ETA: 10:16 - loss: 1.9599 - regression_loss: 1.4593 - classification_loss: 0.5006
1850/3000 [=================>............] - ETA: 10:15 - loss: 1.9593 - regression_loss: 1.4588 - classification_loss: 0.5005
1851/3000 [=================>............] - ETA: 10:14 - loss: 1.9588 - regression_loss: 1.4583 - classification_loss: 0.5005
1852/3000 [=================>............] - ETA: 10:14 - loss: 1.9582 - regression_loss: 1.4577 - classification_loss: 0.5004
1853/3000 [=================>............] - ETA: 10:13 - loss: 1.9576 - regression_loss: 1.4572 - classification_loss: 0.5004
1854/3000 [=================>............] - ETA: 10:13 - loss: 1.9574 - regression_loss: 1.4571 - classification_loss: 0.5003
1855/3000 [=================>............] - ETA: 10:12 - loss: 1.9571 - regression_loss: 1.4569 - classification_loss: 0.5002
1856/3000 [=================>............] - ETA: 10:11 - loss: 1.9566 - regression_loss: 1.4565 - classification_loss: 0.5001
1857/3000 [=================>............] - ETA: 10:11 - loss: 1.9564 - regression_loss: 1.4563 - classification_loss: 0.5001
1858/3000 [=================>............] - ETA: 10:10 - loss: 1.9561 - regression_loss: 1.4561 - classification_loss: 0.5000
1859/3000 [=================>............] - ETA: 10:10 - loss: 1.9556 - regression_loss: 1.4556 - classification_loss: 0.5000
1860/3000 [=================>............] - ETA: 10:09 - loss: 1.9549 - regression_loss: 1.4550 - classification_loss: 0.4999
1861/3000 [=================>............] - ETA: 10:08 - loss: 1.9544 - regression_loss: 1.4545 - classification_loss: 0.4999
1862/3000 [=================>............] - ETA: 10:08 - loss: 1.9539 - regression_loss: 1.4540 - classification_loss: 0.4998
1863/3000 [=================>............] - ETA: 10:07 - loss: 1.9534 - regression_loss: 1.4537 - classification_loss: 0.4998
1864/3000 [=================>............] - ETA: 10:06 - loss: 1.9528 - regression_loss: 1.4532 - classification_loss: 0.4997
1865/3000 [=================>............] - ETA: 10:06 - loss: 1.9523 - regression_loss: 1.4527 - classification_loss: 0.4996
1866/3000 [=================>............] - ETA: 10:05 - loss: 1.9521 - regression_loss: 1.4526 - classification_loss: 0.4995
1867/3000 [=================>............] - ETA: 10:05 - loss: 1.9517 - regression_loss: 1.4522 - classification_loss: 0.4995
1868/3000 [=================>............] - ETA: 10:04 - loss: 1.9512 - regression_loss: 1.4518 - classification_loss: 0.4994
1869/3000 [=================>............] - ETA: 10:03 - loss: 1.9509 - regression_loss: 1.4516 - classification_loss: 0.4993
1870/3000 [=================>............] - ETA: 10:03 - loss: 1.9506 - regression_loss: 1.4514 - classification_loss: 0.4992
1871/3000 [=================>............] - ETA: 10:02 - loss: 1.9500 - regression_loss: 1.4509 - classification_loss: 0.4991
1872/3000 [=================>............] - ETA: 10:02 - loss: 1.9495 - regression_loss: 1.4505 - classification_loss: 0.4990
1873/3000 [=================>............] - ETA: 10:01 - loss: 1.9491 - regression_loss: 1.4501 - classification_loss: 0.4990
1874/3000 [=================>............] - ETA: 10:00 - loss: 1.9485 - regression_loss: 1.4496 - classification_loss: 0.4989
1875/3000 [=================>............] - ETA: 10:00 - loss: 1.9479 - regression_loss: 1.4490 - classification_loss: 0.4989
1876/3000 [=================>............] - ETA: 9:59 - loss: 1.9476 - regression_loss: 1.4488 - classification_loss: 0.4988 
1877/3000 [=================>............] - ETA: 9:59 - loss: 1.9471 - regression_loss: 1.4484 - classification_loss: 0.4987
1878/3000 [=================>............] - ETA: 9:58 - loss: 1.9465 - regression_loss: 1.4478 - classification_loss: 0.4987
1879/3000 [=================>............] - ETA: 9:57 - loss: 1.9458 - regression_loss: 1.4472 - classification_loss: 0.4986
1880/3000 [=================>............] - ETA: 9:57 - loss: 1.9455 - regression_loss: 1.4469 - classification_loss: 0.4986
1881/3000 [=================>............] - ETA: 9:56 - loss: 1.9449 - regression_loss: 1.4464 - classification_loss: 0.4985
1882/3000 [=================>............] - ETA: 9:56 - loss: 1.9443 - regression_loss: 1.4458 - classification_loss: 0.4985
1883/3000 [=================>............] - ETA: 9:55 - loss: 1.9439 - regression_loss: 1.4454 - classification_loss: 0.4984
1884/3000 [=================>............] - ETA: 9:54 - loss: 1.9435 - regression_loss: 1.4451 - classification_loss: 0.4984
1885/3000 [=================>............] - ETA: 9:54 - loss: 1.9429 - regression_loss: 1.4445 - classification_loss: 0.4984
1886/3000 [=================>............] - ETA: 9:53 - loss: 1.9422 - regression_loss: 1.4440 - classification_loss: 0.4983
1887/3000 [=================>............] - ETA: 9:53 - loss: 1.9419 - regression_loss: 1.4437 - classification_loss: 0.4982
1888/3000 [=================>............] - ETA: 9:52 - loss: 1.9415 - regression_loss: 1.4434 - classification_loss: 0.4982
1889/3000 [=================>............] - ETA: 9:51 - loss: 1.9413 - regression_loss: 1.4431 - classification_loss: 0.4981
1890/3000 [=================>............] - ETA: 9:51 - loss: 1.9406 - regression_loss: 1.4426 - classification_loss: 0.4980
1891/3000 [=================>............] - ETA: 9:50 - loss: 1.9401 - regression_loss: 1.4422 - classification_loss: 0.4980
1892/3000 [=================>............] - ETA: 9:50 - loss: 1.9398 - regression_loss: 1.4419 - classification_loss: 0.4979
1893/3000 [=================>............] - ETA: 9:49 - loss: 1.9393 - regression_loss: 1.4415 - classification_loss: 0.4979
1894/3000 [=================>............] - ETA: 9:48 - loss: 1.9387 - regression_loss: 1.4409 - classification_loss: 0.4978
1895/3000 [=================>............] - ETA: 9:48 - loss: 1.9383 - regression_loss: 1.4406 - classification_loss: 0.4977
1896/3000 [=================>............] - ETA: 9:47 - loss: 1.9378 - regression_loss: 1.4401 - classification_loss: 0.4976
1897/3000 [=================>............] - ETA: 9:47 - loss: 1.9374 - regression_loss: 1.4398 - classification_loss: 0.4976
1898/3000 [=================>............] - ETA: 9:46 - loss: 1.9371 - regression_loss: 1.4395 - classification_loss: 0.4976
1899/3000 [=================>............] - ETA: 9:45 - loss: 1.9367 - regression_loss: 1.4392 - classification_loss: 0.4975
1900/3000 [==================>...........] - ETA: 9:45 - loss: 1.9364 - regression_loss: 1.4389 - classification_loss: 0.4975
1901/3000 [==================>...........] - ETA: 9:44 - loss: 1.9359 - regression_loss: 1.4385 - classification_loss: 0.4974
1902/3000 [==================>...........] - ETA: 9:43 - loss: 1.9356 - regression_loss: 1.4382 - classification_loss: 0.4974
1903/3000 [==================>...........] - ETA: 9:43 - loss: 1.9355 - regression_loss: 1.4381 - classification_loss: 0.4973
1904/3000 [==================>...........] - ETA: 9:42 - loss: 1.9351 - regression_loss: 1.4378 - classification_loss: 0.4973
1905/3000 [==================>...........] - ETA: 9:42 - loss: 1.9345 - regression_loss: 1.4372 - classification_loss: 0.4972
1906/3000 [==================>...........] - ETA: 9:41 - loss: 1.9341 - regression_loss: 1.4370 - classification_loss: 0.4972
1907/3000 [==================>...........] - ETA: 9:40 - loss: 1.9337 - regression_loss: 1.4366 - classification_loss: 0.4971
1908/3000 [==================>...........] - ETA: 9:40 - loss: 1.9331 - regression_loss: 1.4361 - classification_loss: 0.4970
1909/3000 [==================>...........] - ETA: 9:39 - loss: 1.9325 - regression_loss: 1.4356 - classification_loss: 0.4969
1910/3000 [==================>...........] - ETA: 9:39 - loss: 1.9323 - regression_loss: 1.4354 - classification_loss: 0.4968
1911/3000 [==================>...........] - ETA: 9:38 - loss: 1.9318 - regression_loss: 1.4350 - classification_loss: 0.4967
1912/3000 [==================>...........] - ETA: 9:37 - loss: 1.9314 - regression_loss: 1.4347 - classification_loss: 0.4967
1913/3000 [==================>...........] - ETA: 9:37 - loss: 1.9312 - regression_loss: 1.4345 - classification_loss: 0.4967
1914/3000 [==================>...........] - ETA: 9:36 - loss: 1.9307 - regression_loss: 1.4341 - classification_loss: 0.4967
1915/3000 [==================>...........] - ETA: 9:36 - loss: 1.9301 - regression_loss: 1.4336 - classification_loss: 0.4966
1916/3000 [==================>...........] - ETA: 9:35 - loss: 1.9296 - regression_loss: 1.4330 - classification_loss: 0.4966
1917/3000 [==================>...........] - ETA: 9:35 - loss: 1.9293 - regression_loss: 1.4328 - classification_loss: 0.4965
1918/3000 [==================>...........] - ETA: 9:34 - loss: 1.9289 - regression_loss: 1.4325 - classification_loss: 0.4964
1919/3000 [==================>...........] - ETA: 9:33 - loss: 1.9286 - regression_loss: 1.4323 - classification_loss: 0.4963
1920/3000 [==================>...........] - ETA: 9:33 - loss: 1.9283 - regression_loss: 1.4320 - classification_loss: 0.4962
1921/3000 [==================>...........] - ETA: 9:32 - loss: 1.9278 - regression_loss: 1.4316 - classification_loss: 0.4962
1922/3000 [==================>...........] - ETA: 9:32 - loss: 1.9273 - regression_loss: 1.4311 - classification_loss: 0.4961
1923/3000 [==================>...........] - ETA: 9:31 - loss: 1.9270 - regression_loss: 1.4309 - classification_loss: 0.4961
1924/3000 [==================>...........] - ETA: 9:30 - loss: 1.9266 - regression_loss: 1.4306 - classification_loss: 0.4960
1925/3000 [==================>...........] - ETA: 9:30 - loss: 1.9260 - regression_loss: 1.4301 - classification_loss: 0.4959
1926/3000 [==================>...........] - ETA: 9:29 - loss: 1.9256 - regression_loss: 1.4297 - classification_loss: 0.4959
1927/3000 [==================>...........] - ETA: 9:29 - loss: 1.9251 - regression_loss: 1.4292 - classification_loss: 0.4959
1928/3000 [==================>...........] - ETA: 9:28 - loss: 1.9245 - regression_loss: 1.4287 - classification_loss: 0.4958
1929/3000 [==================>...........] - ETA: 9:27 - loss: 1.9241 - regression_loss: 1.4284 - classification_loss: 0.4957
1930/3000 [==================>...........] - ETA: 9:27 - loss: 1.9237 - regression_loss: 1.4280 - classification_loss: 0.4957
1931/3000 [==================>...........] - ETA: 9:26 - loss: 1.9232 - regression_loss: 1.4276 - classification_loss: 0.4956
1932/3000 [==================>...........] - ETA: 9:26 - loss: 1.9231 - regression_loss: 1.4275 - classification_loss: 0.4956
1933/3000 [==================>...........] - ETA: 9:25 - loss: 1.9225 - regression_loss: 1.4270 - classification_loss: 0.4955
1934/3000 [==================>...........] - ETA: 9:24 - loss: 1.9220 - regression_loss: 1.4266 - classification_loss: 0.4954
1935/3000 [==================>...........] - ETA: 9:24 - loss: 1.9215 - regression_loss: 1.4261 - classification_loss: 0.4954
1936/3000 [==================>...........] - ETA: 9:23 - loss: 1.9208 - regression_loss: 1.4255 - classification_loss: 0.4953
1937/3000 [==================>...........] - ETA: 9:23 - loss: 1.9203 - regression_loss: 1.4251 - classification_loss: 0.4952
1938/3000 [==================>...........] - ETA: 9:22 - loss: 1.9199 - regression_loss: 1.4247 - classification_loss: 0.4952
1939/3000 [==================>...........] - ETA: 9:21 - loss: 1.9196 - regression_loss: 1.4244 - classification_loss: 0.4952
1940/3000 [==================>...........] - ETA: 9:21 - loss: 1.9190 - regression_loss: 1.4239 - classification_loss: 0.4951
1941/3000 [==================>...........] - ETA: 9:20 - loss: 1.9185 - regression_loss: 1.4235 - classification_loss: 0.4950
1942/3000 [==================>...........] - ETA: 9:20 - loss: 1.9180 - regression_loss: 1.4230 - classification_loss: 0.4950
1943/3000 [==================>...........] - ETA: 9:19 - loss: 1.9175 - regression_loss: 1.4226 - classification_loss: 0.4949
1944/3000 [==================>...........] - ETA: 9:18 - loss: 1.9170 - regression_loss: 1.4223 - classification_loss: 0.4948
1945/3000 [==================>...........] - ETA: 9:18 - loss: 1.9166 - regression_loss: 1.4219 - classification_loss: 0.4947
1946/3000 [==================>...........] - ETA: 9:17 - loss: 1.9160 - regression_loss: 1.4213 - classification_loss: 0.4946
1947/3000 [==================>...........] - ETA: 9:17 - loss: 1.9155 - regression_loss: 1.4209 - classification_loss: 0.4946
1948/3000 [==================>...........] - ETA: 9:16 - loss: 1.9152 - regression_loss: 1.4207 - classification_loss: 0.4945
1949/3000 [==================>...........] - ETA: 9:15 - loss: 1.9147 - regression_loss: 1.4203 - classification_loss: 0.4945
1950/3000 [==================>...........] - ETA: 9:15 - loss: 1.9142 - regression_loss: 1.4198 - classification_loss: 0.4944
1951/3000 [==================>...........] - ETA: 9:14 - loss: 1.9137 - regression_loss: 1.4193 - classification_loss: 0.4943
1952/3000 [==================>...........] - ETA: 9:14 - loss: 1.9131 - regression_loss: 1.4189 - classification_loss: 0.4942
1953/3000 [==================>...........] - ETA: 9:13 - loss: 1.9127 - regression_loss: 1.4185 - classification_loss: 0.4942
1954/3000 [==================>...........] - ETA: 9:12 - loss: 1.9124 - regression_loss: 1.4183 - classification_loss: 0.4941
1955/3000 [==================>...........] - ETA: 9:12 - loss: 1.9120 - regression_loss: 1.4180 - classification_loss: 0.4940
1956/3000 [==================>...........] - ETA: 9:11 - loss: 1.9115 - regression_loss: 1.4176 - classification_loss: 0.4939
1957/3000 [==================>...........] - ETA: 9:11 - loss: 1.9111 - regression_loss: 1.4172 - classification_loss: 0.4939
1958/3000 [==================>...........] - ETA: 9:10 - loss: 1.9106 - regression_loss: 1.4167 - classification_loss: 0.4939
1959/3000 [==================>...........] - ETA: 9:10 - loss: 1.9104 - regression_loss: 1.4165 - classification_loss: 0.4938
1960/3000 [==================>...........] - ETA: 9:09 - loss: 1.9098 - regression_loss: 1.4161 - classification_loss: 0.4937
1961/3000 [==================>...........] - ETA: 9:08 - loss: 1.9093 - regression_loss: 1.4156 - classification_loss: 0.4937
1962/3000 [==================>...........] - ETA: 9:08 - loss: 1.9091 - regression_loss: 1.4154 - classification_loss: 0.4936
1963/3000 [==================>...........] - ETA: 9:07 - loss: 1.9086 - regression_loss: 1.4151 - classification_loss: 0.4935
1964/3000 [==================>...........] - ETA: 9:07 - loss: 1.9080 - regression_loss: 1.4146 - classification_loss: 0.4935
1965/3000 [==================>...........] - ETA: 9:06 - loss: 1.9075 - regression_loss: 1.4141 - classification_loss: 0.4934
1966/3000 [==================>...........] - ETA: 9:05 - loss: 1.9071 - regression_loss: 1.4137 - classification_loss: 0.4934
1967/3000 [==================>...........] - ETA: 9:05 - loss: 1.9069 - regression_loss: 1.4135 - classification_loss: 0.4933
1968/3000 [==================>...........] - ETA: 9:04 - loss: 1.9064 - regression_loss: 1.4131 - classification_loss: 0.4933
1969/3000 [==================>...........] - ETA: 9:04 - loss: 1.9061 - regression_loss: 1.4128 - classification_loss: 0.4933
1970/3000 [==================>...........] - ETA: 9:03 - loss: 1.9055 - regression_loss: 1.4124 - classification_loss: 0.4931
1971/3000 [==================>...........] - ETA: 9:02 - loss: 1.9052 - regression_loss: 1.4121 - classification_loss: 0.4931
1972/3000 [==================>...........] - ETA: 9:02 - loss: 1.9047 - regression_loss: 1.4117 - classification_loss: 0.4930
1973/3000 [==================>...........] - ETA: 9:01 - loss: 1.9045 - regression_loss: 1.4115 - classification_loss: 0.4930
1974/3000 [==================>...........] - ETA: 9:01 - loss: 1.9040 - regression_loss: 1.4110 - classification_loss: 0.4930
1975/3000 [==================>...........] - ETA: 9:00 - loss: 1.9034 - regression_loss: 1.4105 - classification_loss: 0.4929
1976/3000 [==================>...........] - ETA: 9:00 - loss: 1.9028 - regression_loss: 1.4100 - classification_loss: 0.4928
1977/3000 [==================>...........] - ETA: 8:59 - loss: 1.9024 - regression_loss: 1.4097 - classification_loss: 0.4928
1978/3000 [==================>...........] - ETA: 8:58 - loss: 1.9020 - regression_loss: 1.4093 - classification_loss: 0.4927
1979/3000 [==================>...........] - ETA: 8:58 - loss: 1.9015 - regression_loss: 1.4088 - classification_loss: 0.4927
1980/3000 [==================>...........] - ETA: 8:57 - loss: 1.9009 - regression_loss: 1.4084 - classification_loss: 0.4926
1981/3000 [==================>...........] - ETA: 8:57 - loss: 1.9007 - regression_loss: 1.4082 - classification_loss: 0.4925
1982/3000 [==================>...........] - ETA: 8:56 - loss: 1.9002 - regression_loss: 1.4077 - classification_loss: 0.4925
1983/3000 [==================>...........] - ETA: 8:55 - loss: 1.8998 - regression_loss: 1.4074 - classification_loss: 0.4925
1984/3000 [==================>...........] - ETA: 8:55 - loss: 1.8993 - regression_loss: 1.4069 - classification_loss: 0.4924
1985/3000 [==================>...........] - ETA: 8:54 - loss: 1.8987 - regression_loss: 1.4064 - classification_loss: 0.4923
1986/3000 [==================>...........] - ETA: 8:54 - loss: 1.8982 - regression_loss: 1.4059 - classification_loss: 0.4923
1987/3000 [==================>...........] - ETA: 8:53 - loss: 1.8978 - regression_loss: 1.4056 - classification_loss: 0.4922
1988/3000 [==================>...........] - ETA: 8:52 - loss: 1.8972 - regression_loss: 1.4051 - classification_loss: 0.4921
1989/3000 [==================>...........] - ETA: 8:52 - loss: 1.8967 - regression_loss: 1.4046 - classification_loss: 0.4921
1990/3000 [==================>...........] - ETA: 8:51 - loss: 1.8962 - regression_loss: 1.4041 - classification_loss: 0.4920
1991/3000 [==================>...........] - ETA: 8:51 - loss: 1.8956 - regression_loss: 1.4037 - classification_loss: 0.4920
1992/3000 [==================>...........] - ETA: 8:50 - loss: 1.8952 - regression_loss: 1.4033 - classification_loss: 0.4919
1993/3000 [==================>...........] - ETA: 8:50 - loss: 1.8949 - regression_loss: 1.4031 - classification_loss: 0.4918
1994/3000 [==================>...........] - ETA: 8:49 - loss: 1.8944 - regression_loss: 1.4027 - classification_loss: 0.4917
1995/3000 [==================>...........] - ETA: 8:48 - loss: 1.8941 - regression_loss: 1.4025 - classification_loss: 0.4916
1996/3000 [==================>...........] - ETA: 8:48 - loss: 1.8936 - regression_loss: 1.4020 - classification_loss: 0.4916
1997/3000 [==================>...........] - ETA: 8:47 - loss: 1.8933 - regression_loss: 1.4017 - classification_loss: 0.4916
1998/3000 [==================>...........] - ETA: 8:47 - loss: 1.8928 - regression_loss: 1.4013 - classification_loss: 0.4915
1999/3000 [==================>...........] - ETA: 8:46 - loss: 1.8924 - regression_loss: 1.4009 - classification_loss: 0.4914
2000/3000 [===================>..........] - ETA: 8:45 - loss: 1.8923 - regression_loss: 1.4010 - classification_loss: 0.4914
2001/3000 [===================>..........] - ETA: 8:45 - loss: 1.8921 - regression_loss: 1.4008 - classification_loss: 0.4914
2002/3000 [===================>..........] - ETA: 8:44 - loss: 1.8918 - regression_loss: 1.4005 - classification_loss: 0.4913
2003/3000 [===================>..........] - ETA: 8:44 - loss: 1.8912 - regression_loss: 1.4000 - classification_loss: 0.4912
2004/3000 [===================>..........] - ETA: 8:43 - loss: 1.8907 - regression_loss: 1.3996 - classification_loss: 0.4912
2005/3000 [===================>..........] - ETA: 8:43 - loss: 1.8906 - regression_loss: 1.3995 - classification_loss: 0.4911
2006/3000 [===================>..........] - ETA: 8:42 - loss: 1.8903 - regression_loss: 1.3992 - classification_loss: 0.4910
2007/3000 [===================>..........] - ETA: 8:41 - loss: 1.8898 - regression_loss: 1.3989 - classification_loss: 0.4910
2008/3000 [===================>..........] - ETA: 8:41 - loss: 1.8894 - regression_loss: 1.3985 - classification_loss: 0.4909
2009/3000 [===================>..........] - ETA: 8:40 - loss: 1.8891 - regression_loss: 1.3982 - classification_loss: 0.4908
2010/3000 [===================>..........] - ETA: 8:40 - loss: 1.8887 - regression_loss: 1.3980 - classification_loss: 0.4908
2011/3000 [===================>..........] - ETA: 8:39 - loss: 1.8884 - regression_loss: 1.3977 - classification_loss: 0.4907
2012/3000 [===================>..........] - ETA: 8:38 - loss: 1.8878 - regression_loss: 1.3973 - classification_loss: 0.4906
2013/3000 [===================>..........] - ETA: 8:38 - loss: 1.8874 - regression_loss: 1.3968 - classification_loss: 0.4905
2014/3000 [===================>..........] - ETA: 8:37 - loss: 1.8869 - regression_loss: 1.3964 - classification_loss: 0.4904
2015/3000 [===================>..........] - ETA: 8:37 - loss: 1.8864 - regression_loss: 1.3961 - classification_loss: 0.4904
2016/3000 [===================>..........] - ETA: 8:36 - loss: 1.8859 - regression_loss: 1.3957 - classification_loss: 0.4903
2017/3000 [===================>..........] - ETA: 8:36 - loss: 1.8855 - regression_loss: 1.3953 - classification_loss: 0.4902
2018/3000 [===================>..........] - ETA: 8:35 - loss: 1.8850 - regression_loss: 1.3949 - classification_loss: 0.4901
2019/3000 [===================>..........] - ETA: 8:34 - loss: 1.8846 - regression_loss: 1.3945 - classification_loss: 0.4901
2020/3000 [===================>..........] - ETA: 8:34 - loss: 1.8843 - regression_loss: 1.3942 - classification_loss: 0.4900
2021/3000 [===================>..........] - ETA: 8:33 - loss: 1.8838 - regression_loss: 1.3938 - classification_loss: 0.4900
2022/3000 [===================>..........] - ETA: 8:33 - loss: 1.8833 - regression_loss: 1.3933 - classification_loss: 0.4900
2023/3000 [===================>..........] - ETA: 8:32 - loss: 1.8831 - regression_loss: 1.3932 - classification_loss: 0.4899
2024/3000 [===================>..........] - ETA: 8:31 - loss: 1.8825 - regression_loss: 1.3927 - classification_loss: 0.4898
2025/3000 [===================>..........] - ETA: 8:31 - loss: 1.8821 - regression_loss: 1.3924 - classification_loss: 0.4897
2026/3000 [===================>..........] - ETA: 8:30 - loss: 1.8821 - regression_loss: 1.3924 - classification_loss: 0.4897
2027/3000 [===================>..........] - ETA: 8:30 - loss: 1.8819 - regression_loss: 1.3922 - classification_loss: 0.4896
2028/3000 [===================>..........] - ETA: 8:29 - loss: 1.8816 - regression_loss: 1.3920 - classification_loss: 0.4896
2029/3000 [===================>..........] - ETA: 8:29 - loss: 1.8812 - regression_loss: 1.3916 - classification_loss: 0.4895
2030/3000 [===================>..........] - ETA: 8:28 - loss: 1.8808 - regression_loss: 1.3913 - classification_loss: 0.4895
2031/3000 [===================>..........] - ETA: 8:27 - loss: 1.8802 - regression_loss: 1.3908 - classification_loss: 0.4894
2032/3000 [===================>..........] - ETA: 8:27 - loss: 1.8801 - regression_loss: 1.3908 - classification_loss: 0.4893
2033/3000 [===================>..........] - ETA: 8:26 - loss: 1.8797 - regression_loss: 1.3904 - classification_loss: 0.4893
2034/3000 [===================>..........] - ETA: 8:26 - loss: 1.8793 - regression_loss: 1.3900 - classification_loss: 0.4892
2035/3000 [===================>..........] - ETA: 8:25 - loss: 1.8792 - regression_loss: 1.3901 - classification_loss: 0.4892
2036/3000 [===================>..........] - ETA: 8:25 - loss: 1.8787 - regression_loss: 1.3896 - classification_loss: 0.4891
2037/3000 [===================>..........] - ETA: 8:24 - loss: 1.8782 - regression_loss: 1.3892 - classification_loss: 0.4890
2038/3000 [===================>..........] - ETA: 8:23 - loss: 1.8777 - regression_loss: 1.3887 - classification_loss: 0.4890
2039/3000 [===================>..........] - ETA: 8:23 - loss: 1.8773 - regression_loss: 1.3884 - classification_loss: 0.4889
2040/3000 [===================>..........] - ETA: 8:22 - loss: 1.8771 - regression_loss: 1.3883 - classification_loss: 0.4888
2041/3000 [===================>..........] - ETA: 8:22 - loss: 1.8766 - regression_loss: 1.3879 - classification_loss: 0.4887
2042/3000 [===================>..........] - ETA: 8:21 - loss: 1.8763 - regression_loss: 1.3876 - classification_loss: 0.4887
2043/3000 [===================>..........] - ETA: 8:21 - loss: 1.8759 - regression_loss: 1.3873 - classification_loss: 0.4886
2044/3000 [===================>..........] - ETA: 8:20 - loss: 1.8755 - regression_loss: 1.3870 - classification_loss: 0.4885
2045/3000 [===================>..........] - ETA: 8:19 - loss: 1.8751 - regression_loss: 1.3866 - classification_loss: 0.4885
2046/3000 [===================>..........] - ETA: 8:19 - loss: 1.8747 - regression_loss: 1.3862 - classification_loss: 0.4885
2047/3000 [===================>..........] - ETA: 8:18 - loss: 1.8742 - regression_loss: 1.3857 - classification_loss: 0.4885
2048/3000 [===================>..........] - ETA: 8:18 - loss: 1.8736 - regression_loss: 1.3852 - classification_loss: 0.4884
2049/3000 [===================>..........] - ETA: 8:17 - loss: 1.8730 - regression_loss: 1.3847 - classification_loss: 0.4884
2050/3000 [===================>..........] - ETA: 8:16 - loss: 1.8726 - regression_loss: 1.3843 - classification_loss: 0.4883
2051/3000 [===================>..........] - ETA: 8:16 - loss: 1.8724 - regression_loss: 1.3841 - classification_loss: 0.4883
2052/3000 [===================>..........] - ETA: 8:15 - loss: 1.8718 - regression_loss: 1.3836 - classification_loss: 0.4882
2053/3000 [===================>..........] - ETA: 8:15 - loss: 1.8715 - regression_loss: 1.3833 - classification_loss: 0.4882
2054/3000 [===================>..........] - ETA: 8:14 - loss: 1.8710 - regression_loss: 1.3828 - classification_loss: 0.4881
2055/3000 [===================>..........] - ETA: 8:14 - loss: 1.8707 - regression_loss: 1.3827 - classification_loss: 0.4881
2056/3000 [===================>..........] - ETA: 8:13 - loss: 1.8702 - regression_loss: 1.3822 - classification_loss: 0.4880
2057/3000 [===================>..........] - ETA: 8:12 - loss: 1.8696 - regression_loss: 1.3818 - classification_loss: 0.4879
2058/3000 [===================>..........] - ETA: 8:12 - loss: 1.8692 - regression_loss: 1.3815 - classification_loss: 0.4878
2059/3000 [===================>..........] - ETA: 8:11 - loss: 1.8688 - regression_loss: 1.3811 - classification_loss: 0.4877
2060/3000 [===================>..........] - ETA: 8:11 - loss: 1.8685 - regression_loss: 1.3809 - classification_loss: 0.4876
2061/3000 [===================>..........] - ETA: 8:10 - loss: 1.8682 - regression_loss: 1.3806 - classification_loss: 0.4876
2062/3000 [===================>..........] - ETA: 8:10 - loss: 1.8677 - regression_loss: 1.3802 - classification_loss: 0.4875
2063/3000 [===================>..........] - ETA: 8:09 - loss: 1.8672 - regression_loss: 1.3797 - classification_loss: 0.4875
2064/3000 [===================>..........] - ETA: 8:08 - loss: 1.8670 - regression_loss: 1.3796 - classification_loss: 0.4874
2065/3000 [===================>..........] - ETA: 8:08 - loss: 1.8666 - regression_loss: 1.3792 - classification_loss: 0.4874
2066/3000 [===================>..........] - ETA: 8:07 - loss: 1.8662 - regression_loss: 1.3788 - classification_loss: 0.4873
2067/3000 [===================>..........] - ETA: 8:07 - loss: 1.8656 - regression_loss: 1.3784 - classification_loss: 0.4873
2068/3000 [===================>..........] - ETA: 8:06 - loss: 1.8655 - regression_loss: 1.3782 - classification_loss: 0.4872
2069/3000 [===================>..........] - ETA: 8:06 - loss: 1.8651 - regression_loss: 1.3779 - classification_loss: 0.4872
2070/3000 [===================>..........] - ETA: 8:05 - loss: 1.8648 - regression_loss: 1.3777 - classification_loss: 0.4871
2071/3000 [===================>..........] - ETA: 8:04 - loss: 1.8644 - regression_loss: 1.3774 - classification_loss: 0.4871
2072/3000 [===================>..........] - ETA: 8:04 - loss: 1.8643 - regression_loss: 1.3772 - classification_loss: 0.4870
2073/3000 [===================>..........] - ETA: 8:03 - loss: 1.8642 - regression_loss: 1.3772 - classification_loss: 0.4870
2074/3000 [===================>..........] - ETA: 8:03 - loss: 1.8638 - regression_loss: 1.3768 - classification_loss: 0.4869
2075/3000 [===================>..........] - ETA: 8:02 - loss: 1.8635 - regression_loss: 1.3766 - classification_loss: 0.4869
2076/3000 [===================>..........] - ETA: 8:02 - loss: 1.8631 - regression_loss: 1.3763 - classification_loss: 0.4868
2077/3000 [===================>..........] - ETA: 8:01 - loss: 1.8626 - regression_loss: 1.3758 - classification_loss: 0.4868
2078/3000 [===================>..........] - ETA: 8:00 - loss: 1.8621 - regression_loss: 1.3754 - classification_loss: 0.4867
2079/3000 [===================>..........] - ETA: 8:00 - loss: 1.8616 - regression_loss: 1.3749 - classification_loss: 0.4867
2080/3000 [===================>..........] - ETA: 7:59 - loss: 1.8613 - regression_loss: 1.3747 - classification_loss: 0.4867
2081/3000 [===================>..........] - ETA: 7:59 - loss: 1.8608 - regression_loss: 1.3742 - classification_loss: 0.4866
2082/3000 [===================>..........] - ETA: 7:58 - loss: 1.8605 - regression_loss: 1.3740 - classification_loss: 0.4865
2083/3000 [===================>..........] - ETA: 7:58 - loss: 1.8599 - regression_loss: 1.3735 - classification_loss: 0.4864
2084/3000 [===================>..........] - ETA: 7:57 - loss: 1.8596 - regression_loss: 1.3733 - classification_loss: 0.4864
2085/3000 [===================>..........] - ETA: 7:56 - loss: 1.8594 - regression_loss: 1.3731 - classification_loss: 0.4863
2086/3000 [===================>..........] - ETA: 7:56 - loss: 1.8590 - regression_loss: 1.3728 - classification_loss: 0.4862
2087/3000 [===================>..........] - ETA: 7:55 - loss: 1.8587 - regression_loss: 1.3725 - classification_loss: 0.4862
2088/3000 [===================>..........] - ETA: 7:55 - loss: 1.8582 - regression_loss: 1.3721 - classification_loss: 0.4861
2089/3000 [===================>..........] - ETA: 7:54 - loss: 1.8577 - regression_loss: 1.3717 - classification_loss: 0.4860
2090/3000 [===================>..........] - ETA: 7:54 - loss: 1.8572 - regression_loss: 1.3713 - classification_loss: 0.4859
2091/3000 [===================>..........] - ETA: 7:53 - loss: 1.8569 - regression_loss: 1.3711 - classification_loss: 0.4858
2092/3000 [===================>..........] - ETA: 7:53 - loss: 1.8565 - regression_loss: 1.3707 - classification_loss: 0.4858
2093/3000 [===================>..........] - ETA: 7:52 - loss: 1.8564 - regression_loss: 1.3707 - classification_loss: 0.4857
2094/3000 [===================>..........] - ETA: 7:51 - loss: 1.8560 - regression_loss: 1.3703 - classification_loss: 0.4856
2095/3000 [===================>..........] - ETA: 7:51 - loss: 1.8555 - regression_loss: 1.3699 - classification_loss: 0.4856
2096/3000 [===================>..........] - ETA: 7:50 - loss: 1.8551 - regression_loss: 1.3695 - classification_loss: 0.4855
2097/3000 [===================>..........] - ETA: 7:50 - loss: 1.8547 - regression_loss: 1.3693 - classification_loss: 0.4854
2098/3000 [===================>..........] - ETA: 7:49 - loss: 1.8544 - regression_loss: 1.3691 - classification_loss: 0.4854
2099/3000 [===================>..........] - ETA: 7:49 - loss: 1.8543 - regression_loss: 1.3689 - classification_loss: 0.4853
2100/3000 [====================>.........] - ETA: 7:48 - loss: 1.8538 - regression_loss: 1.3685 - classification_loss: 0.4853
2101/3000 [====================>.........] - ETA: 7:47 - loss: 1.8534 - regression_loss: 1.3681 - classification_loss: 0.4852
2102/3000 [====================>.........] - ETA: 7:47 - loss: 1.8530 - regression_loss: 1.3678 - classification_loss: 0.4852
2103/3000 [====================>.........] - ETA: 7:46 - loss: 1.8528 - regression_loss: 1.3677 - classification_loss: 0.4851
2104/3000 [====================>.........] - ETA: 7:46 - loss: 1.8525 - regression_loss: 1.3673 - classification_loss: 0.4852
2105/3000 [====================>.........] - ETA: 7:45 - loss: 1.8521 - regression_loss: 1.3669 - classification_loss: 0.4852
2106/3000 [====================>.........] - ETA: 7:45 - loss: 1.8515 - regression_loss: 1.3664 - classification_loss: 0.4851
2107/3000 [====================>.........] - ETA: 7:44 - loss: 1.8510 - regression_loss: 1.3660 - classification_loss: 0.4850
2108/3000 [====================>.........] - ETA: 7:43 - loss: 1.8506 - regression_loss: 1.3656 - classification_loss: 0.4850
2109/3000 [====================>.........] - ETA: 7:43 - loss: 1.8502 - regression_loss: 1.3653 - classification_loss: 0.4850
2110/3000 [====================>.........] - ETA: 7:42 - loss: 1.8499 - regression_loss: 1.3649 - classification_loss: 0.4850
2111/3000 [====================>.........] - ETA: 7:42 - loss: 1.8497 - regression_loss: 1.3648 - classification_loss: 0.4849
2112/3000 [====================>.........] - ETA: 7:41 - loss: 1.8492 - regression_loss: 1.3643 - classification_loss: 0.4849
2113/3000 [====================>.........] - ETA: 7:41 - loss: 1.8489 - regression_loss: 1.3640 - classification_loss: 0.4849
2114/3000 [====================>.........] - ETA: 7:40 - loss: 1.8484 - regression_loss: 1.3636 - classification_loss: 0.4848
2115/3000 [====================>.........] - ETA: 7:39 - loss: 1.8480 - regression_loss: 1.3632 - classification_loss: 0.4848
2116/3000 [====================>.........] - ETA: 7:39 - loss: 1.8477 - regression_loss: 1.3629 - classification_loss: 0.4848
2117/3000 [====================>.........] - ETA: 7:38 - loss: 1.8472 - regression_loss: 1.3625 - classification_loss: 0.4847
2118/3000 [====================>.........] - ETA: 7:38 - loss: 1.8469 - regression_loss: 1.3622 - classification_loss: 0.4847
2119/3000 [====================>.........] - ETA: 7:37 - loss: 1.8463 - regression_loss: 1.3618 - classification_loss: 0.4846
2120/3000 [====================>.........] - ETA: 7:37 - loss: 1.8458 - regression_loss: 1.3614 - classification_loss: 0.4845
2121/3000 [====================>.........] - ETA: 7:36 - loss: 1.8455 - regression_loss: 1.3611 - classification_loss: 0.4844
2122/3000 [====================>.........] - ETA: 7:36 - loss: 1.8451 - regression_loss: 1.3608 - classification_loss: 0.4843
2123/3000 [====================>.........] - ETA: 7:35 - loss: 1.8447 - regression_loss: 1.3604 - classification_loss: 0.4843
2124/3000 [====================>.........] - ETA: 7:34 - loss: 1.8442 - regression_loss: 1.3601 - classification_loss: 0.4841
2125/3000 [====================>.........] - ETA: 7:34 - loss: 1.8439 - regression_loss: 1.3597 - classification_loss: 0.4842
2126/3000 [====================>.........] - ETA: 7:33 - loss: 1.8435 - regression_loss: 1.3593 - classification_loss: 0.4842
2127/3000 [====================>.........] - ETA: 7:33 - loss: 1.8432 - regression_loss: 1.3591 - classification_loss: 0.4841
2128/3000 [====================>.........] - ETA: 7:32 - loss: 1.8428 - regression_loss: 1.3586 - classification_loss: 0.4841
2129/3000 [====================>.........] - ETA: 7:32 - loss: 1.8423 - regression_loss: 1.3583 - classification_loss: 0.4841
2130/3000 [====================>.........] - ETA: 7:31 - loss: 1.8419 - regression_loss: 1.3579 - classification_loss: 0.4840
2131/3000 [====================>.........] - ETA: 7:30 - loss: 1.8417 - regression_loss: 1.3577 - classification_loss: 0.4840
2132/3000 [====================>.........] - ETA: 7:30 - loss: 1.8414 - regression_loss: 1.3575 - classification_loss: 0.4839
2133/3000 [====================>.........] - ETA: 7:29 - loss: 1.8410 - regression_loss: 1.3570 - classification_loss: 0.4839
2134/3000 [====================>.........] - ETA: 7:29 - loss: 1.8406 - regression_loss: 1.3567 - classification_loss: 0.4839
2135/3000 [====================>.........] - ETA: 7:28 - loss: 1.8403 - regression_loss: 1.3565 - classification_loss: 0.4838
2136/3000 [====================>.........] - ETA: 7:28 - loss: 1.8400 - regression_loss: 1.3561 - classification_loss: 0.4838
2137/3000 [====================>.........] - ETA: 7:27 - loss: 1.8397 - regression_loss: 1.3560 - classification_loss: 0.4838
2138/3000 [====================>.........] - ETA: 7:26 - loss: 1.8393 - regression_loss: 1.3555 - classification_loss: 0.4837
2139/3000 [====================>.........] - ETA: 7:26 - loss: 1.8388 - regression_loss: 1.3551 - classification_loss: 0.4837
2140/3000 [====================>.........] - ETA: 7:25 - loss: 1.8384 - regression_loss: 1.3548 - classification_loss: 0.4836
2141/3000 [====================>.........] - ETA: 7:25 - loss: 1.8379 - regression_loss: 1.3543 - classification_loss: 0.4836
2142/3000 [====================>.........] - ETA: 7:24 - loss: 1.8374 - regression_loss: 1.3539 - classification_loss: 0.4835
2143/3000 [====================>.........] - ETA: 7:24 - loss: 1.8370 - regression_loss: 1.3535 - classification_loss: 0.4835
2144/3000 [====================>.........] - ETA: 7:23 - loss: 1.8366 - regression_loss: 1.3531 - classification_loss: 0.4834
2145/3000 [====================>.........] - ETA: 7:23 - loss: 1.8362 - regression_loss: 1.3528 - classification_loss: 0.4834
2146/3000 [====================>.........] - ETA: 7:22 - loss: 1.8360 - regression_loss: 1.3527 - classification_loss: 0.4833
2147/3000 [====================>.........] - ETA: 7:21 - loss: 1.8357 - regression_loss: 1.3524 - classification_loss: 0.4833
2148/3000 [====================>.........] - ETA: 7:21 - loss: 1.8355 - regression_loss: 1.3522 - classification_loss: 0.4832
2149/3000 [====================>.........] - ETA: 7:20 - loss: 1.8351 - regression_loss: 1.3519 - classification_loss: 0.4832
2150/3000 [====================>.........] - ETA: 7:20 - loss: 1.8346 - regression_loss: 1.3514 - classification_loss: 0.4832
2151/3000 [====================>.........] - ETA: 7:19 - loss: 1.8342 - regression_loss: 1.3511 - classification_loss: 0.4831
2152/3000 [====================>.........] - ETA: 7:19 - loss: 1.8338 - regression_loss: 1.3508 - classification_loss: 0.4830
2153/3000 [====================>.........] - ETA: 7:18 - loss: 1.8334 - regression_loss: 1.3505 - classification_loss: 0.4829
2154/3000 [====================>.........] - ETA: 7:18 - loss: 1.8331 - regression_loss: 1.3502 - classification_loss: 0.4829
2155/3000 [====================>.........] - ETA: 7:17 - loss: 1.8325 - regression_loss: 1.3497 - classification_loss: 0.4828
2156/3000 [====================>.........] - ETA: 7:16 - loss: 1.8322 - regression_loss: 1.3494 - classification_loss: 0.4827
2157/3000 [====================>.........] - ETA: 7:16 - loss: 1.8319 - regression_loss: 1.3493 - classification_loss: 0.4827
2158/3000 [====================>.........] - ETA: 7:15 - loss: 1.8314 - regression_loss: 1.3488 - classification_loss: 0.4826
2159/3000 [====================>.........] - ETA: 7:15 - loss: 1.8309 - regression_loss: 1.3484 - classification_loss: 0.4825
2160/3000 [====================>.........] - ETA: 7:14 - loss: 1.8304 - regression_loss: 1.3479 - classification_loss: 0.4825
2161/3000 [====================>.........] - ETA: 7:14 - loss: 1.8299 - regression_loss: 1.3475 - classification_loss: 0.4824
2162/3000 [====================>.........] - ETA: 7:13 - loss: 1.8297 - regression_loss: 1.3473 - classification_loss: 0.4824
2163/3000 [====================>.........] - ETA: 7:12 - loss: 1.8294 - regression_loss: 1.3470 - classification_loss: 0.4824
2164/3000 [====================>.........] - ETA: 7:12 - loss: 1.8288 - regression_loss: 1.3465 - classification_loss: 0.4823
2165/3000 [====================>.........] - ETA: 7:11 - loss: 1.8284 - regression_loss: 1.3461 - classification_loss: 0.4823
2166/3000 [====================>.........] - ETA: 7:11 - loss: 1.8280 - regression_loss: 1.3458 - classification_loss: 0.4822
2167/3000 [====================>.........] - ETA: 7:10 - loss: 1.8276 - regression_loss: 1.3455 - classification_loss: 0.4822
2168/3000 [====================>.........] - ETA: 7:10 - loss: 1.8272 - regression_loss: 1.3451 - classification_loss: 0.4821
2169/3000 [====================>.........] - ETA: 7:09 - loss: 1.8269 - regression_loss: 1.3449 - classification_loss: 0.4821
2170/3000 [====================>.........] - ETA: 7:09 - loss: 1.8267 - regression_loss: 1.3447 - classification_loss: 0.4821
2171/3000 [====================>.........] - ETA: 7:08 - loss: 1.8264 - regression_loss: 1.3444 - classification_loss: 0.4821
2172/3000 [====================>.........] - ETA: 7:07 - loss: 1.8262 - regression_loss: 1.3442 - classification_loss: 0.4820
2173/3000 [====================>.........] - ETA: 7:07 - loss: 1.8259 - regression_loss: 1.3440 - classification_loss: 0.4819
2174/3000 [====================>.........] - ETA: 7:06 - loss: 1.8256 - regression_loss: 1.3437 - classification_loss: 0.4819
2175/3000 [====================>.........] - ETA: 7:06 - loss: 1.8252 - regression_loss: 1.3434 - classification_loss: 0.4818
2176/3000 [====================>.........] - ETA: 7:05 - loss: 1.8248 - regression_loss: 1.3430 - classification_loss: 0.4818
2177/3000 [====================>.........] - ETA: 7:05 - loss: 1.8245 - regression_loss: 1.3428 - classification_loss: 0.4817
2178/3000 [====================>.........] - ETA: 7:04 - loss: 1.8244 - regression_loss: 1.3427 - classification_loss: 0.4817
2179/3000 [====================>.........] - ETA: 7:04 - loss: 1.8242 - regression_loss: 1.3426 - classification_loss: 0.4816
2180/3000 [====================>.........] - ETA: 7:03 - loss: 1.8238 - regression_loss: 1.3422 - classification_loss: 0.4816
2181/3000 [====================>.........] - ETA: 7:02 - loss: 1.8235 - regression_loss: 1.3419 - classification_loss: 0.4816
2182/3000 [====================>.........] - ETA: 7:02 - loss: 1.8233 - regression_loss: 1.3418 - classification_loss: 0.4815
2183/3000 [====================>.........] - ETA: 7:01 - loss: 1.8229 - regression_loss: 1.3415 - classification_loss: 0.4815
2184/3000 [====================>.........] - ETA: 7:01 - loss: 1.8226 - regression_loss: 1.3412 - classification_loss: 0.4814
2185/3000 [====================>.........] - ETA: 7:00 - loss: 1.8222 - regression_loss: 1.3408 - classification_loss: 0.4814
2186/3000 [====================>.........] - ETA: 7:00 - loss: 1.8219 - regression_loss: 1.3406 - classification_loss: 0.4813
2187/3000 [====================>.........] - ETA: 6:59 - loss: 1.8220 - regression_loss: 1.3405 - classification_loss: 0.4815
2188/3000 [====================>.........] - ETA: 6:59 - loss: 1.8219 - regression_loss: 1.3404 - classification_loss: 0.4815
2189/3000 [====================>.........] - ETA: 6:58 - loss: 1.8215 - regression_loss: 1.3401 - classification_loss: 0.4814
2190/3000 [====================>.........] - ETA: 6:57 - loss: 1.8211 - regression_loss: 1.3397 - classification_loss: 0.4814
2191/3000 [====================>.........] - ETA: 6:57 - loss: 1.8207 - regression_loss: 1.3393 - classification_loss: 0.4814
2192/3000 [====================>.........] - ETA: 6:56 - loss: 1.8205 - regression_loss: 1.3392 - classification_loss: 0.4814
2193/3000 [====================>.........] - ETA: 6:56 - loss: 1.8201 - regression_loss: 1.3388 - classification_loss: 0.4813
2194/3000 [====================>.........] - ETA: 6:55 - loss: 1.8199 - regression_loss: 1.3386 - classification_loss: 0.4813
2195/3000 [====================>.........] - ETA: 6:55 - loss: 1.8198 - regression_loss: 1.3385 - classification_loss: 0.4813
2196/3000 [====================>.........] - ETA: 6:54 - loss: 1.8195 - regression_loss: 1.3383 - classification_loss: 0.4812
2197/3000 [====================>.........] - ETA: 6:54 - loss: 1.8191 - regression_loss: 1.3379 - classification_loss: 0.4812
2198/3000 [====================>.........] - ETA: 6:53 - loss: 1.8186 - regression_loss: 1.3374 - classification_loss: 0.4811
2199/3000 [====================>.........] - ETA: 6:52 - loss: 1.8182 - regression_loss: 1.3372 - classification_loss: 0.4810
2200/3000 [=====================>........] - ETA: 6:52 - loss: 1.8182 - regression_loss: 1.3372 - classification_loss: 0.4810
2201/3000 [=====================>........] - ETA: 6:51 - loss: 1.8178 - regression_loss: 1.3369 - classification_loss: 0.4809
2202/3000 [=====================>........] - ETA: 6:51 - loss: 1.8175 - regression_loss: 1.3366 - classification_loss: 0.4809
2203/3000 [=====================>........] - ETA: 6:50 - loss: 1.8172 - regression_loss: 1.3364 - classification_loss: 0.4808
2204/3000 [=====================>........] - ETA: 6:50 - loss: 1.8169 - regression_loss: 1.3361 - classification_loss: 0.4807
2205/3000 [=====================>........] - ETA: 6:49 - loss: 1.8167 - regression_loss: 1.3360 - classification_loss: 0.4807
2206/3000 [=====================>........] - ETA: 6:49 - loss: 1.8166 - regression_loss: 1.3359 - classification_loss: 0.4807
2207/3000 [=====================>........] - ETA: 6:48 - loss: 1.8163 - regression_loss: 1.3357 - classification_loss: 0.4806
2208/3000 [=====================>........] - ETA: 6:47 - loss: 1.8159 - regression_loss: 1.3354 - classification_loss: 0.4806
2209/3000 [=====================>........] - ETA: 6:47 - loss: 1.8155 - regression_loss: 1.3350 - classification_loss: 0.4805
2210/3000 [=====================>........] - ETA: 6:46 - loss: 1.8152 - regression_loss: 1.3347 - classification_loss: 0.4804
2211/3000 [=====================>........] - ETA: 6:46 - loss: 1.8148 - regression_loss: 1.3344 - classification_loss: 0.4804
2212/3000 [=====================>........] - ETA: 6:45 - loss: 1.8143 - regression_loss: 1.3340 - classification_loss: 0.4803
2213/3000 [=====================>........] - ETA: 6:45 - loss: 1.8141 - regression_loss: 1.3339 - classification_loss: 0.4802
2214/3000 [=====================>........] - ETA: 6:44 - loss: 1.8137 - regression_loss: 1.3335 - classification_loss: 0.4802
2215/3000 [=====================>........] - ETA: 6:44 - loss: 1.8134 - regression_loss: 1.3333 - classification_loss: 0.4801
2216/3000 [=====================>........] - ETA: 6:43 - loss: 1.8131 - regression_loss: 1.3330 - classification_loss: 0.4801
2217/3000 [=====================>........] - ETA: 6:43 - loss: 1.8127 - regression_loss: 1.3326 - classification_loss: 0.4800
2218/3000 [=====================>........] - ETA: 6:42 - loss: 1.8122 - regression_loss: 1.3322 - classification_loss: 0.4800
2219/3000 [=====================>........] - ETA: 6:41 - loss: 1.8117 - regression_loss: 1.3318 - classification_loss: 0.4799
2220/3000 [=====================>........] - ETA: 6:41 - loss: 1.8113 - regression_loss: 1.3315 - classification_loss: 0.4798
2221/3000 [=====================>........] - ETA: 6:40 - loss: 1.8110 - regression_loss: 1.3312 - classification_loss: 0.4798
2222/3000 [=====================>........] - ETA: 6:40 - loss: 1.8108 - regression_loss: 1.3311 - classification_loss: 0.4797
2223/3000 [=====================>........] - ETA: 6:39 - loss: 1.8105 - regression_loss: 1.3309 - classification_loss: 0.4797
2224/3000 [=====================>........] - ETA: 6:39 - loss: 1.8101 - regression_loss: 1.3305 - classification_loss: 0.4796
2225/3000 [=====================>........] - ETA: 6:38 - loss: 1.8098 - regression_loss: 1.3303 - classification_loss: 0.4795
2226/3000 [=====================>........] - ETA: 6:38 - loss: 1.8095 - regression_loss: 1.3301 - classification_loss: 0.4795
2227/3000 [=====================>........] - ETA: 6:37 - loss: 1.8090 - regression_loss: 1.3297 - classification_loss: 0.4794
2228/3000 [=====================>........] - ETA: 6:36 - loss: 1.8088 - regression_loss: 1.3294 - classification_loss: 0.4794
2229/3000 [=====================>........] - ETA: 6:36 - loss: 1.8084 - regression_loss: 1.3291 - classification_loss: 0.4793
2230/3000 [=====================>........] - ETA: 6:35 - loss: 1.8080 - regression_loss: 1.3288 - classification_loss: 0.4793
2231/3000 [=====================>........] - ETA: 6:35 - loss: 1.8077 - regression_loss: 1.3285 - classification_loss: 0.4792
2232/3000 [=====================>........] - ETA: 6:34 - loss: 1.8073 - regression_loss: 1.3282 - classification_loss: 0.4791
2233/3000 [=====================>........] - ETA: 6:34 - loss: 1.8071 - regression_loss: 1.3280 - classification_loss: 0.4791
2234/3000 [=====================>........] - ETA: 6:33 - loss: 1.8067 - regression_loss: 1.3277 - classification_loss: 0.4790
2235/3000 [=====================>........] - ETA: 6:33 - loss: 1.8065 - regression_loss: 1.3275 - classification_loss: 0.4789
2236/3000 [=====================>........] - ETA: 6:32 - loss: 1.8062 - regression_loss: 1.3273 - classification_loss: 0.4789
2237/3000 [=====================>........] - ETA: 6:31 - loss: 1.8059 - regression_loss: 1.3271 - classification_loss: 0.4788
2238/3000 [=====================>........] - ETA: 6:31 - loss: 1.8055 - regression_loss: 1.3267 - classification_loss: 0.4788
2239/3000 [=====================>........] - ETA: 6:30 - loss: 1.8052 - regression_loss: 1.3265 - classification_loss: 0.4787
2240/3000 [=====================>........] - ETA: 6:30 - loss: 1.8049 - regression_loss: 1.3263 - classification_loss: 0.4787
2241/3000 [=====================>........] - ETA: 6:29 - loss: 1.8046 - regression_loss: 1.3259 - classification_loss: 0.4786
2242/3000 [=====================>........] - ETA: 6:29 - loss: 1.8044 - regression_loss: 1.3258 - classification_loss: 0.4786
2243/3000 [=====================>........] - ETA: 6:28 - loss: 1.8040 - regression_loss: 1.3254 - classification_loss: 0.4785
2244/3000 [=====================>........] - ETA: 6:28 - loss: 1.8036 - regression_loss: 1.3251 - classification_loss: 0.4785
2245/3000 [=====================>........] - ETA: 6:27 - loss: 1.8032 - regression_loss: 1.3247 - classification_loss: 0.4785
2246/3000 [=====================>........] - ETA: 6:27 - loss: 1.8029 - regression_loss: 1.3244 - classification_loss: 0.4784
2247/3000 [=====================>........] - ETA: 6:26 - loss: 1.8024 - regression_loss: 1.3240 - classification_loss: 0.4784
2248/3000 [=====================>........] - ETA: 6:25 - loss: 1.8023 - regression_loss: 1.3240 - classification_loss: 0.4783
2249/3000 [=====================>........] - ETA: 6:25 - loss: 1.8019 - regression_loss: 1.3236 - classification_loss: 0.4783
2250/3000 [=====================>........] - ETA: 6:24 - loss: 1.8014 - regression_loss: 1.3232 - classification_loss: 0.4782
2251/3000 [=====================>........] - ETA: 6:24 - loss: 1.8010 - regression_loss: 1.3229 - classification_loss: 0.4781
2252/3000 [=====================>........] - ETA: 6:23 - loss: 1.8006 - regression_loss: 1.3226 - classification_loss: 0.4780
2253/3000 [=====================>........] - ETA: 6:23 - loss: 1.8001 - regression_loss: 1.3221 - classification_loss: 0.4780
2254/3000 [=====================>........] - ETA: 6:22 - loss: 1.7996 - regression_loss: 1.3217 - classification_loss: 0.4779
2255/3000 [=====================>........] - ETA: 6:22 - loss: 1.7994 - regression_loss: 1.3215 - classification_loss: 0.4779
2256/3000 [=====================>........] - ETA: 6:21 - loss: 1.7990 - regression_loss: 1.3211 - classification_loss: 0.4778
2257/3000 [=====================>........] - ETA: 6:21 - loss: 1.7985 - regression_loss: 1.3207 - classification_loss: 0.4778
2258/3000 [=====================>........] - ETA: 6:20 - loss: 1.7980 - regression_loss: 1.3203 - classification_loss: 0.4777
2259/3000 [=====================>........] - ETA: 6:19 - loss: 1.7977 - regression_loss: 1.3199 - classification_loss: 0.4777
2260/3000 [=====================>........] - ETA: 6:19 - loss: 1.7977 - regression_loss: 1.3199 - classification_loss: 0.4778
2261/3000 [=====================>........] - ETA: 6:18 - loss: 1.7974 - regression_loss: 1.3197 - classification_loss: 0.4777
2262/3000 [=====================>........] - ETA: 6:18 - loss: 1.7970 - regression_loss: 1.3194 - classification_loss: 0.4776
2263/3000 [=====================>........] - ETA: 6:17 - loss: 1.7967 - regression_loss: 1.3191 - classification_loss: 0.4776
2264/3000 [=====================>........] - ETA: 6:17 - loss: 1.7964 - regression_loss: 1.3188 - classification_loss: 0.4775
2265/3000 [=====================>........] - ETA: 6:16 - loss: 1.7961 - regression_loss: 1.3186 - classification_loss: 0.4775
2266/3000 [=====================>........] - ETA: 6:16 - loss: 1.7957 - regression_loss: 1.3183 - classification_loss: 0.4774
2267/3000 [=====================>........] - ETA: 6:15 - loss: 1.7954 - regression_loss: 1.3180 - classification_loss: 0.4774
2268/3000 [=====================>........] - ETA: 6:15 - loss: 1.7950 - regression_loss: 1.3176 - classification_loss: 0.4774
2269/3000 [=====================>........] - ETA: 6:14 - loss: 1.7948 - regression_loss: 1.3175 - classification_loss: 0.4774
2270/3000 [=====================>........] - ETA: 6:13 - loss: 1.7946 - regression_loss: 1.3173 - classification_loss: 0.4773
2271/3000 [=====================>........] - ETA: 6:13 - loss: 1.7944 - regression_loss: 1.3172 - classification_loss: 0.4772
2272/3000 [=====================>........] - ETA: 6:12 - loss: 1.7940 - regression_loss: 1.3169 - classification_loss: 0.4771
2273/3000 [=====================>........] - ETA: 6:12 - loss: 1.7938 - regression_loss: 1.3167 - classification_loss: 0.4771
2274/3000 [=====================>........] - ETA: 6:11 - loss: 1.7934 - regression_loss: 1.3164 - classification_loss: 0.4770
2275/3000 [=====================>........] - ETA: 6:11 - loss: 1.7932 - regression_loss: 1.3163 - classification_loss: 0.4770
2276/3000 [=====================>........] - ETA: 6:10 - loss: 1.7930 - regression_loss: 1.3161 - classification_loss: 0.4769
2277/3000 [=====================>........] - ETA: 6:10 - loss: 1.7926 - regression_loss: 1.3158 - classification_loss: 0.4768
2278/3000 [=====================>........] - ETA: 6:09 - loss: 1.7922 - regression_loss: 1.3154 - classification_loss: 0.4768
2279/3000 [=====================>........] - ETA: 6:09 - loss: 1.7919 - regression_loss: 1.3152 - classification_loss: 0.4767
2280/3000 [=====================>........] - ETA: 6:08 - loss: 1.7916 - regression_loss: 1.3150 - classification_loss: 0.4767
2281/3000 [=====================>........] - ETA: 6:07 - loss: 1.7915 - regression_loss: 1.3149 - classification_loss: 0.4766
2282/3000 [=====================>........] - ETA: 6:07 - loss: 1.7912 - regression_loss: 1.3146 - classification_loss: 0.4766
2283/3000 [=====================>........] - ETA: 6:06 - loss: 1.7911 - regression_loss: 1.3145 - classification_loss: 0.4765
2284/3000 [=====================>........] - ETA: 6:06 - loss: 1.7907 - regression_loss: 1.3142 - classification_loss: 0.4765
2285/3000 [=====================>........] - ETA: 6:05 - loss: 1.7904 - regression_loss: 1.3140 - classification_loss: 0.4765
2286/3000 [=====================>........] - ETA: 6:05 - loss: 1.7899 - regression_loss: 1.3135 - classification_loss: 0.4764
2287/3000 [=====================>........] - ETA: 6:04 - loss: 1.7896 - regression_loss: 1.3132 - classification_loss: 0.4764
2288/3000 [=====================>........] - ETA: 6:04 - loss: 1.7891 - regression_loss: 1.3128 - classification_loss: 0.4763
2289/3000 [=====================>........] - ETA: 6:03 - loss: 1.7889 - regression_loss: 1.3126 - classification_loss: 0.4763
2290/3000 [=====================>........] - ETA: 6:03 - loss: 1.7885 - regression_loss: 1.3123 - classification_loss: 0.4762
2291/3000 [=====================>........] - ETA: 6:02 - loss: 1.7880 - regression_loss: 1.3119 - classification_loss: 0.4761
2292/3000 [=====================>........] - ETA: 6:01 - loss: 1.7877 - regression_loss: 1.3117 - classification_loss: 0.4760
2293/3000 [=====================>........] - ETA: 6:01 - loss: 1.7874 - regression_loss: 1.3114 - classification_loss: 0.4759
2294/3000 [=====================>........] - ETA: 6:00 - loss: 1.7869 - regression_loss: 1.3111 - classification_loss: 0.4759
2295/3000 [=====================>........] - ETA: 6:00 - loss: 1.7867 - regression_loss: 1.3109 - classification_loss: 0.4758
2296/3000 [=====================>........] - ETA: 5:59 - loss: 1.7863 - regression_loss: 1.3105 - classification_loss: 0.4758
2297/3000 [=====================>........] - ETA: 5:59 - loss: 1.7860 - regression_loss: 1.3103 - classification_loss: 0.4757
2298/3000 [=====================>........] - ETA: 5:58 - loss: 1.7859 - regression_loss: 1.3102 - classification_loss: 0.4757
2299/3000 [=====================>........] - ETA: 5:58 - loss: 1.7855 - regression_loss: 1.3099 - classification_loss: 0.4756
2300/3000 [======================>.......] - ETA: 5:57 - loss: 1.7851 - regression_loss: 1.3095 - classification_loss: 0.4756
2301/3000 [======================>.......] - ETA: 5:57 - loss: 1.7848 - regression_loss: 1.3092 - classification_loss: 0.4756
2302/3000 [======================>.......] - ETA: 5:56 - loss: 1.7845 - regression_loss: 1.3089 - classification_loss: 0.4756
2303/3000 [======================>.......] - ETA: 5:56 - loss: 1.7841 - regression_loss: 1.3085 - classification_loss: 0.4755
2304/3000 [======================>.......] - ETA: 5:55 - loss: 1.7837 - regression_loss: 1.3082 - classification_loss: 0.4755
2305/3000 [======================>.......] - ETA: 5:54 - loss: 1.7833 - regression_loss: 1.3078 - classification_loss: 0.4754
2306/3000 [======================>.......] - ETA: 5:54 - loss: 1.7828 - regression_loss: 1.3074 - classification_loss: 0.4754
2307/3000 [======================>.......] - ETA: 5:53 - loss: 1.7825 - regression_loss: 1.3072 - classification_loss: 0.4753
2308/3000 [======================>.......] - ETA: 5:53 - loss: 1.7823 - regression_loss: 1.3070 - classification_loss: 0.4753
2309/3000 [======================>.......] - ETA: 5:52 - loss: 1.7820 - regression_loss: 1.3068 - classification_loss: 0.4753
2310/3000 [======================>.......] - ETA: 5:52 - loss: 1.7816 - regression_loss: 1.3064 - classification_loss: 0.4752
2311/3000 [======================>.......] - ETA: 5:51 - loss: 1.7814 - regression_loss: 1.3062 - classification_loss: 0.4752
2312/3000 [======================>.......] - ETA: 5:51 - loss: 1.7809 - regression_loss: 1.3058 - classification_loss: 0.4751
2313/3000 [======================>.......] - ETA: 5:50 - loss: 1.7805 - regression_loss: 1.3055 - classification_loss: 0.4750
2314/3000 [======================>.......] - ETA: 5:50 - loss: 1.7801 - regression_loss: 1.3051 - classification_loss: 0.4749
2315/3000 [======================>.......] - ETA: 5:49 - loss: 1.7797 - regression_loss: 1.3048 - classification_loss: 0.4749
2316/3000 [======================>.......] - ETA: 5:48 - loss: 1.7793 - regression_loss: 1.3044 - classification_loss: 0.4749
2317/3000 [======================>.......] - ETA: 5:48 - loss: 1.7789 - regression_loss: 1.3041 - classification_loss: 0.4748
2318/3000 [======================>.......] - ETA: 5:47 - loss: 1.7786 - regression_loss: 1.3039 - classification_loss: 0.4748
2319/3000 [======================>.......] - ETA: 5:47 - loss: 1.7784 - regression_loss: 1.3037 - classification_loss: 0.4747
2320/3000 [======================>.......] - ETA: 5:46 - loss: 1.7780 - regression_loss: 1.3033 - classification_loss: 0.4747
2321/3000 [======================>.......] - ETA: 5:46 - loss: 1.7775 - regression_loss: 1.3029 - classification_loss: 0.4746
2322/3000 [======================>.......] - ETA: 5:45 - loss: 1.7773 - regression_loss: 1.3027 - classification_loss: 0.4746
2323/3000 [======================>.......] - ETA: 5:45 - loss: 1.7769 - regression_loss: 1.3025 - classification_loss: 0.4745
2324/3000 [======================>.......] - ETA: 5:44 - loss: 1.7765 - regression_loss: 1.3021 - classification_loss: 0.4744
2325/3000 [======================>.......] - ETA: 5:44 - loss: 1.7761 - regression_loss: 1.3018 - classification_loss: 0.4744
2326/3000 [======================>.......] - ETA: 5:43 - loss: 1.7758 - regression_loss: 1.3015 - classification_loss: 0.4743
2327/3000 [======================>.......] - ETA: 5:43 - loss: 1.7754 - regression_loss: 1.3012 - classification_loss: 0.4742
2328/3000 [======================>.......] - ETA: 5:42 - loss: 1.7751 - regression_loss: 1.3009 - classification_loss: 0.4742
2329/3000 [======================>.......] - ETA: 5:41 - loss: 1.7746 - regression_loss: 1.3005 - classification_loss: 0.4741
2330/3000 [======================>.......] - ETA: 5:41 - loss: 1.7742 - regression_loss: 1.3002 - classification_loss: 0.4741
2331/3000 [======================>.......] - ETA: 5:40 - loss: 1.7738 - regression_loss: 1.2998 - classification_loss: 0.4740
2332/3000 [======================>.......] - ETA: 5:40 - loss: 1.7734 - regression_loss: 1.2995 - classification_loss: 0.4739
2333/3000 [======================>.......] - ETA: 5:39 - loss: 1.7729 - regression_loss: 1.2991 - classification_loss: 0.4738
2334/3000 [======================>.......] - ETA: 5:39 - loss: 1.7725 - regression_loss: 1.2987 - classification_loss: 0.4738
2335/3000 [======================>.......] - ETA: 5:38 - loss: 1.7721 - regression_loss: 1.2984 - classification_loss: 0.4737
2336/3000 [======================>.......] - ETA: 5:38 - loss: 1.7720 - regression_loss: 1.2983 - classification_loss: 0.4737
2337/3000 [======================>.......] - ETA: 5:37 - loss: 1.7718 - regression_loss: 1.2982 - classification_loss: 0.4736
2338/3000 [======================>.......] - ETA: 5:37 - loss: 1.7715 - regression_loss: 1.2979 - classification_loss: 0.4736
2339/3000 [======================>.......] - ETA: 5:36 - loss: 1.7713 - regression_loss: 1.2977 - classification_loss: 0.4736
2340/3000 [======================>.......] - ETA: 5:36 - loss: 1.7708 - regression_loss: 1.2973 - classification_loss: 0.4735
2341/3000 [======================>.......] - ETA: 5:35 - loss: 1.7707 - regression_loss: 1.2972 - classification_loss: 0.4735
2342/3000 [======================>.......] - ETA: 5:35 - loss: 1.7704 - regression_loss: 1.2970 - classification_loss: 0.4734
2343/3000 [======================>.......] - ETA: 5:34 - loss: 1.7700 - regression_loss: 1.2967 - classification_loss: 0.4733
2344/3000 [======================>.......] - ETA: 5:33 - loss: 1.7699 - regression_loss: 1.2965 - classification_loss: 0.4733
2345/3000 [======================>.......] - ETA: 5:33 - loss: 1.7695 - regression_loss: 1.2962 - classification_loss: 0.4733
2346/3000 [======================>.......] - ETA: 5:32 - loss: 1.7691 - regression_loss: 1.2958 - classification_loss: 0.4733
2347/3000 [======================>.......] - ETA: 5:32 - loss: 1.7687 - regression_loss: 1.2954 - classification_loss: 0.4733
2348/3000 [======================>.......] - ETA: 5:31 - loss: 1.7683 - regression_loss: 1.2951 - classification_loss: 0.4732
2349/3000 [======================>.......] - ETA: 5:31 - loss: 1.7681 - regression_loss: 1.2950 - classification_loss: 0.4732
2350/3000 [======================>.......] - ETA: 5:30 - loss: 1.7678 - regression_loss: 1.2947 - classification_loss: 0.4731
2351/3000 [======================>.......] - ETA: 5:30 - loss: 1.7676 - regression_loss: 1.2945 - classification_loss: 0.4730
2352/3000 [======================>.......] - ETA: 5:29 - loss: 1.7676 - regression_loss: 1.2946 - classification_loss: 0.4730
2353/3000 [======================>.......] - ETA: 5:29 - loss: 1.7672 - regression_loss: 1.2943 - classification_loss: 0.4730
2354/3000 [======================>.......] - ETA: 5:28 - loss: 1.7669 - regression_loss: 1.2940 - classification_loss: 0.4729
2355/3000 [======================>.......] - ETA: 5:28 - loss: 1.7667 - regression_loss: 1.2939 - classification_loss: 0.4729
2356/3000 [======================>.......] - ETA: 5:27 - loss: 1.7664 - regression_loss: 1.2936 - classification_loss: 0.4728
2357/3000 [======================>.......] - ETA: 5:26 - loss: 1.7661 - regression_loss: 1.2933 - classification_loss: 0.4728
2358/3000 [======================>.......] - ETA: 5:26 - loss: 1.7659 - regression_loss: 1.2932 - classification_loss: 0.4727
2359/3000 [======================>.......] - ETA: 5:25 - loss: 1.7656 - regression_loss: 1.2929 - classification_loss: 0.4727
2360/3000 [======================>.......] - ETA: 5:25 - loss: 1.7651 - regression_loss: 1.2925 - classification_loss: 0.4727
2361/3000 [======================>.......] - ETA: 5:24 - loss: 1.7647 - regression_loss: 1.2921 - classification_loss: 0.4726
2362/3000 [======================>.......] - ETA: 5:24 - loss: 1.7642 - regression_loss: 1.2916 - classification_loss: 0.4726
2363/3000 [======================>.......] - ETA: 5:23 - loss: 1.7637 - regression_loss: 1.2912 - classification_loss: 0.4725
2364/3000 [======================>.......] - ETA: 5:23 - loss: 1.7634 - regression_loss: 1.2910 - classification_loss: 0.4725
2365/3000 [======================>.......] - ETA: 5:22 - loss: 1.7630 - regression_loss: 1.2906 - classification_loss: 0.4724
2366/3000 [======================>.......] - ETA: 5:22 - loss: 1.7627 - regression_loss: 1.2903 - classification_loss: 0.4724
2367/3000 [======================>.......] - ETA: 5:21 - loss: 1.7623 - regression_loss: 1.2899 - classification_loss: 0.4724
2368/3000 [======================>.......] - ETA: 5:21 - loss: 1.7621 - regression_loss: 1.2898 - classification_loss: 0.4723
2369/3000 [======================>.......] - ETA: 5:20 - loss: 1.7618 - regression_loss: 1.2895 - classification_loss: 0.4723
2370/3000 [======================>.......] - ETA: 5:20 - loss: 1.7615 - regression_loss: 1.2892 - classification_loss: 0.4723
2371/3000 [======================>.......] - ETA: 5:19 - loss: 1.7611 - regression_loss: 1.2889 - classification_loss: 0.4722
2372/3000 [======================>.......] - ETA: 5:18 - loss: 1.7607 - regression_loss: 1.2886 - classification_loss: 0.4722
2373/3000 [======================>.......] - ETA: 5:18 - loss: 1.7604 - regression_loss: 1.2883 - classification_loss: 0.4721
2374/3000 [======================>.......] - ETA: 5:17 - loss: 1.7602 - regression_loss: 1.2882 - classification_loss: 0.4720
2375/3000 [======================>.......] - ETA: 5:17 - loss: 1.7599 - regression_loss: 1.2879 - classification_loss: 0.4720
2376/3000 [======================>.......] - ETA: 5:16 - loss: 1.7596 - regression_loss: 1.2876 - classification_loss: 0.4720
2377/3000 [======================>.......] - ETA: 5:16 - loss: 1.7593 - regression_loss: 1.2874 - classification_loss: 0.4719
2378/3000 [======================>.......] - ETA: 5:15 - loss: 1.7589 - regression_loss: 1.2870 - classification_loss: 0.4719
2379/3000 [======================>.......] - ETA: 5:15 - loss: 1.7586 - regression_loss: 1.2868 - classification_loss: 0.4718
2380/3000 [======================>.......] - ETA: 5:14 - loss: 1.7584 - regression_loss: 1.2866 - classification_loss: 0.4718
2381/3000 [======================>.......] - ETA: 5:14 - loss: 1.7579 - regression_loss: 1.2862 - classification_loss: 0.4717
2382/3000 [======================>.......] - ETA: 5:13 - loss: 1.7576 - regression_loss: 1.2859 - classification_loss: 0.4717
2383/3000 [======================>.......] - ETA: 5:13 - loss: 1.7574 - regression_loss: 1.2857 - classification_loss: 0.4717
2384/3000 [======================>.......] - ETA: 5:12 - loss: 1.7570 - regression_loss: 1.2854 - classification_loss: 0.4716
2385/3000 [======================>.......] - ETA: 5:12 - loss: 1.7567 - regression_loss: 1.2851 - classification_loss: 0.4716
2386/3000 [======================>.......] - ETA: 5:11 - loss: 1.7564 - regression_loss: 1.2849 - classification_loss: 0.4715
2387/3000 [======================>.......] - ETA: 5:10 - loss: 1.7560 - regression_loss: 1.2845 - classification_loss: 0.4715
2388/3000 [======================>.......] - ETA: 5:10 - loss: 1.7556 - regression_loss: 1.2842 - classification_loss: 0.4714
2389/3000 [======================>.......] - ETA: 5:09 - loss: 1.7553 - regression_loss: 1.2839 - classification_loss: 0.4714
2390/3000 [======================>.......] - ETA: 5:09 - loss: 1.7549 - regression_loss: 1.2835 - classification_loss: 0.4714
2391/3000 [======================>.......] - ETA: 5:08 - loss: 1.7545 - regression_loss: 1.2832 - classification_loss: 0.4713
2392/3000 [======================>.......] - ETA: 5:08 - loss: 1.7543 - regression_loss: 1.2831 - classification_loss: 0.4712
2393/3000 [======================>.......] - ETA: 5:07 - loss: 1.7540 - regression_loss: 1.2828 - classification_loss: 0.4712
2394/3000 [======================>.......] - ETA: 5:07 - loss: 1.7536 - regression_loss: 1.2825 - classification_loss: 0.4712
2395/3000 [======================>.......] - ETA: 5:06 - loss: 1.7534 - regression_loss: 1.2823 - classification_loss: 0.4711
2396/3000 [======================>.......] - ETA: 5:06 - loss: 1.7529 - regression_loss: 1.2819 - classification_loss: 0.4711
2397/3000 [======================>.......] - ETA: 5:05 - loss: 1.7526 - regression_loss: 1.2816 - classification_loss: 0.4711
2398/3000 [======================>.......] - ETA: 5:05 - loss: 1.7522 - regression_loss: 1.2812 - classification_loss: 0.4710
2399/3000 [======================>.......] - ETA: 5:04 - loss: 1.7519 - regression_loss: 1.2809 - classification_loss: 0.4710
2400/3000 [=======================>......] - ETA: 5:04 - loss: 1.7517 - regression_loss: 1.2808 - classification_loss: 0.4709
2401/3000 [=======================>......] - ETA: 5:03 - loss: 1.7515 - regression_loss: 1.2806 - classification_loss: 0.4709
2402/3000 [=======================>......] - ETA: 5:02 - loss: 1.7511 - regression_loss: 1.2802 - classification_loss: 0.4709
2403/3000 [=======================>......] - ETA: 5:02 - loss: 1.7507 - regression_loss: 1.2799 - classification_loss: 0.4708
2404/3000 [=======================>......] - ETA: 5:01 - loss: 1.7504 - regression_loss: 1.2796 - classification_loss: 0.4707
2405/3000 [=======================>......] - ETA: 5:01 - loss: 1.7500 - regression_loss: 1.2794 - classification_loss: 0.4706
2406/3000 [=======================>......] - ETA: 5:00 - loss: 1.7496 - regression_loss: 1.2790 - classification_loss: 0.4706
2407/3000 [=======================>......] - ETA: 5:00 - loss: 1.7492 - regression_loss: 1.2787 - classification_loss: 0.4705
2408/3000 [=======================>......] - ETA: 4:59 - loss: 1.7489 - regression_loss: 1.2784 - classification_loss: 0.4705
2409/3000 [=======================>......] - ETA: 4:59 - loss: 1.7484 - regression_loss: 1.2781 - classification_loss: 0.4704
2410/3000 [=======================>......] - ETA: 4:58 - loss: 1.7481 - regression_loss: 1.2778 - classification_loss: 0.4704
2411/3000 [=======================>......] - ETA: 4:58 - loss: 1.7477 - regression_loss: 1.2774 - classification_loss: 0.4703
2412/3000 [=======================>......] - ETA: 4:57 - loss: 1.7475 - regression_loss: 1.2772 - classification_loss: 0.4703
2413/3000 [=======================>......] - ETA: 4:57 - loss: 1.7472 - regression_loss: 1.2769 - classification_loss: 0.4702
2414/3000 [=======================>......] - ETA: 4:56 - loss: 1.7468 - regression_loss: 1.2766 - classification_loss: 0.4702
2415/3000 [=======================>......] - ETA: 4:56 - loss: 1.7463 - regression_loss: 1.2762 - classification_loss: 0.4701
2416/3000 [=======================>......] - ETA: 4:55 - loss: 1.7461 - regression_loss: 1.2760 - classification_loss: 0.4701
2417/3000 [=======================>......] - ETA: 4:55 - loss: 1.7457 - regression_loss: 1.2757 - classification_loss: 0.4700
2418/3000 [=======================>......] - ETA: 4:54 - loss: 1.7453 - regression_loss: 1.2754 - classification_loss: 0.4699
2419/3000 [=======================>......] - ETA: 4:53 - loss: 1.7449 - regression_loss: 1.2750 - classification_loss: 0.4699
2420/3000 [=======================>......] - ETA: 4:53 - loss: 1.7445 - regression_loss: 1.2746 - classification_loss: 0.4698
2421/3000 [=======================>......] - ETA: 4:52 - loss: 1.7441 - regression_loss: 1.2743 - classification_loss: 0.4698
2422/3000 [=======================>......] - ETA: 4:52 - loss: 1.7439 - regression_loss: 1.2741 - classification_loss: 0.4698
2423/3000 [=======================>......] - ETA: 4:51 - loss: 1.7436 - regression_loss: 1.2739 - classification_loss: 0.4697
2424/3000 [=======================>......] - ETA: 4:51 - loss: 1.7432 - regression_loss: 1.2735 - classification_loss: 0.4697
2425/3000 [=======================>......] - ETA: 4:50 - loss: 1.7429 - regression_loss: 1.2733 - classification_loss: 0.4696
2426/3000 [=======================>......] - ETA: 4:50 - loss: 1.7427 - regression_loss: 1.2731 - classification_loss: 0.4696
2427/3000 [=======================>......] - ETA: 4:49 - loss: 1.7425 - regression_loss: 1.2729 - classification_loss: 0.4695
2428/3000 [=======================>......] - ETA: 4:49 - loss: 1.7422 - regression_loss: 1.2727 - classification_loss: 0.4695
2429/3000 [=======================>......] - ETA: 4:48 - loss: 1.7418 - regression_loss: 1.2724 - classification_loss: 0.4694
2430/3000 [=======================>......] - ETA: 4:48 - loss: 1.7416 - regression_loss: 1.2722 - classification_loss: 0.4694
2431/3000 [=======================>......] - ETA: 4:47 - loss: 1.7412 - regression_loss: 1.2719 - classification_loss: 0.4693
2432/3000 [=======================>......] - ETA: 4:47 - loss: 1.7410 - regression_loss: 1.2717 - classification_loss: 0.4693
2433/3000 [=======================>......] - ETA: 4:46 - loss: 1.7406 - regression_loss: 1.2714 - classification_loss: 0.4692
2434/3000 [=======================>......] - ETA: 4:46 - loss: 1.7403 - regression_loss: 1.2712 - classification_loss: 0.4691
2435/3000 [=======================>......] - ETA: 4:45 - loss: 1.7404 - regression_loss: 1.2712 - classification_loss: 0.4692
2436/3000 [=======================>......] - ETA: 4:45 - loss: 1.7401 - regression_loss: 1.2709 - classification_loss: 0.4692
2437/3000 [=======================>......] - ETA: 4:44 - loss: 1.7398 - regression_loss: 1.2706 - classification_loss: 0.4692
2438/3000 [=======================>......] - ETA: 4:43 - loss: 1.7394 - regression_loss: 1.2702 - classification_loss: 0.4692
2439/3000 [=======================>......] - ETA: 4:43 - loss: 1.7391 - regression_loss: 1.2700 - classification_loss: 0.4691
2440/3000 [=======================>......] - ETA: 4:42 - loss: 1.7389 - regression_loss: 1.2698 - classification_loss: 0.4691
2441/3000 [=======================>......] - ETA: 4:42 - loss: 1.7386 - regression_loss: 1.2695 - classification_loss: 0.4690
2442/3000 [=======================>......] - ETA: 4:41 - loss: 1.7383 - regression_loss: 1.2693 - classification_loss: 0.4690
2443/3000 [=======================>......] - ETA: 4:41 - loss: 1.7380 - regression_loss: 1.2691 - classification_loss: 0.4689
2444/3000 [=======================>......] - ETA: 4:40 - loss: 1.7379 - regression_loss: 1.2690 - classification_loss: 0.4689
2445/3000 [=======================>......] - ETA: 4:40 - loss: 1.7375 - regression_loss: 1.2686 - classification_loss: 0.4689
2446/3000 [=======================>......] - ETA: 4:39 - loss: 1.7372 - regression_loss: 1.2683 - classification_loss: 0.4689
2447/3000 [=======================>......] - ETA: 4:39 - loss: 1.7370 - regression_loss: 1.2682 - classification_loss: 0.4688
2448/3000 [=======================>......] - ETA: 4:38 - loss: 1.7367 - regression_loss: 1.2679 - classification_loss: 0.4688
2449/3000 [=======================>......] - ETA: 4:38 - loss: 1.7365 - regression_loss: 1.2678 - classification_loss: 0.4688
2450/3000 [=======================>......] - ETA: 4:37 - loss: 1.7362 - regression_loss: 1.2675 - classification_loss: 0.4687
2451/3000 [=======================>......] - ETA: 4:37 - loss: 1.7359 - regression_loss: 1.2673 - classification_loss: 0.4687
2452/3000 [=======================>......] - ETA: 4:36 - loss: 1.7359 - regression_loss: 1.2673 - classification_loss: 0.4686
2453/3000 [=======================>......] - ETA: 4:36 - loss: 1.7359 - regression_loss: 1.2673 - classification_loss: 0.4686
2454/3000 [=======================>......] - ETA: 4:35 - loss: 1.7356 - regression_loss: 1.2670 - classification_loss: 0.4686
2455/3000 [=======================>......] - ETA: 4:35 - loss: 1.7353 - regression_loss: 1.2667 - classification_loss: 0.4685
2456/3000 [=======================>......] - ETA: 4:34 - loss: 1.7349 - regression_loss: 1.2665 - classification_loss: 0.4685
2457/3000 [=======================>......] - ETA: 4:33 - loss: 1.7348 - regression_loss: 1.2664 - classification_loss: 0.4684
2458/3000 [=======================>......] - ETA: 4:33 - loss: 1.7345 - regression_loss: 1.2661 - classification_loss: 0.4684
2459/3000 [=======================>......] - ETA: 4:32 - loss: 1.7342 - regression_loss: 1.2658 - classification_loss: 0.4683
2460/3000 [=======================>......] - ETA: 4:32 - loss: 1.7340 - regression_loss: 1.2656 - classification_loss: 0.4683
2461/3000 [=======================>......] - ETA: 4:31 - loss: 1.7337 - regression_loss: 1.2654 - classification_loss: 0.4683
2462/3000 [=======================>......] - ETA: 4:31 - loss: 1.7334 - regression_loss: 1.2652 - classification_loss: 0.4682
2463/3000 [=======================>......] - ETA: 4:30 - loss: 1.7330 - regression_loss: 1.2649 - classification_loss: 0.4681
2464/3000 [=======================>......] - ETA: 4:30 - loss: 1.7327 - regression_loss: 1.2646 - classification_loss: 0.4680
2465/3000 [=======================>......] - ETA: 4:29 - loss: 1.7323 - regression_loss: 1.2643 - classification_loss: 0.4680
2466/3000 [=======================>......] - ETA: 4:29 - loss: 1.7320 - regression_loss: 1.2640 - classification_loss: 0.4679
2467/3000 [=======================>......] - ETA: 4:28 - loss: 1.7317 - regression_loss: 1.2638 - classification_loss: 0.4679
2468/3000 [=======================>......] - ETA: 4:28 - loss: 1.7313 - regression_loss: 1.2634 - classification_loss: 0.4679
2469/3000 [=======================>......] - ETA: 4:27 - loss: 1.7309 - regression_loss: 1.2631 - classification_loss: 0.4678
2470/3000 [=======================>......] - ETA: 4:27 - loss: 1.7306 - regression_loss: 1.2628 - classification_loss: 0.4678
2471/3000 [=======================>......] - ETA: 4:26 - loss: 1.7303 - regression_loss: 1.2626 - classification_loss: 0.4677
2472/3000 [=======================>......] - ETA: 4:26 - loss: 1.7299 - regression_loss: 1.2622 - classification_loss: 0.4677
2473/3000 [=======================>......] - ETA: 4:25 - loss: 1.7296 - regression_loss: 1.2620 - classification_loss: 0.4676
2474/3000 [=======================>......] - ETA: 4:25 - loss: 1.7292 - regression_loss: 1.2616 - classification_loss: 0.4675
2475/3000 [=======================>......] - ETA: 4:24 - loss: 1.7290 - regression_loss: 1.2615 - classification_loss: 0.4675
2476/3000 [=======================>......] - ETA: 4:24 - loss: 1.7287 - regression_loss: 1.2612 - classification_loss: 0.4675
2477/3000 [=======================>......] - ETA: 4:23 - loss: 1.7283 - regression_loss: 1.2609 - classification_loss: 0.4674
2478/3000 [=======================>......] - ETA: 4:22 - loss: 1.7280 - regression_loss: 1.2606 - classification_loss: 0.4674
2479/3000 [=======================>......] - ETA: 4:22 - loss: 1.7276 - regression_loss: 1.2603 - classification_loss: 0.4673
2480/3000 [=======================>......] - ETA: 4:21 - loss: 1.7274 - regression_loss: 1.2601 - classification_loss: 0.4673
2481/3000 [=======================>......] - ETA: 4:21 - loss: 1.7270 - regression_loss: 1.2598 - classification_loss: 0.4673
2482/3000 [=======================>......] - ETA: 4:20 - loss: 1.7267 - regression_loss: 1.2594 - classification_loss: 0.4673
2483/3000 [=======================>......] - ETA: 4:20 - loss: 1.7265 - regression_loss: 1.2592 - classification_loss: 0.4673
2484/3000 [=======================>......] - ETA: 4:19 - loss: 1.7261 - regression_loss: 1.2589 - classification_loss: 0.4672
2485/3000 [=======================>......] - ETA: 4:19 - loss: 1.7257 - regression_loss: 1.2585 - classification_loss: 0.4671
2486/3000 [=======================>......] - ETA: 4:18 - loss: 1.7253 - regression_loss: 1.2582 - classification_loss: 0.4671
2487/3000 [=======================>......] - ETA: 4:18 - loss: 1.7250 - regression_loss: 1.2579 - classification_loss: 0.4671
2488/3000 [=======================>......] - ETA: 4:17 - loss: 1.7246 - regression_loss: 1.2576 - classification_loss: 0.4670
2489/3000 [=======================>......] - ETA: 4:17 - loss: 1.7241 - regression_loss: 1.2572 - classification_loss: 0.4670
2490/3000 [=======================>......] - ETA: 4:16 - loss: 1.7238 - regression_loss: 1.2569 - classification_loss: 0.4669
2491/3000 [=======================>......] - ETA: 4:16 - loss: 1.7237 - regression_loss: 1.2568 - classification_loss: 0.4669
2492/3000 [=======================>......] - ETA: 4:15 - loss: 1.7233 - regression_loss: 1.2565 - classification_loss: 0.4668
2493/3000 [=======================>......] - ETA: 4:15 - loss: 1.7228 - regression_loss: 1.2561 - classification_loss: 0.4667
2494/3000 [=======================>......] - ETA: 4:14 - loss: 1.7225 - regression_loss: 1.2558 - classification_loss: 0.4667
2495/3000 [=======================>......] - ETA: 4:14 - loss: 1.7223 - regression_loss: 1.2555 - classification_loss: 0.4667
2496/3000 [=======================>......] - ETA: 4:13 - loss: 1.7220 - regression_loss: 1.2553 - classification_loss: 0.4667
2497/3000 [=======================>......] - ETA: 4:13 - loss: 1.7216 - regression_loss: 1.2550 - classification_loss: 0.4666
2498/3000 [=======================>......] - ETA: 4:12 - loss: 1.7213 - regression_loss: 1.2548 - classification_loss: 0.4665
2499/3000 [=======================>......] - ETA: 4:12 - loss: 1.7209 - regression_loss: 1.2544 - classification_loss: 0.4665
2500/3000 [========================>.....] - ETA: 4:11 - loss: 1.7205 - regression_loss: 1.2541 - classification_loss: 0.4665
2501/3000 [========================>.....] - ETA: 4:10 - loss: 1.7203 - regression_loss: 1.2538 - classification_loss: 0.4664
2502/3000 [========================>.....] - ETA: 4:10 - loss: 1.7199 - regression_loss: 1.2535 - classification_loss: 0.4664
2503/3000 [========================>.....] - ETA: 4:09 - loss: 1.7196 - regression_loss: 1.2532 - classification_loss: 0.4664
2504/3000 [========================>.....] - ETA: 4:09 - loss: 1.7194 - regression_loss: 1.2531 - classification_loss: 0.4663
2505/3000 [========================>.....] - ETA: 4:08 - loss: 1.7190 - regression_loss: 1.2528 - classification_loss: 0.4663
2506/3000 [========================>.....] - ETA: 4:08 - loss: 1.7187 - regression_loss: 1.2524 - classification_loss: 0.4662
2507/3000 [========================>.....] - ETA: 4:07 - loss: 1.7183 - regression_loss: 1.2522 - classification_loss: 0.4661
2508/3000 [========================>.....] - ETA: 4:07 - loss: 1.7181 - regression_loss: 1.2520 - classification_loss: 0.4661
2509/3000 [========================>.....] - ETA: 4:06 - loss: 1.7179 - regression_loss: 1.2519 - classification_loss: 0.4661
2510/3000 [========================>.....] - ETA: 4:06 - loss: 1.7176 - regression_loss: 1.2515 - classification_loss: 0.4660
2511/3000 [========================>.....] - ETA: 4:05 - loss: 1.7172 - regression_loss: 1.2512 - classification_loss: 0.4660
2512/3000 [========================>.....] - ETA: 4:05 - loss: 1.7169 - regression_loss: 1.2509 - classification_loss: 0.4660
2513/3000 [========================>.....] - ETA: 4:04 - loss: 1.7165 - regression_loss: 1.2506 - classification_loss: 0.4659
2514/3000 [========================>.....] - ETA: 4:04 - loss: 1.7164 - regression_loss: 1.2505 - classification_loss: 0.4659
2515/3000 [========================>.....] - ETA: 4:03 - loss: 1.7161 - regression_loss: 1.2503 - classification_loss: 0.4658
2516/3000 [========================>.....] - ETA: 4:03 - loss: 1.7158 - regression_loss: 1.2500 - classification_loss: 0.4658
2517/3000 [========================>.....] - ETA: 4:02 - loss: 1.7155 - regression_loss: 1.2498 - classification_loss: 0.4657
2518/3000 [========================>.....] - ETA: 4:02 - loss: 1.7151 - regression_loss: 1.2495 - classification_loss: 0.4656
2519/3000 [========================>.....] - ETA: 4:01 - loss: 1.7147 - regression_loss: 1.2492 - classification_loss: 0.4656
2520/3000 [========================>.....] - ETA: 4:01 - loss: 1.7145 - regression_loss: 1.2489 - classification_loss: 0.4656
2521/3000 [========================>.....] - ETA: 4:00 - loss: 1.7142 - regression_loss: 1.2487 - classification_loss: 0.4655
2522/3000 [========================>.....] - ETA: 4:00 - loss: 1.7139 - regression_loss: 1.2484 - classification_loss: 0.4654
2523/3000 [========================>.....] - ETA: 3:59 - loss: 1.7137 - regression_loss: 1.2482 - classification_loss: 0.4654
2524/3000 [========================>.....] - ETA: 3:58 - loss: 1.7134 - regression_loss: 1.2481 - classification_loss: 0.4654
2525/3000 [========================>.....] - ETA: 3:58 - loss: 1.7133 - regression_loss: 1.2480 - classification_loss: 0.4653
2526/3000 [========================>.....] - ETA: 3:57 - loss: 1.7132 - regression_loss: 1.2479 - classification_loss: 0.4653
2527/3000 [========================>.....] - ETA: 3:57 - loss: 1.7128 - regression_loss: 1.2476 - classification_loss: 0.4652
2528/3000 [========================>.....] - ETA: 3:56 - loss: 1.7125 - regression_loss: 1.2473 - classification_loss: 0.4651
2529/3000 [========================>.....] - ETA: 3:56 - loss: 1.7122 - regression_loss: 1.2471 - classification_loss: 0.4651
2530/3000 [========================>.....] - ETA: 3:55 - loss: 1.7118 - regression_loss: 1.2468 - classification_loss: 0.4651
2531/3000 [========================>.....] - ETA: 3:55 - loss: 1.7115 - regression_loss: 1.2465 - classification_loss: 0.4650
2532/3000 [========================>.....] - ETA: 3:54 - loss: 1.7111 - regression_loss: 1.2462 - classification_loss: 0.4650
2533/3000 [========================>.....] - ETA: 3:54 - loss: 1.7107 - regression_loss: 1.2458 - classification_loss: 0.4649
2534/3000 [========================>.....] - ETA: 3:53 - loss: 1.7105 - regression_loss: 1.2456 - classification_loss: 0.4649
2535/3000 [========================>.....] - ETA: 3:53 - loss: 1.7106 - regression_loss: 1.2457 - classification_loss: 0.4648
2536/3000 [========================>.....] - ETA: 3:52 - loss: 1.7102 - regression_loss: 1.2455 - classification_loss: 0.4647
2537/3000 [========================>.....] - ETA: 3:52 - loss: 1.7100 - regression_loss: 1.2453 - classification_loss: 0.4647
2538/3000 [========================>.....] - ETA: 3:51 - loss: 1.7098 - regression_loss: 1.2452 - classification_loss: 0.4647
2539/3000 [========================>.....] - ETA: 3:51 - loss: 1.7096 - regression_loss: 1.2450 - classification_loss: 0.4646
2540/3000 [========================>.....] - ETA: 3:50 - loss: 1.7092 - regression_loss: 1.2447 - classification_loss: 0.4645
2541/3000 [========================>.....] - ETA: 3:50 - loss: 1.7090 - regression_loss: 1.2445 - classification_loss: 0.4645
2542/3000 [========================>.....] - ETA: 3:49 - loss: 1.7088 - regression_loss: 1.2443 - classification_loss: 0.4645
2543/3000 [========================>.....] - ETA: 3:49 - loss: 1.7085 - regression_loss: 1.2441 - classification_loss: 0.4644
2544/3000 [========================>.....] - ETA: 3:48 - loss: 1.7082 - regression_loss: 1.2438 - classification_loss: 0.4644
2545/3000 [========================>.....] - ETA: 3:48 - loss: 1.7078 - regression_loss: 1.2435 - classification_loss: 0.4643
2546/3000 [========================>.....] - ETA: 3:47 - loss: 1.7074 - regression_loss: 1.2431 - classification_loss: 0.4643
2547/3000 [========================>.....] - ETA: 3:47 - loss: 1.7072 - regression_loss: 1.2430 - classification_loss: 0.4642
2548/3000 [========================>.....] - ETA: 3:46 - loss: 1.7069 - regression_loss: 1.2428 - classification_loss: 0.4641
2549/3000 [========================>.....] - ETA: 3:46 - loss: 1.7066 - regression_loss: 1.2425 - classification_loss: 0.4640
2550/3000 [========================>.....] - ETA: 3:45 - loss: 1.7062 - regression_loss: 1.2422 - classification_loss: 0.4640
2551/3000 [========================>.....] - ETA: 3:45 - loss: 1.7059 - regression_loss: 1.2419 - classification_loss: 0.4639
2552/3000 [========================>.....] - ETA: 3:44 - loss: 1.7056 - regression_loss: 1.2417 - classification_loss: 0.4639
2553/3000 [========================>.....] - ETA: 3:43 - loss: 1.7052 - regression_loss: 1.2414 - classification_loss: 0.4638
2554/3000 [========================>.....] - ETA: 3:43 - loss: 1.7051 - regression_loss: 1.2413 - classification_loss: 0.4638
2555/3000 [========================>.....] - ETA: 3:42 - loss: 1.7048 - regression_loss: 1.2411 - classification_loss: 0.4637
2556/3000 [========================>.....] - ETA: 3:42 - loss: 1.7044 - regression_loss: 1.2408 - classification_loss: 0.4636
2557/3000 [========================>.....] - ETA: 3:41 - loss: 1.7042 - regression_loss: 1.2406 - classification_loss: 0.4636
2558/3000 [========================>.....] - ETA: 3:41 - loss: 1.7039 - regression_loss: 1.2403 - classification_loss: 0.4636
2559/3000 [========================>.....] - ETA: 3:40 - loss: 1.7035 - regression_loss: 1.2400 - classification_loss: 0.4635
2560/3000 [========================>.....] - ETA: 3:40 - loss: 1.7033 - regression_loss: 1.2398 - classification_loss: 0.4635
2561/3000 [========================>.....] - ETA: 3:39 - loss: 1.7031 - regression_loss: 1.2396 - classification_loss: 0.4634
2562/3000 [========================>.....] - ETA: 3:39 - loss: 1.7027 - regression_loss: 1.2394 - classification_loss: 0.4634
2563/3000 [========================>.....] - ETA: 3:38 - loss: 1.7024 - regression_loss: 1.2391 - classification_loss: 0.4633
2564/3000 [========================>.....] - ETA: 3:38 - loss: 1.7021 - regression_loss: 1.2388 - classification_loss: 0.4633
2565/3000 [========================>.....] - ETA: 3:37 - loss: 1.7020 - regression_loss: 1.2388 - classification_loss: 0.4632
2566/3000 [========================>.....] - ETA: 3:37 - loss: 1.7018 - regression_loss: 1.2386 - classification_loss: 0.4632
2567/3000 [========================>.....] - ETA: 3:36 - loss: 1.7015 - regression_loss: 1.2384 - classification_loss: 0.4632
2568/3000 [========================>.....] - ETA: 3:36 - loss: 1.7012 - regression_loss: 1.2381 - classification_loss: 0.4632
2569/3000 [========================>.....] - ETA: 3:35 - loss: 1.7008 - regression_loss: 1.2377 - classification_loss: 0.4631
2570/3000 [========================>.....] - ETA: 3:35 - loss: 1.7007 - regression_loss: 1.2377 - classification_loss: 0.4631
2571/3000 [========================>.....] - ETA: 3:34 - loss: 1.7005 - regression_loss: 1.2374 - classification_loss: 0.4631
2572/3000 [========================>.....] - ETA: 3:34 - loss: 1.7002 - regression_loss: 1.2372 - classification_loss: 0.4630
2573/3000 [========================>.....] - ETA: 3:33 - loss: 1.7001 - regression_loss: 1.2370 - classification_loss: 0.4630
2574/3000 [========================>.....] - ETA: 3:33 - loss: 1.6998 - regression_loss: 1.2368 - classification_loss: 0.4630
2575/3000 [========================>.....] - ETA: 3:32 - loss: 1.6995 - regression_loss: 1.2365 - classification_loss: 0.4630
2576/3000 [========================>.....] - ETA: 3:32 - loss: 1.6992 - regression_loss: 1.2362 - classification_loss: 0.4630
2577/3000 [========================>.....] - ETA: 3:31 - loss: 1.6989 - regression_loss: 1.2360 - classification_loss: 0.4629
2578/3000 [========================>.....] - ETA: 3:31 - loss: 1.6984 - regression_loss: 1.2356 - classification_loss: 0.4628
2579/3000 [========================>.....] - ETA: 3:30 - loss: 1.6982 - regression_loss: 1.2354 - classification_loss: 0.4628
2580/3000 [========================>.....] - ETA: 3:30 - loss: 1.6978 - regression_loss: 1.2351 - classification_loss: 0.4627
2581/3000 [========================>.....] - ETA: 3:29 - loss: 1.6977 - regression_loss: 1.2350 - classification_loss: 0.4627
2582/3000 [========================>.....] - ETA: 3:29 - loss: 1.6975 - regression_loss: 1.2349 - classification_loss: 0.4626
2583/3000 [========================>.....] - ETA: 3:28 - loss: 1.6972 - regression_loss: 1.2346 - classification_loss: 0.4626
2584/3000 [========================>.....] - ETA: 3:27 - loss: 1.6969 - regression_loss: 1.2343 - classification_loss: 0.4626
2585/3000 [========================>.....] - ETA: 3:27 - loss: 1.6965 - regression_loss: 1.2340 - classification_loss: 0.4625
2586/3000 [========================>.....] - ETA: 3:26 - loss: 1.6962 - regression_loss: 1.2337 - classification_loss: 0.4625
2587/3000 [========================>.....] - ETA: 3:26 - loss: 1.6958 - regression_loss: 1.2334 - classification_loss: 0.4624
2588/3000 [========================>.....] - ETA: 3:25 - loss: 1.6955 - regression_loss: 1.2332 - classification_loss: 0.4623
2589/3000 [========================>.....] - ETA: 3:25 - loss: 1.6952 - regression_loss: 1.2329 - classification_loss: 0.4623
2590/3000 [========================>.....] - ETA: 3:24 - loss: 1.6949 - regression_loss: 1.2326 - classification_loss: 0.4623
2591/3000 [========================>.....] - ETA: 3:24 - loss: 1.6946 - regression_loss: 1.2324 - classification_loss: 0.4622
2592/3000 [========================>.....] - ETA: 3:23 - loss: 1.6943 - regression_loss: 1.2321 - classification_loss: 0.4622
2593/3000 [========================>.....] - ETA: 3:23 - loss: 1.6938 - regression_loss: 1.2317 - classification_loss: 0.4621
2594/3000 [========================>.....] - ETA: 3:22 - loss: 1.6937 - regression_loss: 1.2317 - classification_loss: 0.4621
2595/3000 [========================>.....] - ETA: 3:22 - loss: 1.6935 - regression_loss: 1.2314 - classification_loss: 0.4621
2596/3000 [========================>.....] - ETA: 3:21 - loss: 1.6932 - regression_loss: 1.2312 - classification_loss: 0.4620
2597/3000 [========================>.....] - ETA: 3:21 - loss: 1.6930 - regression_loss: 1.2310 - classification_loss: 0.4620
2598/3000 [========================>.....] - ETA: 3:20 - loss: 1.6929 - regression_loss: 1.2309 - classification_loss: 0.4620
2599/3000 [========================>.....] - ETA: 3:20 - loss: 1.6927 - regression_loss: 1.2307 - classification_loss: 0.4620
2600/3000 [=========================>....] - ETA: 3:19 - loss: 1.6924 - regression_loss: 1.2305 - classification_loss: 0.4620
2601/3000 [=========================>....] - ETA: 3:19 - loss: 1.6922 - regression_loss: 1.2303 - classification_loss: 0.4619
2602/3000 [=========================>....] - ETA: 3:18 - loss: 1.6920 - regression_loss: 1.2301 - classification_loss: 0.4619
2603/3000 [=========================>....] - ETA: 3:18 - loss: 1.6917 - regression_loss: 1.2298 - classification_loss: 0.4619
2604/3000 [=========================>....] - ETA: 3:17 - loss: 1.6915 - regression_loss: 1.2297 - classification_loss: 0.4619
2605/3000 [=========================>....] - ETA: 3:17 - loss: 1.6911 - regression_loss: 1.2293 - classification_loss: 0.4618
2606/3000 [=========================>....] - ETA: 3:16 - loss: 1.6907 - regression_loss: 1.2290 - classification_loss: 0.4618
2607/3000 [=========================>....] - ETA: 3:16 - loss: 1.6905 - regression_loss: 1.2287 - classification_loss: 0.4617
2608/3000 [=========================>....] - ETA: 3:15 - loss: 1.6901 - regression_loss: 1.2284 - classification_loss: 0.4617
2609/3000 [=========================>....] - ETA: 3:15 - loss: 1.6897 - regression_loss: 1.2281 - classification_loss: 0.4616
2610/3000 [=========================>....] - ETA: 3:14 - loss: 1.6893 - regression_loss: 1.2277 - classification_loss: 0.4616
2611/3000 [=========================>....] - ETA: 3:14 - loss: 1.6890 - regression_loss: 1.2274 - classification_loss: 0.4616
2612/3000 [=========================>....] - ETA: 3:13 - loss: 1.6887 - regression_loss: 1.2272 - classification_loss: 0.4615
2613/3000 [=========================>....] - ETA: 3:13 - loss: 1.6884 - regression_loss: 1.2269 - classification_loss: 0.4615
2614/3000 [=========================>....] - ETA: 3:12 - loss: 1.6883 - regression_loss: 1.2268 - classification_loss: 0.4615
2615/3000 [=========================>....] - ETA: 3:12 - loss: 1.6880 - regression_loss: 1.2266 - classification_loss: 0.4615
2616/3000 [=========================>....] - ETA: 3:11 - loss: 1.6877 - regression_loss: 1.2263 - classification_loss: 0.4614
2617/3000 [=========================>....] - ETA: 3:11 - loss: 1.6874 - regression_loss: 1.2261 - classification_loss: 0.4614
2618/3000 [=========================>....] - ETA: 3:10 - loss: 1.6871 - regression_loss: 1.2257 - classification_loss: 0.4613
2619/3000 [=========================>....] - ETA: 3:10 - loss: 1.6867 - regression_loss: 1.2254 - classification_loss: 0.4613
2620/3000 [=========================>....] - ETA: 3:09 - loss: 1.6864 - regression_loss: 1.2251 - classification_loss: 0.4612
2621/3000 [=========================>....] - ETA: 3:09 - loss: 1.6862 - regression_loss: 1.2250 - classification_loss: 0.4612
2622/3000 [=========================>....] - ETA: 3:08 - loss: 1.6859 - regression_loss: 1.2247 - classification_loss: 0.4611
2623/3000 [=========================>....] - ETA: 3:07 - loss: 1.6855 - regression_loss: 1.2244 - classification_loss: 0.4611
2624/3000 [=========================>....] - ETA: 3:07 - loss: 1.6851 - regression_loss: 1.2241 - classification_loss: 0.4610
2625/3000 [=========================>....] - ETA: 3:06 - loss: 1.6848 - regression_loss: 1.2238 - classification_loss: 0.4610
2626/3000 [=========================>....] - ETA: 3:06 - loss: 1.6846 - regression_loss: 1.2236 - classification_loss: 0.4610
2627/3000 [=========================>....] - ETA: 3:05 - loss: 1.6844 - regression_loss: 1.2234 - classification_loss: 0.4609
2628/3000 [=========================>....] - ETA: 3:05 - loss: 1.6840 - regression_loss: 1.2231 - classification_loss: 0.4609
2629/3000 [=========================>....] - ETA: 3:04 - loss: 1.6837 - regression_loss: 1.2228 - classification_loss: 0.4609
2630/3000 [=========================>....] - ETA: 3:04 - loss: 1.6834 - regression_loss: 1.2225 - classification_loss: 0.4609
2631/3000 [=========================>....] - ETA: 3:03 - loss: 1.6831 - regression_loss: 1.2223 - classification_loss: 0.4608
2632/3000 [=========================>....] - ETA: 3:03 - loss: 1.6828 - regression_loss: 1.2220 - classification_loss: 0.4608
2633/3000 [=========================>....] - ETA: 3:02 - loss: 1.6825 - regression_loss: 1.2218 - classification_loss: 0.4607
2634/3000 [=========================>....] - ETA: 3:02 - loss: 1.6823 - regression_loss: 1.2216 - classification_loss: 0.4607
2635/3000 [=========================>....] - ETA: 3:01 - loss: 1.6821 - regression_loss: 1.2214 - classification_loss: 0.4606
2636/3000 [=========================>....] - ETA: 3:01 - loss: 1.6818 - regression_loss: 1.2213 - classification_loss: 0.4606
2637/3000 [=========================>....] - ETA: 3:00 - loss: 1.6814 - regression_loss: 1.2210 - classification_loss: 0.4605
2638/3000 [=========================>....] - ETA: 3:00 - loss: 1.6811 - regression_loss: 1.2207 - classification_loss: 0.4605
2639/3000 [=========================>....] - ETA: 2:59 - loss: 1.6807 - regression_loss: 1.2203 - classification_loss: 0.4604
2640/3000 [=========================>....] - ETA: 2:59 - loss: 1.6804 - regression_loss: 1.2200 - classification_loss: 0.4604
2641/3000 [=========================>....] - ETA: 2:58 - loss: 1.6801 - regression_loss: 1.2198 - classification_loss: 0.4603
2642/3000 [=========================>....] - ETA: 2:58 - loss: 1.6798 - regression_loss: 1.2196 - classification_loss: 0.4603
2643/3000 [=========================>....] - ETA: 2:57 - loss: 1.6795 - regression_loss: 1.2193 - classification_loss: 0.4602
2644/3000 [=========================>....] - ETA: 2:57 - loss: 1.6792 - regression_loss: 1.2190 - classification_loss: 0.4602
2645/3000 [=========================>....] - ETA: 2:56 - loss: 1.6789 - regression_loss: 1.2188 - classification_loss: 0.4601
2646/3000 [=========================>....] - ETA: 2:56 - loss: 1.6787 - regression_loss: 1.2186 - classification_loss: 0.4601
2647/3000 [=========================>....] - ETA: 2:55 - loss: 1.6784 - regression_loss: 1.2183 - classification_loss: 0.4601
2648/3000 [=========================>....] - ETA: 2:55 - loss: 1.6781 - regression_loss: 1.2181 - classification_loss: 0.4600
2649/3000 [=========================>....] - ETA: 2:54 - loss: 1.6777 - regression_loss: 1.2178 - classification_loss: 0.4600
2650/3000 [=========================>....] - ETA: 2:54 - loss: 1.6774 - regression_loss: 1.2175 - classification_loss: 0.4599
2651/3000 [=========================>....] - ETA: 2:53 - loss: 1.6773 - regression_loss: 1.2173 - classification_loss: 0.4599
2652/3000 [=========================>....] - ETA: 2:53 - loss: 1.6770 - regression_loss: 1.2171 - classification_loss: 0.4599
2653/3000 [=========================>....] - ETA: 2:52 - loss: 1.6767 - regression_loss: 1.2168 - classification_loss: 0.4598
2654/3000 [=========================>....] - ETA: 2:52 - loss: 1.6763 - regression_loss: 1.2165 - classification_loss: 0.4598
2655/3000 [=========================>....] - ETA: 2:51 - loss: 1.6760 - regression_loss: 1.2163 - classification_loss: 0.4597
2656/3000 [=========================>....] - ETA: 2:51 - loss: 1.6758 - regression_loss: 1.2161 - classification_loss: 0.4597
2657/3000 [=========================>....] - ETA: 2:50 - loss: 1.6754 - regression_loss: 1.2158 - classification_loss: 0.4596
2658/3000 [=========================>....] - ETA: 2:50 - loss: 1.6751 - regression_loss: 1.2155 - classification_loss: 0.4596
2659/3000 [=========================>....] - ETA: 2:49 - loss: 1.6747 - regression_loss: 1.2152 - classification_loss: 0.4595
2660/3000 [=========================>....] - ETA: 2:49 - loss: 1.6745 - regression_loss: 1.2150 - classification_loss: 0.4595
2661/3000 [=========================>....] - ETA: 2:48 - loss: 1.6742 - regression_loss: 1.2148 - classification_loss: 0.4594
2662/3000 [=========================>....] - ETA: 2:48 - loss: 1.6740 - regression_loss: 1.2147 - classification_loss: 0.4593
2663/3000 [=========================>....] - ETA: 2:47 - loss: 1.6736 - regression_loss: 1.2143 - classification_loss: 0.4593
2664/3000 [=========================>....] - ETA: 2:47 - loss: 1.6733 - regression_loss: 1.2140 - classification_loss: 0.4593
2665/3000 [=========================>....] - ETA: 2:46 - loss: 1.6732 - regression_loss: 1.2140 - classification_loss: 0.4593
2666/3000 [=========================>....] - ETA: 2:46 - loss: 1.6729 - regression_loss: 1.2138 - classification_loss: 0.4592
2667/3000 [=========================>....] - ETA: 2:45 - loss: 1.6726 - regression_loss: 1.2135 - classification_loss: 0.4591
2668/3000 [=========================>....] - ETA: 2:45 - loss: 1.6723 - regression_loss: 1.2132 - classification_loss: 0.4591
2669/3000 [=========================>....] - ETA: 2:44 - loss: 1.6721 - regression_loss: 1.2131 - classification_loss: 0.4590
2670/3000 [=========================>....] - ETA: 2:44 - loss: 1.6720 - regression_loss: 1.2130 - classification_loss: 0.4590
2671/3000 [=========================>....] - ETA: 2:43 - loss: 1.6718 - regression_loss: 1.2129 - classification_loss: 0.4589
2672/3000 [=========================>....] - ETA: 2:43 - loss: 1.6716 - regression_loss: 1.2127 - classification_loss: 0.4589
2673/3000 [=========================>....] - ETA: 2:42 - loss: 1.6713 - regression_loss: 1.2124 - classification_loss: 0.4589
2674/3000 [=========================>....] - ETA: 2:42 - loss: 1.6711 - regression_loss: 1.2122 - classification_loss: 0.4589
2675/3000 [=========================>....] - ETA: 2:41 - loss: 1.6710 - regression_loss: 1.2122 - classification_loss: 0.4588
2676/3000 [=========================>....] - ETA: 2:41 - loss: 1.6707 - regression_loss: 1.2120 - classification_loss: 0.4588
2677/3000 [=========================>....] - ETA: 2:40 - loss: 1.6705 - regression_loss: 1.2118 - classification_loss: 0.4588
2678/3000 [=========================>....] - ETA: 2:40 - loss: 1.6701 - regression_loss: 1.2115 - classification_loss: 0.4587
2679/3000 [=========================>....] - ETA: 2:39 - loss: 1.6698 - regression_loss: 1.2112 - classification_loss: 0.4586
2680/3000 [=========================>....] - ETA: 2:38 - loss: 1.6695 - regression_loss: 1.2110 - classification_loss: 0.4586
2681/3000 [=========================>....] - ETA: 2:38 - loss: 1.6693 - regression_loss: 1.2107 - classification_loss: 0.4586
2682/3000 [=========================>....] - ETA: 2:37 - loss: 1.6690 - regression_loss: 1.2105 - classification_loss: 0.4585
2683/3000 [=========================>....] - ETA: 2:37 - loss: 1.6687 - regression_loss: 1.2103 - classification_loss: 0.4585
2684/3000 [=========================>....] - ETA: 2:36 - loss: 1.6683 - regression_loss: 1.2099 - classification_loss: 0.4584
2685/3000 [=========================>....] - ETA: 2:36 - loss: 1.6681 - regression_loss: 1.2098 - classification_loss: 0.4583
2686/3000 [=========================>....] - ETA: 2:35 - loss: 1.6679 - regression_loss: 1.2096 - classification_loss: 0.4583
2687/3000 [=========================>....] - ETA: 2:35 - loss: 1.6676 - regression_loss: 1.2093 - classification_loss: 0.4583
2688/3000 [=========================>....] - ETA: 2:34 - loss: 1.6673 - regression_loss: 1.2091 - classification_loss: 0.4582
2689/3000 [=========================>....] - ETA: 2:34 - loss: 1.6670 - regression_loss: 1.2089 - classification_loss: 0.4582
2690/3000 [=========================>....] - ETA: 2:33 - loss: 1.6669 - regression_loss: 1.2087 - classification_loss: 0.4581
2691/3000 [=========================>....] - ETA: 2:33 - loss: 1.6665 - regression_loss: 1.2085 - classification_loss: 0.4580
2692/3000 [=========================>....] - ETA: 2:32 - loss: 1.6662 - regression_loss: 1.2082 - classification_loss: 0.4580
2693/3000 [=========================>....] - ETA: 2:32 - loss: 1.6658 - regression_loss: 1.2079 - classification_loss: 0.4579
2694/3000 [=========================>....] - ETA: 2:31 - loss: 1.6654 - regression_loss: 1.2075 - classification_loss: 0.4579
2695/3000 [=========================>....] - ETA: 2:31 - loss: 1.6653 - regression_loss: 1.2074 - classification_loss: 0.4579
2696/3000 [=========================>....] - ETA: 2:30 - loss: 1.6650 - regression_loss: 1.2071 - classification_loss: 0.4578
2697/3000 [=========================>....] - ETA: 2:30 - loss: 1.6648 - regression_loss: 1.2070 - classification_loss: 0.4578
2698/3000 [=========================>....] - ETA: 2:29 - loss: 1.6645 - regression_loss: 1.2068 - classification_loss: 0.4577
2699/3000 [=========================>....] - ETA: 2:29 - loss: 1.6644 - regression_loss: 1.2067 - classification_loss: 0.4577
2700/3000 [==========================>...] - ETA: 2:28 - loss: 1.6641 - regression_loss: 1.2065 - classification_loss: 0.4576
2701/3000 [==========================>...] - ETA: 2:28 - loss: 1.6639 - regression_loss: 1.2063 - classification_loss: 0.4576
2702/3000 [==========================>...] - ETA: 2:27 - loss: 1.6635 - regression_loss: 1.2060 - classification_loss: 0.4575
2703/3000 [==========================>...] - ETA: 2:27 - loss: 1.6631 - regression_loss: 1.2057 - classification_loss: 0.4575
2704/3000 [==========================>...] - ETA: 2:26 - loss: 1.6630 - regression_loss: 1.2055 - classification_loss: 0.4575
2705/3000 [==========================>...] - ETA: 2:26 - loss: 1.6627 - regression_loss: 1.2053 - classification_loss: 0.4574
2706/3000 [==========================>...] - ETA: 2:25 - loss: 1.6623 - regression_loss: 1.2050 - classification_loss: 0.4574
2707/3000 [==========================>...] - ETA: 2:25 - loss: 1.6620 - regression_loss: 1.2047 - classification_loss: 0.4573
2708/3000 [==========================>...] - ETA: 2:24 - loss: 1.6618 - regression_loss: 1.2045 - classification_loss: 0.4573
2709/3000 [==========================>...] - ETA: 2:24 - loss: 1.6614 - regression_loss: 1.2042 - classification_loss: 0.4572
2710/3000 [==========================>...] - ETA: 2:23 - loss: 1.6612 - regression_loss: 1.2040 - classification_loss: 0.4572
2711/3000 [==========================>...] - ETA: 2:23 - loss: 1.6610 - regression_loss: 1.2038 - classification_loss: 0.4571
2712/3000 [==========================>...] - ETA: 2:22 - loss: 1.6607 - regression_loss: 1.2036 - classification_loss: 0.4571
2713/3000 [==========================>...] - ETA: 2:22 - loss: 1.6604 - regression_loss: 1.2033 - classification_loss: 0.4571
2714/3000 [==========================>...] - ETA: 2:21 - loss: 1.6601 - regression_loss: 1.2030 - classification_loss: 0.4571
2715/3000 [==========================>...] - ETA: 2:21 - loss: 1.6598 - regression_loss: 1.2027 - classification_loss: 0.4571
2716/3000 [==========================>...] - ETA: 2:20 - loss: 1.6595 - regression_loss: 1.2025 - classification_loss: 0.4570
2717/3000 [==========================>...] - ETA: 2:20 - loss: 1.6592 - regression_loss: 1.2022 - classification_loss: 0.4570
2718/3000 [==========================>...] - ETA: 2:19 - loss: 1.6589 - regression_loss: 1.2019 - classification_loss: 0.4570
2719/3000 [==========================>...] - ETA: 2:19 - loss: 1.6586 - regression_loss: 1.2016 - classification_loss: 0.4570
2720/3000 [==========================>...] - ETA: 2:18 - loss: 1.6583 - regression_loss: 1.2013 - classification_loss: 0.4569
2721/3000 [==========================>...] - ETA: 2:18 - loss: 1.6579 - regression_loss: 1.2010 - classification_loss: 0.4569
2722/3000 [==========================>...] - ETA: 2:17 - loss: 1.6577 - regression_loss: 1.2008 - classification_loss: 0.4569
2723/3000 [==========================>...] - ETA: 2:17 - loss: 1.6574 - regression_loss: 1.2006 - classification_loss: 0.4568
2724/3000 [==========================>...] - ETA: 2:16 - loss: 1.6572 - regression_loss: 1.2005 - classification_loss: 0.4568
2725/3000 [==========================>...] - ETA: 2:16 - loss: 1.6569 - regression_loss: 1.2002 - classification_loss: 0.4567
2726/3000 [==========================>...] - ETA: 2:15 - loss: 1.6571 - regression_loss: 1.2003 - classification_loss: 0.4567
2727/3000 [==========================>...] - ETA: 2:15 - loss: 1.6567 - regression_loss: 1.2001 - classification_loss: 0.4567
2728/3000 [==========================>...] - ETA: 2:14 - loss: 1.6564 - regression_loss: 1.1998 - classification_loss: 0.4566
2729/3000 [==========================>...] - ETA: 2:14 - loss: 1.6562 - regression_loss: 1.1997 - classification_loss: 0.4566
2730/3000 [==========================>...] - ETA: 2:13 - loss: 1.6560 - regression_loss: 1.1995 - classification_loss: 0.4566
2731/3000 [==========================>...] - ETA: 2:13 - loss: 1.6557 - regression_loss: 1.1992 - classification_loss: 0.4565
2732/3000 [==========================>...] - ETA: 2:12 - loss: 1.6554 - regression_loss: 1.1989 - classification_loss: 0.4565
2733/3000 [==========================>...] - ETA: 2:12 - loss: 1.6550 - regression_loss: 1.1986 - classification_loss: 0.4564
2734/3000 [==========================>...] - ETA: 2:11 - loss: 1.6548 - regression_loss: 1.1984 - classification_loss: 0.4563
2735/3000 [==========================>...] - ETA: 2:11 - loss: 1.6546 - regression_loss: 1.1983 - classification_loss: 0.4563
2736/3000 [==========================>...] - ETA: 2:10 - loss: 1.6542 - regression_loss: 1.1980 - classification_loss: 0.4562
2737/3000 [==========================>...] - ETA: 2:10 - loss: 1.6538 - regression_loss: 1.1977 - classification_loss: 0.4562
2738/3000 [==========================>...] - ETA: 2:09 - loss: 1.6536 - regression_loss: 1.1974 - classification_loss: 0.4562
2739/3000 [==========================>...] - ETA: 2:09 - loss: 1.6534 - regression_loss: 1.1972 - classification_loss: 0.4561
2740/3000 [==========================>...] - ETA: 2:08 - loss: 1.6531 - regression_loss: 1.1971 - classification_loss: 0.4560
2741/3000 [==========================>...] - ETA: 2:08 - loss: 1.6529 - regression_loss: 1.1969 - classification_loss: 0.4560
2742/3000 [==========================>...] - ETA: 2:07 - loss: 1.6526 - regression_loss: 1.1967 - classification_loss: 0.4559
2743/3000 [==========================>...] - ETA: 2:07 - loss: 1.6523 - regression_loss: 1.1964 - classification_loss: 0.4559
2744/3000 [==========================>...] - ETA: 2:06 - loss: 1.6521 - regression_loss: 1.1963 - classification_loss: 0.4558
2745/3000 [==========================>...] - ETA: 2:06 - loss: 1.6518 - regression_loss: 1.1960 - classification_loss: 0.4558
2746/3000 [==========================>...] - ETA: 2:05 - loss: 1.6516 - regression_loss: 1.1958 - classification_loss: 0.4558
2747/3000 [==========================>...] - ETA: 2:05 - loss: 1.6514 - regression_loss: 1.1957 - classification_loss: 0.4558
2748/3000 [==========================>...] - ETA: 2:04 - loss: 1.6511 - regression_loss: 1.1954 - classification_loss: 0.4557
2749/3000 [==========================>...] - ETA: 2:04 - loss: 1.6508 - regression_loss: 1.1951 - classification_loss: 0.4557
2750/3000 [==========================>...] - ETA: 2:03 - loss: 1.6504 - regression_loss: 1.1948 - classification_loss: 0.4557
2751/3000 [==========================>...] - ETA: 2:03 - loss: 1.6503 - regression_loss: 1.1946 - classification_loss: 0.4556
2752/3000 [==========================>...] - ETA: 2:02 - loss: 1.6499 - regression_loss: 1.1943 - classification_loss: 0.4556
2753/3000 [==========================>...] - ETA: 2:02 - loss: 1.6496 - regression_loss: 1.1941 - classification_loss: 0.4556
2754/3000 [==========================>...] - ETA: 2:01 - loss: 1.6493 - regression_loss: 1.1938 - classification_loss: 0.4555
2755/3000 [==========================>...] - ETA: 2:01 - loss: 1.6489 - regression_loss: 1.1934 - classification_loss: 0.4555
2756/3000 [==========================>...] - ETA: 2:00 - loss: 1.6487 - regression_loss: 1.1933 - classification_loss: 0.4554
2757/3000 [==========================>...] - ETA: 2:00 - loss: 1.6484 - regression_loss: 1.1930 - classification_loss: 0.4554
2758/3000 [==========================>...] - ETA: 1:59 - loss: 1.6483 - regression_loss: 1.1929 - classification_loss: 0.4554
2759/3000 [==========================>...] - ETA: 1:59 - loss: 1.6481 - regression_loss: 1.1927 - classification_loss: 0.4554
2760/3000 [==========================>...] - ETA: 1:58 - loss: 1.6478 - regression_loss: 1.1925 - classification_loss: 0.4553
2761/3000 [==========================>...] - ETA: 1:58 - loss: 1.6475 - regression_loss: 1.1922 - classification_loss: 0.4553
2762/3000 [==========================>...] - ETA: 1:57 - loss: 1.6471 - regression_loss: 1.1919 - classification_loss: 0.4553
2763/3000 [==========================>...] - ETA: 1:57 - loss: 1.6469 - regression_loss: 1.1916 - classification_loss: 0.4552
2764/3000 [==========================>...] - ETA: 1:56 - loss: 1.6466 - regression_loss: 1.1914 - classification_loss: 0.4552
2765/3000 [==========================>...] - ETA: 1:56 - loss: 1.6463 - regression_loss: 1.1912 - classification_loss: 0.4552
2766/3000 [==========================>...] - ETA: 1:55 - loss: 1.6460 - regression_loss: 1.1909 - classification_loss: 0.4551
2767/3000 [==========================>...] - ETA: 1:55 - loss: 1.6459 - regression_loss: 1.1908 - classification_loss: 0.4551
2768/3000 [==========================>...] - ETA: 1:54 - loss: 1.6456 - regression_loss: 1.1906 - classification_loss: 0.4550
2769/3000 [==========================>...] - ETA: 1:54 - loss: 1.6453 - regression_loss: 1.1903 - classification_loss: 0.4550
2770/3000 [==========================>...] - ETA: 1:53 - loss: 1.6451 - regression_loss: 1.1902 - classification_loss: 0.4550
2771/3000 [==========================>...] - ETA: 1:53 - loss: 1.6448 - regression_loss: 1.1899 - classification_loss: 0.4549
2772/3000 [==========================>...] - ETA: 1:52 - loss: 1.6445 - regression_loss: 1.1896 - classification_loss: 0.4548
2773/3000 [==========================>...] - ETA: 1:52 - loss: 1.6442 - regression_loss: 1.1894 - classification_loss: 0.4548
2774/3000 [==========================>...] - ETA: 1:51 - loss: 1.6441 - regression_loss: 1.1893 - classification_loss: 0.4547
2775/3000 [==========================>...] - ETA: 1:51 - loss: 1.6438 - regression_loss: 1.1891 - classification_loss: 0.4547
2776/3000 [==========================>...] - ETA: 1:50 - loss: 1.6434 - regression_loss: 1.1888 - classification_loss: 0.4546
2777/3000 [==========================>...] - ETA: 1:50 - loss: 1.6432 - regression_loss: 1.1886 - classification_loss: 0.4546
2778/3000 [==========================>...] - ETA: 1:49 - loss: 1.6430 - regression_loss: 1.1884 - classification_loss: 0.4545
2779/3000 [==========================>...] - ETA: 1:49 - loss: 1.6427 - regression_loss: 1.1882 - classification_loss: 0.4545
2780/3000 [==========================>...] - ETA: 1:48 - loss: 1.6423 - regression_loss: 1.1879 - classification_loss: 0.4544
2781/3000 [==========================>...] - ETA: 1:48 - loss: 1.6421 - regression_loss: 1.1877 - classification_loss: 0.4544
2782/3000 [==========================>...] - ETA: 1:47 - loss: 1.6418 - regression_loss: 1.1874 - classification_loss: 0.4543
2783/3000 [==========================>...] - ETA: 1:47 - loss: 1.6415 - regression_loss: 1.1872 - classification_loss: 0.4543
2784/3000 [==========================>...] - ETA: 1:46 - loss: 1.6412 - regression_loss: 1.1869 - classification_loss: 0.4543
2785/3000 [==========================>...] - ETA: 1:46 - loss: 1.6410 - regression_loss: 1.1868 - classification_loss: 0.4543
2786/3000 [==========================>...] - ETA: 1:45 - loss: 1.6410 - regression_loss: 1.1868 - classification_loss: 0.4542
2787/3000 [==========================>...] - ETA: 1:45 - loss: 1.6406 - regression_loss: 1.1865 - classification_loss: 0.4541
2788/3000 [==========================>...] - ETA: 1:44 - loss: 1.6403 - regression_loss: 1.1863 - classification_loss: 0.4541
2789/3000 [==========================>...] - ETA: 1:44 - loss: 1.6402 - regression_loss: 1.1862 - classification_loss: 0.4540
2790/3000 [==========================>...] - ETA: 1:43 - loss: 1.6399 - regression_loss: 1.1859 - classification_loss: 0.4540
2791/3000 [==========================>...] - ETA: 1:43 - loss: 1.6397 - regression_loss: 1.1857 - classification_loss: 0.4539
2792/3000 [==========================>...] - ETA: 1:42 - loss: 1.6394 - regression_loss: 1.1855 - classification_loss: 0.4539
2793/3000 [==========================>...] - ETA: 1:42 - loss: 1.6393 - regression_loss: 1.1854 - classification_loss: 0.4539
2794/3000 [==========================>...] - ETA: 1:41 - loss: 1.6389 - regression_loss: 1.1851 - classification_loss: 0.4538
2795/3000 [==========================>...] - ETA: 1:41 - loss: 1.6387 - regression_loss: 1.1849 - classification_loss: 0.4538
2796/3000 [==========================>...] - ETA: 1:40 - loss: 1.6385 - regression_loss: 1.1848 - classification_loss: 0.4538
2797/3000 [==========================>...] - ETA: 1:40 - loss: 1.6382 - regression_loss: 1.1845 - classification_loss: 0.4537
2798/3000 [==========================>...] - ETA: 1:39 - loss: 1.6379 - regression_loss: 1.1842 - classification_loss: 0.4537
2799/3000 [==========================>...] - ETA: 1:39 - loss: 1.6376 - regression_loss: 1.1839 - classification_loss: 0.4537
2800/3000 [===========================>..] - ETA: 1:38 - loss: 1.6372 - regression_loss: 1.1836 - classification_loss: 0.4536
2801/3000 [===========================>..] - ETA: 1:38 - loss: 1.6370 - regression_loss: 1.1835 - classification_loss: 0.4536
2802/3000 [===========================>..] - ETA: 1:37 - loss: 1.6366 - regression_loss: 1.1831 - classification_loss: 0.4535
2803/3000 [===========================>..] - ETA: 1:37 - loss: 1.6363 - regression_loss: 1.1829 - classification_loss: 0.4534
2804/3000 [===========================>..] - ETA: 1:36 - loss: 1.6362 - regression_loss: 1.1828 - classification_loss: 0.4534
2805/3000 [===========================>..] - ETA: 1:36 - loss: 1.6358 - regression_loss: 1.1825 - classification_loss: 0.4534
2806/3000 [===========================>..] - ETA: 1:35 - loss: 1.6355 - regression_loss: 1.1822 - classification_loss: 0.4533
2807/3000 [===========================>..] - ETA: 1:35 - loss: 1.6354 - regression_loss: 1.1820 - classification_loss: 0.4534
2808/3000 [===========================>..] - ETA: 1:34 - loss: 1.6353 - regression_loss: 1.1819 - classification_loss: 0.4533
2809/3000 [===========================>..] - ETA: 1:34 - loss: 1.6351 - regression_loss: 1.1818 - classification_loss: 0.4533
2810/3000 [===========================>..] - ETA: 1:33 - loss: 1.6348 - regression_loss: 1.1816 - classification_loss: 0.4532
2811/3000 [===========================>..] - ETA: 1:33 - loss: 1.6345 - regression_loss: 1.1814 - classification_loss: 0.4531
2812/3000 [===========================>..] - ETA: 1:32 - loss: 1.6343 - regression_loss: 1.1812 - classification_loss: 0.4531
2813/3000 [===========================>..] - ETA: 1:32 - loss: 1.6339 - regression_loss: 1.1809 - classification_loss: 0.4530
2814/3000 [===========================>..] - ETA: 1:31 - loss: 1.6335 - regression_loss: 1.1806 - classification_loss: 0.4529
2815/3000 [===========================>..] - ETA: 1:31 - loss: 1.6333 - regression_loss: 1.1804 - classification_loss: 0.4529
2816/3000 [===========================>..] - ETA: 1:30 - loss: 1.6330 - regression_loss: 1.1801 - classification_loss: 0.4529
2817/3000 [===========================>..] - ETA: 1:30 - loss: 1.6329 - regression_loss: 1.1800 - classification_loss: 0.4529
2818/3000 [===========================>..] - ETA: 1:29 - loss: 1.6327 - regression_loss: 1.1799 - classification_loss: 0.4528
2819/3000 [===========================>..] - ETA: 1:29 - loss: 1.6324 - regression_loss: 1.1796 - classification_loss: 0.4528
2820/3000 [===========================>..] - ETA: 1:28 - loss: 1.6322 - regression_loss: 1.1795 - classification_loss: 0.4528
2821/3000 [===========================>..] - ETA: 1:28 - loss: 1.6320 - regression_loss: 1.1793 - classification_loss: 0.4527
2822/3000 [===========================>..] - ETA: 1:27 - loss: 1.6319 - regression_loss: 1.1791 - classification_loss: 0.4527
2823/3000 [===========================>..] - ETA: 1:27 - loss: 1.6316 - regression_loss: 1.1789 - classification_loss: 0.4527
2824/3000 [===========================>..] - ETA: 1:26 - loss: 1.6314 - regression_loss: 1.1788 - classification_loss: 0.4526
2825/3000 [===========================>..] - ETA: 1:26 - loss: 1.6311 - regression_loss: 1.1785 - classification_loss: 0.4526
2826/3000 [===========================>..] - ETA: 1:25 - loss: 1.6308 - regression_loss: 1.1783 - classification_loss: 0.4525
2827/3000 [===========================>..] - ETA: 1:25 - loss: 1.6305 - regression_loss: 1.1781 - classification_loss: 0.4525
2828/3000 [===========================>..] - ETA: 1:24 - loss: 1.6302 - regression_loss: 1.1778 - classification_loss: 0.4524
2829/3000 [===========================>..] - ETA: 1:24 - loss: 1.6298 - regression_loss: 1.1774 - classification_loss: 0.4524
2830/3000 [===========================>..] - ETA: 1:23 - loss: 1.6296 - regression_loss: 1.1773 - classification_loss: 0.4523
2831/3000 [===========================>..] - ETA: 1:23 - loss: 1.6293 - regression_loss: 1.1770 - classification_loss: 0.4523
2832/3000 [===========================>..] - ETA: 1:22 - loss: 1.6290 - regression_loss: 1.1767 - classification_loss: 0.4523
2833/3000 [===========================>..] - ETA: 1:22 - loss: 1.6287 - regression_loss: 1.1765 - classification_loss: 0.4522
2834/3000 [===========================>..] - ETA: 1:21 - loss: 1.6285 - regression_loss: 1.1763 - classification_loss: 0.4522
2835/3000 [===========================>..] - ETA: 1:21 - loss: 1.6282 - regression_loss: 1.1761 - classification_loss: 0.4522
2836/3000 [===========================>..] - ETA: 1:20 - loss: 1.6281 - regression_loss: 1.1759 - classification_loss: 0.4521
2837/3000 [===========================>..] - ETA: 1:20 - loss: 1.6277 - regression_loss: 1.1756 - classification_loss: 0.4521
2838/3000 [===========================>..] - ETA: 1:19 - loss: 1.6274 - regression_loss: 1.1754 - classification_loss: 0.4520
2839/3000 [===========================>..] - ETA: 1:19 - loss: 1.6271 - regression_loss: 1.1751 - classification_loss: 0.4520
2840/3000 [===========================>..] - ETA: 1:18 - loss: 1.6268 - regression_loss: 1.1748 - classification_loss: 0.4519
2841/3000 [===========================>..] - ETA: 1:18 - loss: 1.6264 - regression_loss: 1.1745 - classification_loss: 0.4519
2842/3000 [===========================>..] - ETA: 1:17 - loss: 1.6261 - regression_loss: 1.1742 - classification_loss: 0.4518
2843/3000 [===========================>..] - ETA: 1:17 - loss: 1.6257 - regression_loss: 1.1739 - classification_loss: 0.4518
2844/3000 [===========================>..] - ETA: 1:16 - loss: 1.6253 - regression_loss: 1.1736 - classification_loss: 0.4518
2845/3000 [===========================>..] - ETA: 1:16 - loss: 1.6251 - regression_loss: 1.1734 - classification_loss: 0.4517
2846/3000 [===========================>..] - ETA: 1:15 - loss: 1.6248 - regression_loss: 1.1731 - classification_loss: 0.4517
2847/3000 [===========================>..] - ETA: 1:15 - loss: 1.6245 - regression_loss: 1.1728 - classification_loss: 0.4516
2848/3000 [===========================>..] - ETA: 1:14 - loss: 1.6242 - regression_loss: 1.1726 - classification_loss: 0.4516
2849/3000 [===========================>..] - ETA: 1:14 - loss: 1.6239 - regression_loss: 1.1723 - classification_loss: 0.4516
2850/3000 [===========================>..] - ETA: 1:13 - loss: 1.6236 - regression_loss: 1.1720 - classification_loss: 0.4516
2851/3000 [===========================>..] - ETA: 1:13 - loss: 1.6233 - regression_loss: 1.1717 - classification_loss: 0.4515
2852/3000 [===========================>..] - ETA: 1:12 - loss: 1.6231 - regression_loss: 1.1716 - classification_loss: 0.4515
2853/3000 [===========================>..] - ETA: 1:12 - loss: 1.6228 - regression_loss: 1.1714 - classification_loss: 0.4514
2854/3000 [===========================>..] - ETA: 1:11 - loss: 1.6226 - regression_loss: 1.1713 - classification_loss: 0.4514
2855/3000 [===========================>..] - ETA: 1:11 - loss: 1.6223 - regression_loss: 1.1710 - classification_loss: 0.4513
2856/3000 [===========================>..] - ETA: 1:10 - loss: 1.6222 - regression_loss: 1.1709 - classification_loss: 0.4513
2857/3000 [===========================>..] - ETA: 1:10 - loss: 1.6219 - regression_loss: 1.1706 - classification_loss: 0.4512
2858/3000 [===========================>..] - ETA: 1:09 - loss: 1.6216 - regression_loss: 1.1704 - classification_loss: 0.4512
2859/3000 [===========================>..] - ETA: 1:09 - loss: 1.6213 - regression_loss: 1.1701 - classification_loss: 0.4512
2860/3000 [===========================>..] - ETA: 1:08 - loss: 1.6211 - regression_loss: 1.1699 - classification_loss: 0.4512
2861/3000 [===========================>..] - ETA: 1:08 - loss: 1.6208 - regression_loss: 1.1696 - classification_loss: 0.4512
2862/3000 [===========================>..] - ETA: 1:07 - loss: 1.6204 - regression_loss: 1.1693 - classification_loss: 0.4511
2863/3000 [===========================>..] - ETA: 1:07 - loss: 1.6203 - regression_loss: 1.1693 - classification_loss: 0.4510
2864/3000 [===========================>..] - ETA: 1:06 - loss: 1.6200 - regression_loss: 1.1690 - classification_loss: 0.4510
2865/3000 [===========================>..] - ETA: 1:06 - loss: 1.6199 - regression_loss: 1.1689 - classification_loss: 0.4510
2866/3000 [===========================>..] - ETA: 1:05 - loss: 1.6198 - regression_loss: 1.1688 - classification_loss: 0.4509
2867/3000 [===========================>..] - ETA: 1:05 - loss: 1.6195 - regression_loss: 1.1686 - classification_loss: 0.4509
2868/3000 [===========================>..] - ETA: 1:04 - loss: 1.6192 - regression_loss: 1.1683 - classification_loss: 0.4508
2869/3000 [===========================>..] - ETA: 1:04 - loss: 1.6188 - regression_loss: 1.1680 - classification_loss: 0.4508
2870/3000 [===========================>..] - ETA: 1:03 - loss: 1.6185 - regression_loss: 1.1677 - classification_loss: 0.4508
2871/3000 [===========================>..] - ETA: 1:03 - loss: 1.6181 - regression_loss: 1.1674 - classification_loss: 0.4507
2872/3000 [===========================>..] - ETA: 1:02 - loss: 1.6179 - regression_loss: 1.1672 - classification_loss: 0.4507
2873/3000 [===========================>..] - ETA: 1:02 - loss: 1.6177 - regression_loss: 1.1670 - classification_loss: 0.4507
2874/3000 [===========================>..] - ETA: 1:01 - loss: 1.6175 - regression_loss: 1.1669 - classification_loss: 0.4506
2875/3000 [===========================>..] - ETA: 1:01 - loss: 1.6172 - regression_loss: 1.1666 - classification_loss: 0.4506
2876/3000 [===========================>..] - ETA: 1:00 - loss: 1.6169 - regression_loss: 1.1664 - classification_loss: 0.4506
2877/3000 [===========================>..] - ETA: 1:00 - loss: 1.6167 - regression_loss: 1.1661 - classification_loss: 0.4505
2878/3000 [===========================>..] - ETA: 59s - loss: 1.6164 - regression_loss: 1.1659 - classification_loss: 0.4505 
2879/3000 [===========================>..] - ETA: 59s - loss: 1.6160 - regression_loss: 1.1656 - classification_loss: 0.4504
2880/3000 [===========================>..] - ETA: 58s - loss: 1.6158 - regression_loss: 1.1654 - classification_loss: 0.4504
2881/3000 [===========================>..] - ETA: 58s - loss: 1.6156 - regression_loss: 1.1652 - classification_loss: 0.4503
2882/3000 [===========================>..] - ETA: 57s - loss: 1.6153 - regression_loss: 1.1650 - classification_loss: 0.4503
2883/3000 [===========================>..] - ETA: 57s - loss: 1.6150 - regression_loss: 1.1648 - classification_loss: 0.4503
2884/3000 [===========================>..] - ETA: 56s - loss: 1.6148 - regression_loss: 1.1646 - classification_loss: 0.4502
2885/3000 [===========================>..] - ETA: 56s - loss: 1.6146 - regression_loss: 1.1644 - classification_loss: 0.4501
2886/3000 [===========================>..] - ETA: 55s - loss: 1.6143 - regression_loss: 1.1642 - classification_loss: 0.4501
2887/3000 [===========================>..] - ETA: 55s - loss: 1.6141 - regression_loss: 1.1640 - classification_loss: 0.4501
2888/3000 [===========================>..] - ETA: 54s - loss: 1.6138 - regression_loss: 1.1638 - classification_loss: 0.4500
2889/3000 [===========================>..] - ETA: 54s - loss: 1.6134 - regression_loss: 1.1635 - classification_loss: 0.4499
2890/3000 [===========================>..] - ETA: 53s - loss: 1.6133 - regression_loss: 1.1634 - classification_loss: 0.4499
2891/3000 [===========================>..] - ETA: 53s - loss: 1.6129 - regression_loss: 1.1631 - classification_loss: 0.4498
2892/3000 [===========================>..] - ETA: 52s - loss: 1.6125 - regression_loss: 1.1628 - classification_loss: 0.4497
2893/3000 [===========================>..] - ETA: 52s - loss: 1.6122 - regression_loss: 1.1624 - classification_loss: 0.4497
2894/3000 [===========================>..] - ETA: 51s - loss: 1.6119 - regression_loss: 1.1621 - classification_loss: 0.4497
2895/3000 [===========================>..] - ETA: 51s - loss: 1.6115 - regression_loss: 1.1619 - classification_loss: 0.4497
2896/3000 [===========================>..] - ETA: 51s - loss: 1.6113 - regression_loss: 1.1617 - classification_loss: 0.4496
2897/3000 [===========================>..] - ETA: 50s - loss: 1.6112 - regression_loss: 1.1616 - classification_loss: 0.4496
2898/3000 [===========================>..] - ETA: 50s - loss: 1.6109 - regression_loss: 1.1613 - classification_loss: 0.4496
2899/3000 [===========================>..] - ETA: 49s - loss: 1.6107 - regression_loss: 1.1611 - classification_loss: 0.4496
2900/3000 [============================>.] - ETA: 49s - loss: 1.6103 - regression_loss: 1.1608 - classification_loss: 0.4495
2901/3000 [============================>.] - ETA: 48s - loss: 1.6100 - regression_loss: 1.1606 - classification_loss: 0.4494
2902/3000 [============================>.] - ETA: 48s - loss: 1.6098 - regression_loss: 1.1604 - classification_loss: 0.4494
2903/3000 [============================>.] - ETA: 47s - loss: 1.6095 - regression_loss: 1.1602 - classification_loss: 0.4493
2904/3000 [============================>.] - ETA: 47s - loss: 1.6092 - regression_loss: 1.1600 - classification_loss: 0.4493
2905/3000 [============================>.] - ETA: 46s - loss: 1.6088 - regression_loss: 1.1596 - classification_loss: 0.4492
2906/3000 [============================>.] - ETA: 46s - loss: 1.6085 - regression_loss: 1.1594 - classification_loss: 0.4492
2907/3000 [============================>.] - ETA: 45s - loss: 1.6082 - regression_loss: 1.1591 - classification_loss: 0.4492
2908/3000 [============================>.] - ETA: 45s - loss: 1.6081 - regression_loss: 1.1590 - classification_loss: 0.4491
2909/3000 [============================>.] - ETA: 44s - loss: 1.6078 - regression_loss: 1.1587 - classification_loss: 0.4491
2910/3000 [============================>.] - ETA: 44s - loss: 1.6075 - regression_loss: 1.1585 - classification_loss: 0.4490
2911/3000 [============================>.] - ETA: 43s - loss: 1.6072 - regression_loss: 1.1583 - classification_loss: 0.4489
2912/3000 [============================>.] - ETA: 43s - loss: 1.6070 - regression_loss: 1.1581 - classification_loss: 0.4489
2913/3000 [============================>.] - ETA: 42s - loss: 1.6068 - regression_loss: 1.1579 - classification_loss: 0.4489
2914/3000 [============================>.] - ETA: 42s - loss: 1.6067 - regression_loss: 1.1578 - classification_loss: 0.4489
2915/3000 [============================>.] - ETA: 41s - loss: 1.6065 - regression_loss: 1.1577 - classification_loss: 0.4488
2916/3000 [============================>.] - ETA: 41s - loss: 1.6063 - regression_loss: 1.1574 - classification_loss: 0.4489
2917/3000 [============================>.] - ETA: 40s - loss: 1.6060 - regression_loss: 1.1572 - classification_loss: 0.4488
2918/3000 [============================>.] - ETA: 40s - loss: 1.6058 - regression_loss: 1.1570 - classification_loss: 0.4488
2919/3000 [============================>.] - ETA: 39s - loss: 1.6055 - regression_loss: 1.1568 - classification_loss: 0.4488
2920/3000 [============================>.] - ETA: 39s - loss: 1.6053 - regression_loss: 1.1566 - classification_loss: 0.4487
2921/3000 [============================>.] - ETA: 38s - loss: 1.6049 - regression_loss: 1.1563 - classification_loss: 0.4486
2922/3000 [============================>.] - ETA: 38s - loss: 1.6046 - regression_loss: 1.1560 - classification_loss: 0.4486
2923/3000 [============================>.] - ETA: 37s - loss: 1.6044 - regression_loss: 1.1559 - classification_loss: 0.4485
2924/3000 [============================>.] - ETA: 37s - loss: 1.6041 - regression_loss: 1.1556 - classification_loss: 0.4485
2925/3000 [============================>.] - ETA: 36s - loss: 1.6039 - regression_loss: 1.1555 - classification_loss: 0.4485
2926/3000 [============================>.] - ETA: 36s - loss: 1.6036 - regression_loss: 1.1552 - classification_loss: 0.4484
2927/3000 [============================>.] - ETA: 35s - loss: 1.6034 - regression_loss: 1.1550 - classification_loss: 0.4483
2928/3000 [============================>.] - ETA: 35s - loss: 1.6031 - regression_loss: 1.1548 - classification_loss: 0.4483
2929/3000 [============================>.] - ETA: 34s - loss: 1.6028 - regression_loss: 1.1545 - classification_loss: 0.4483
2930/3000 [============================>.] - ETA: 34s - loss: 1.6026 - regression_loss: 1.1543 - classification_loss: 0.4483
2931/3000 [============================>.] - ETA: 33s - loss: 1.6024 - regression_loss: 1.1541 - classification_loss: 0.4483
2932/3000 [============================>.] - ETA: 33s - loss: 1.6021 - regression_loss: 1.1539 - classification_loss: 0.4482
2933/3000 [============================>.] - ETA: 32s - loss: 1.6018 - regression_loss: 1.1537 - classification_loss: 0.4482
2934/3000 [============================>.] - ETA: 32s - loss: 1.6015 - regression_loss: 1.1534 - classification_loss: 0.4481
2935/3000 [============================>.] - ETA: 31s - loss: 1.6013 - regression_loss: 1.1532 - classification_loss: 0.4481
2936/3000 [============================>.] - ETA: 31s - loss: 1.6009 - regression_loss: 1.1529 - classification_loss: 0.4481
2937/3000 [============================>.] - ETA: 30s - loss: 1.6006 - regression_loss: 1.1526 - classification_loss: 0.4480
2938/3000 [============================>.] - ETA: 30s - loss: 1.6004 - regression_loss: 1.1524 - classification_loss: 0.4480
2939/3000 [============================>.] - ETA: 29s - loss: 1.6003 - regression_loss: 1.1523 - classification_loss: 0.4480
2940/3000 [============================>.] - ETA: 29s - loss: 1.6001 - regression_loss: 1.1522 - classification_loss: 0.4479
2941/3000 [============================>.] - ETA: 28s - loss: 1.5998 - regression_loss: 1.1519 - classification_loss: 0.4478
2942/3000 [============================>.] - ETA: 28s - loss: 1.5995 - regression_loss: 1.1517 - classification_loss: 0.4478
2943/3000 [============================>.] - ETA: 27s - loss: 1.5993 - regression_loss: 1.1515 - classification_loss: 0.4478
2944/3000 [============================>.] - ETA: 27s - loss: 1.5992 - regression_loss: 1.1514 - classification_loss: 0.4477
2945/3000 [============================>.] - ETA: 26s - loss: 1.5989 - regression_loss: 1.1512 - classification_loss: 0.4477
2946/3000 [============================>.] - ETA: 26s - loss: 1.5986 - regression_loss: 1.1510 - classification_loss: 0.4476
2947/3000 [============================>.] - ETA: 25s - loss: 1.5983 - regression_loss: 1.1508 - classification_loss: 0.4476
2948/3000 [============================>.] - ETA: 25s - loss: 1.5981 - regression_loss: 1.1506 - classification_loss: 0.4475
2949/3000 [============================>.] - ETA: 24s - loss: 1.5979 - regression_loss: 1.1504 - classification_loss: 0.4475
2950/3000 [============================>.] - ETA: 24s - loss: 1.5976 - regression_loss: 1.1502 - classification_loss: 0.4474
2951/3000 [============================>.] - ETA: 23s - loss: 1.5972 - regression_loss: 1.1499 - classification_loss: 0.4473
2952/3000 [============================>.] - ETA: 23s - loss: 1.5971 - regression_loss: 1.1498 - classification_loss: 0.4473
2953/3000 [============================>.] - ETA: 22s - loss: 1.5969 - regression_loss: 1.1496 - classification_loss: 0.4473
2954/3000 [============================>.] - ETA: 22s - loss: 1.5965 - regression_loss: 1.1493 - classification_loss: 0.4472
2955/3000 [============================>.] - ETA: 21s - loss: 1.5963 - regression_loss: 1.1491 - classification_loss: 0.4472
2956/3000 [============================>.] - ETA: 21s - loss: 1.5960 - regression_loss: 1.1489 - classification_loss: 0.4471
2957/3000 [============================>.] - ETA: 21s - loss: 1.5958 - regression_loss: 1.1487 - classification_loss: 0.4470
2958/3000 [============================>.] - ETA: 20s - loss: 1.5956 - regression_loss: 1.1486 - classification_loss: 0.4470
2959/3000 [============================>.] - ETA: 20s - loss: 1.5952 - regression_loss: 1.1483 - classification_loss: 0.4469
2960/3000 [============================>.] - ETA: 19s - loss: 1.5951 - regression_loss: 1.1483 - classification_loss: 0.4469
2961/3000 [============================>.] - ETA: 19s - loss: 1.5949 - regression_loss: 1.1481 - classification_loss: 0.4468
2962/3000 [============================>.] - ETA: 18s - loss: 1.5947 - regression_loss: 1.1479 - classification_loss: 0.4468
2963/3000 [============================>.] - ETA: 18s - loss: 1.5945 - regression_loss: 1.1477 - classification_loss: 0.4468
2964/3000 [============================>.] - ETA: 17s - loss: 1.5943 - regression_loss: 1.1475 - classification_loss: 0.4467
2965/3000 [============================>.] - ETA: 17s - loss: 1.5942 - regression_loss: 1.1475 - classification_loss: 0.4467
2966/3000 [============================>.] - ETA: 16s - loss: 1.5939 - regression_loss: 1.1473 - classification_loss: 0.4467
2967/3000 [============================>.] - ETA: 16s - loss: 1.5938 - regression_loss: 1.1471 - classification_loss: 0.4466
2968/3000 [============================>.] - ETA: 15s - loss: 1.5935 - regression_loss: 1.1470 - classification_loss: 0.4465
2969/3000 [============================>.] - ETA: 15s - loss: 1.5933 - regression_loss: 1.1468 - classification_loss: 0.4465
2970/3000 [============================>.] - ETA: 14s - loss: 1.5931 - regression_loss: 1.1466 - classification_loss: 0.4464
2971/3000 [============================>.] - ETA: 14s - loss: 1.5930 - regression_loss: 1.1466 - classification_loss: 0.4463
2972/3000 [============================>.] - ETA: 13s - loss: 1.5930 - regression_loss: 1.1467 - classification_loss: 0.4463
2973/3000 [============================>.] - ETA: 13s - loss: 1.5928 - regression_loss: 1.1465 - classification_loss: 0.4463
2974/3000 [============================>.] - ETA: 12s - loss: 1.5927 - regression_loss: 1.1465 - classification_loss: 0.4462
2975/3000 [============================>.] - ETA: 12s - loss: 1.5924 - regression_loss: 1.1462 - classification_loss: 0.4462
2976/3000 [============================>.] - ETA: 11s - loss: 1.5922 - regression_loss: 1.1460 - classification_loss: 0.4462
2977/3000 [============================>.] - ETA: 11s - loss: 1.5920 - regression_loss: 1.1459 - classification_loss: 0.4462
2978/3000 [============================>.] - ETA: 10s - loss: 1.5917 - regression_loss: 1.1456 - classification_loss: 0.4461
2979/3000 [============================>.] - ETA: 10s - loss: 1.5915 - regression_loss: 1.1454 - classification_loss: 0.4461
2980/3000 [============================>.] - ETA: 9s - loss: 1.5913 - regression_loss: 1.1452 - classification_loss: 0.4461 
2981/3000 [============================>.] - ETA: 9s - loss: 1.5911 - regression_loss: 1.1451 - classification_loss: 0.4461
2982/3000 [============================>.] - ETA: 8s - loss: 1.5908 - regression_loss: 1.1449 - classification_loss: 0.4460
2983/3000 [============================>.] - ETA: 8s - loss: 1.5905 - regression_loss: 1.1446 - classification_loss: 0.4459
2984/3000 [============================>.] - ETA: 7s - loss: 1.5903 - regression_loss: 1.1443 - classification_loss: 0.4459
2985/3000 [============================>.] - ETA: 7s - loss: 1.5900 - regression_loss: 1.1441 - classification_loss: 0.4458
2986/3000 [============================>.] - ETA: 6s - loss: 1.5897 - regression_loss: 1.1439 - classification_loss: 0.4458
2987/3000 [============================>.] - ETA: 6s - loss: 1.5896 - regression_loss: 1.1438 - classification_loss: 0.4457
2988/3000 [============================>.] - ETA: 5s - loss: 1.5893 - regression_loss: 1.1436 - classification_loss: 0.4457
2989/3000 [============================>.] - ETA: 5s - loss: 1.5889 - regression_loss: 1.1433 - classification_loss: 0.4456
2990/3000 [============================>.] - ETA: 4s - loss: 1.5886 - regression_loss: 1.1430 - classification_loss: 0.4456
2991/3000 [============================>.] - ETA: 4s - loss: 1.5883 - regression_loss: 1.1427 - classification_loss: 0.4455
2992/3000 [============================>.] - ETA: 3s - loss: 1.5880 - regression_loss: 1.1425 - classification_loss: 0.4455
2993/3000 [============================>.] - ETA: 3s - loss: 1.5878 - regression_loss: 1.1423 - classification_loss: 0.4454
2994/3000 [============================>.] - ETA: 2s - loss: 1.5876 - regression_loss: 1.1422 - classification_loss: 0.4454
2995/3000 [============================>.] - ETA: 2s - loss: 1.5873 - regression_loss: 1.1419 - classification_loss: 0.4453
2996/3000 [============================>.] - ETA: 1s - loss: 1.5870 - regression_loss: 1.1417 - classification_loss: 0.4453
2997/3000 [============================>.] - ETA: 1s - loss: 1.5867 - regression_loss: 1.1414 - classification_loss: 0.4453
2998/3000 [============================>.] - ETA: 0s - loss: 1.5864 - regression_loss: 1.1411 - classification_loss: 0.4452
2999/3000 [============================>.] - ETA: 0s - loss: 1.5861 - regression_loss: 1.1409 - classification_loss: 0.4452Epoch 00001: saving model to ./snapshots/resnet50_csv_01.h5

3000/3000 [==============================] - 1519s 506ms/step - loss: 1.5858 - regression_loss: 1.1407 - classification_loss: 0.4451
Epoch 2/50

   1/3000 [..............................] - ETA: 20:46 - loss: 0.6286 - regression_loss: 0.2921 - classification_loss: 0.3365
   2/3000 [..............................] - ETA: 20:10 - loss: 0.8104 - regression_loss: 0.4499 - classification_loss: 0.3605
   3/3000 [..............................] - ETA: 20:06 - loss: 0.8772 - regression_loss: 0.5550 - classification_loss: 0.3221
   4/3000 [..............................] - ETA: 19:59 - loss: 0.8720 - regression_loss: 0.5138 - classification_loss: 0.3582
   5/3000 [..............................] - ETA: 19:45 - loss: 0.8294 - regression_loss: 0.4760 - classification_loss: 0.3534
   6/3000 [..............................] - ETA: 19:39 - loss: 0.8548 - regression_loss: 0.4966 - classification_loss: 0.3582
   7/3000 [..............................] - ETA: 19:37 - loss: 0.8713 - regression_loss: 0.5122 - classification_loss: 0.3591
   8/3000 [..............................] - ETA: 19:36 - loss: 0.8281 - regression_loss: 0.4844 - classification_loss: 0.3437
   9/3000 [..............................] - ETA: 19:24 - loss: 0.8098 - regression_loss: 0.4708 - classification_loss: 0.3390
  10/3000 [..............................] - ETA: 19:21 - loss: 0.8093 - regression_loss: 0.4768 - classification_loss: 0.3325
  11/3000 [..............................] - ETA: 19:19 - loss: 0.8151 - regression_loss: 0.4778 - classification_loss: 0.3373
  12/3000 [..............................] - ETA: 19:18 - loss: 0.8146 - regression_loss: 0.4778 - classification_loss: 0.3368
  13/3000 [..............................] - ETA: 19:18 - loss: 0.8414 - regression_loss: 0.5021 - classification_loss: 0.3392
  14/3000 [..............................] - ETA: 19:18 - loss: 0.8216 - regression_loss: 0.4859 - classification_loss: 0.3357
  15/3000 [..............................] - ETA: 19:20 - loss: 0.8327 - regression_loss: 0.4860 - classification_loss: 0.3466
  16/3000 [..............................] - ETA: 19:20 - loss: 0.8475 - regression_loss: 0.5009 - classification_loss: 0.3466
  17/3000 [..............................] - ETA: 19:19 - loss: 0.8665 - regression_loss: 0.5227 - classification_loss: 0.3438
  18/3000 [..............................] - ETA: 19:19 - loss: 0.8650 - regression_loss: 0.5204 - classification_loss: 0.3447
  19/3000 [..............................] - ETA: 19:20 - loss: 0.8615 - regression_loss: 0.5215 - classification_loss: 0.3400
  20/3000 [..............................] - ETA: 19:24 - loss: 0.8711 - regression_loss: 0.5311 - classification_loss: 0.3401
  21/3000 [..............................] - ETA: 19:21 - loss: 0.8884 - regression_loss: 0.5452 - classification_loss: 0.3431
  22/3000 [..............................] - ETA: 19:21 - loss: 0.8994 - regression_loss: 0.5536 - classification_loss: 0.3458
  23/3000 [..............................] - ETA: 19:20 - loss: 0.8979 - regression_loss: 0.5595 - classification_loss: 0.3384
  24/3000 [..............................] - ETA: 19:18 - loss: 0.9021 - regression_loss: 0.5604 - classification_loss: 0.3418
  25/3000 [..............................] - ETA: 19:17 - loss: 0.8883 - regression_loss: 0.5511 - classification_loss: 0.3372
  26/3000 [..............................] - ETA: 19:18 - loss: 0.8863 - regression_loss: 0.5483 - classification_loss: 0.3380
  27/3000 [..............................] - ETA: 19:16 - loss: 0.9064 - regression_loss: 0.5681 - classification_loss: 0.3382
  28/3000 [..............................] - ETA: 19:16 - loss: 0.9265 - regression_loss: 0.5870 - classification_loss: 0.3395
  29/3000 [..............................] - ETA: 19:16 - loss: 0.9374 - regression_loss: 0.5985 - classification_loss: 0.3390
  30/3000 [..............................] - ETA: 19:16 - loss: 0.9383 - regression_loss: 0.6017 - classification_loss: 0.3366
  31/3000 [..............................] - ETA: 19:16 - loss: 0.9398 - regression_loss: 0.6065 - classification_loss: 0.3333
  32/3000 [..............................] - ETA: 19:16 - loss: 0.9358 - regression_loss: 0.6016 - classification_loss: 0.3342
  33/3000 [..............................] - ETA: 19:15 - loss: 0.9295 - regression_loss: 0.5965 - classification_loss: 0.3331
  34/3000 [..............................] - ETA: 19:15 - loss: 0.9198 - regression_loss: 0.5888 - classification_loss: 0.3309
  35/3000 [..............................] - ETA: 19:14 - loss: 0.9060 - regression_loss: 0.5781 - classification_loss: 0.3279
  36/3000 [..............................] - ETA: 19:13 - loss: 0.9135 - regression_loss: 0.5857 - classification_loss: 0.3278
  37/3000 [..............................] - ETA: 19:13 - loss: 0.9048 - regression_loss: 0.5775 - classification_loss: 0.3273
  38/3000 [..............................] - ETA: 19:13 - loss: 0.9005 - regression_loss: 0.5765 - classification_loss: 0.3240
  39/3000 [..............................] - ETA: 19:13 - loss: 0.8966 - regression_loss: 0.5734 - classification_loss: 0.3232
  40/3000 [..............................] - ETA: 19:12 - loss: 0.8979 - regression_loss: 0.5736 - classification_loss: 0.3243
  41/3000 [..............................] - ETA: 19:11 - loss: 0.9005 - regression_loss: 0.5746 - classification_loss: 0.3259
  42/3000 [..............................] - ETA: 19:11 - loss: 0.9016 - regression_loss: 0.5786 - classification_loss: 0.3230
  43/3000 [..............................] - ETA: 19:12 - loss: 0.9074 - regression_loss: 0.5838 - classification_loss: 0.3237
  44/3000 [..............................] - ETA: 19:11 - loss: 0.9030 - regression_loss: 0.5777 - classification_loss: 0.3253
  45/3000 [..............................] - ETA: 19:09 - loss: 0.9136 - regression_loss: 0.5886 - classification_loss: 0.3250
  46/3000 [..............................] - ETA: 19:09 - loss: 0.9225 - regression_loss: 0.5967 - classification_loss: 0.3258
  47/3000 [..............................] - ETA: 19:08 - loss: 0.9229 - regression_loss: 0.5978 - classification_loss: 0.3251
  48/3000 [..............................] - ETA: 19:06 - loss: 0.9225 - regression_loss: 0.5962 - classification_loss: 0.3263
  49/3000 [..............................] - ETA: 19:05 - loss: 0.9219 - regression_loss: 0.5971 - classification_loss: 0.3248
  50/3000 [..............................] - ETA: 19:05 - loss: 0.9275 - regression_loss: 0.6008 - classification_loss: 0.3267
  51/3000 [..............................] - ETA: 19:05 - loss: 0.9238 - regression_loss: 0.5967 - classification_loss: 0.3271
  52/3000 [..............................] - ETA: 19:05 - loss: 0.9252 - regression_loss: 0.5986 - classification_loss: 0.3266
  53/3000 [..............................] - ETA: 19:04 - loss: 0.9278 - regression_loss: 0.6017 - classification_loss: 0.3261
  54/3000 [..............................] - ETA: 19:04 - loss: 0.9275 - regression_loss: 0.6006 - classification_loss: 0.3270
  55/3000 [..............................] - ETA: 19:03 - loss: 0.9265 - regression_loss: 0.6008 - classification_loss: 0.3258
  56/3000 [..............................] - ETA: 19:03 - loss: 0.9370 - regression_loss: 0.6095 - classification_loss: 0.3275
  57/3000 [..............................] - ETA: 19:03 - loss: 0.9365 - regression_loss: 0.6086 - classification_loss: 0.3279
  58/3000 [..............................] - ETA: 19:03 - loss: 0.9334 - regression_loss: 0.6061 - classification_loss: 0.3273
  59/3000 [..............................] - ETA: 19:03 - loss: 0.9401 - regression_loss: 0.6119 - classification_loss: 0.3282
  60/3000 [..............................] - ETA: 19:03 - loss: 0.9397 - regression_loss: 0.6095 - classification_loss: 0.3302
  61/3000 [..............................] - ETA: 19:02 - loss: 0.9438 - regression_loss: 0.6141 - classification_loss: 0.3297
  62/3000 [..............................] - ETA: 19:01 - loss: 0.9453 - regression_loss: 0.6136 - classification_loss: 0.3317
  63/3000 [..............................] - ETA: 19:01 - loss: 0.9434 - regression_loss: 0.6115 - classification_loss: 0.3318
  64/3000 [..............................] - ETA: 19:01 - loss: 0.9443 - regression_loss: 0.6118 - classification_loss: 0.3325
  65/3000 [..............................] - ETA: 19:01 - loss: 0.9445 - regression_loss: 0.6112 - classification_loss: 0.3333
  66/3000 [..............................] - ETA: 19:00 - loss: 0.9400 - regression_loss: 0.6071 - classification_loss: 0.3329
  67/3000 [..............................] - ETA: 19:00 - loss: 0.9385 - regression_loss: 0.6067 - classification_loss: 0.3318
  68/3000 [..............................] - ETA: 18:59 - loss: 0.9352 - regression_loss: 0.6038 - classification_loss: 0.3314
  69/3000 [..............................] - ETA: 18:59 - loss: 0.9315 - regression_loss: 0.6005 - classification_loss: 0.3310
  70/3000 [..............................] - ETA: 18:58 - loss: 0.9272 - regression_loss: 0.5964 - classification_loss: 0.3308
  71/3000 [..............................] - ETA: 18:58 - loss: 0.9257 - regression_loss: 0.5936 - classification_loss: 0.3320
  72/3000 [..............................] - ETA: 18:58 - loss: 0.9263 - regression_loss: 0.5946 - classification_loss: 0.3317
  73/3000 [..............................] - ETA: 18:57 - loss: 0.9264 - regression_loss: 0.5952 - classification_loss: 0.3312
  74/3000 [..............................] - ETA: 18:57 - loss: 0.9274 - regression_loss: 0.5965 - classification_loss: 0.3309
  75/3000 [..............................] - ETA: 18:57 - loss: 0.9279 - regression_loss: 0.5951 - classification_loss: 0.3328
  76/3000 [..............................] - ETA: 18:57 - loss: 0.9291 - regression_loss: 0.5953 - classification_loss: 0.3338
  77/3000 [..............................] - ETA: 18:58 - loss: 0.9256 - regression_loss: 0.5926 - classification_loss: 0.3331
  78/3000 [..............................] - ETA: 18:58 - loss: 0.9251 - regression_loss: 0.5917 - classification_loss: 0.3334
  79/3000 [..............................] - ETA: 18:58 - loss: 0.9252 - regression_loss: 0.5926 - classification_loss: 0.3326
  80/3000 [..............................] - ETA: 18:59 - loss: 0.9263 - regression_loss: 0.5939 - classification_loss: 0.3325
  81/3000 [..............................] - ETA: 19:00 - loss: 0.9267 - regression_loss: 0.5932 - classification_loss: 0.3334
  82/3000 [..............................] - ETA: 19:00 - loss: 0.9237 - regression_loss: 0.5900 - classification_loss: 0.3336
  83/3000 [..............................] - ETA: 19:01 - loss: 0.9220 - regression_loss: 0.5882 - classification_loss: 0.3338
  84/3000 [..............................] - ETA: 19:01 - loss: 0.9190 - regression_loss: 0.5850 - classification_loss: 0.3340
  85/3000 [..............................] - ETA: 19:01 - loss: 0.9135 - regression_loss: 0.5807 - classification_loss: 0.3329
  86/3000 [..............................] - ETA: 19:02 - loss: 0.9132 - regression_loss: 0.5807 - classification_loss: 0.3325
  87/3000 [..............................] - ETA: 19:02 - loss: 0.9160 - regression_loss: 0.5838 - classification_loss: 0.3322
  88/3000 [..............................] - ETA: 19:02 - loss: 0.9132 - regression_loss: 0.5803 - classification_loss: 0.3329
  89/3000 [..............................] - ETA: 19:02 - loss: 0.9129 - regression_loss: 0.5812 - classification_loss: 0.3317
  90/3000 [..............................] - ETA: 19:03 - loss: 0.9110 - regression_loss: 0.5796 - classification_loss: 0.3315
  91/3000 [..............................] - ETA: 19:03 - loss: 0.9097 - regression_loss: 0.5785 - classification_loss: 0.3312
  92/3000 [..............................] - ETA: 19:03 - loss: 0.9065 - regression_loss: 0.5752 - classification_loss: 0.3312
  93/3000 [..............................] - ETA: 19:03 - loss: 0.9037 - regression_loss: 0.5738 - classification_loss: 0.3299
  94/3000 [..............................] - ETA: 19:04 - loss: 0.9035 - regression_loss: 0.5739 - classification_loss: 0.3296
  95/3000 [..............................] - ETA: 19:04 - loss: 0.9023 - regression_loss: 0.5725 - classification_loss: 0.3298
  96/3000 [..............................] - ETA: 19:05 - loss: 0.9079 - regression_loss: 0.5776 - classification_loss: 0.3303
  97/3000 [..............................] - ETA: 19:05 - loss: 0.9061 - regression_loss: 0.5749 - classification_loss: 0.3311
  98/3000 [..............................] - ETA: 19:05 - loss: 0.9075 - regression_loss: 0.5769 - classification_loss: 0.3306
  99/3000 [..............................] - ETA: 19:05 - loss: 0.9062 - regression_loss: 0.5760 - classification_loss: 0.3302
 100/3000 [>.............................] - ETA: 19:06 - loss: 0.9061 - regression_loss: 0.5767 - classification_loss: 0.3294
 101/3000 [>.............................] - ETA: 19:06 - loss: 0.9047 - regression_loss: 0.5751 - classification_loss: 0.3297
 102/3000 [>.............................] - ETA: 19:06 - loss: 0.9046 - regression_loss: 0.5746 - classification_loss: 0.3301
 103/3000 [>.............................] - ETA: 19:06 - loss: 0.9045 - regression_loss: 0.5740 - classification_loss: 0.3305
 104/3000 [>.............................] - ETA: 19:06 - loss: 0.9051 - regression_loss: 0.5751 - classification_loss: 0.3299
 105/3000 [>.............................] - ETA: 19:06 - loss: 0.9075 - regression_loss: 0.5780 - classification_loss: 0.3295
 106/3000 [>.............................] - ETA: 19:06 - loss: 0.9052 - regression_loss: 0.5757 - classification_loss: 0.3295
 107/3000 [>.............................] - ETA: 19:07 - loss: 0.9061 - regression_loss: 0.5781 - classification_loss: 0.3280
 108/3000 [>.............................] - ETA: 19:07 - loss: 0.9058 - regression_loss: 0.5785 - classification_loss: 0.3274
 109/3000 [>.............................] - ETA: 19:07 - loss: 0.9048 - regression_loss: 0.5773 - classification_loss: 0.3275
 110/3000 [>.............................] - ETA: 19:07 - loss: 0.9072 - regression_loss: 0.5798 - classification_loss: 0.3274
 111/3000 [>.............................] - ETA: 19:07 - loss: 0.9057 - regression_loss: 0.5783 - classification_loss: 0.3273
 112/3000 [>.............................] - ETA: 19:07 - loss: 0.9072 - regression_loss: 0.5800 - classification_loss: 0.3272
 113/3000 [>.............................] - ETA: 19:07 - loss: 0.9052 - regression_loss: 0.5790 - classification_loss: 0.3262
 114/3000 [>.............................] - ETA: 19:07 - loss: 0.9058 - regression_loss: 0.5798 - classification_loss: 0.3259
 115/3000 [>.............................] - ETA: 19:07 - loss: 0.9029 - regression_loss: 0.5776 - classification_loss: 0.3253
 116/3000 [>.............................] - ETA: 19:07 - loss: 0.9013 - regression_loss: 0.5755 - classification_loss: 0.3258
 117/3000 [>.............................] - ETA: 19:07 - loss: 0.9007 - regression_loss: 0.5742 - classification_loss: 0.3264
 118/3000 [>.............................] - ETA: 19:07 - loss: 0.9005 - regression_loss: 0.5745 - classification_loss: 0.3260
 119/3000 [>.............................] - ETA: 19:07 - loss: 0.8986 - regression_loss: 0.5729 - classification_loss: 0.3257
 120/3000 [>.............................] - ETA: 19:07 - loss: 0.9017 - regression_loss: 0.5761 - classification_loss: 0.3256
 121/3000 [>.............................] - ETA: 19:07 - loss: 0.9003 - regression_loss: 0.5745 - classification_loss: 0.3258
 122/3000 [>.............................] - ETA: 19:07 - loss: 0.8991 - regression_loss: 0.5750 - classification_loss: 0.3241
 123/3000 [>.............................] - ETA: 19:07 - loss: 0.8976 - regression_loss: 0.5741 - classification_loss: 0.3236
 124/3000 [>.............................] - ETA: 19:07 - loss: 0.8976 - regression_loss: 0.5740 - classification_loss: 0.3237
 125/3000 [>.............................] - ETA: 19:07 - loss: 0.8975 - regression_loss: 0.5747 - classification_loss: 0.3228
 126/3000 [>.............................] - ETA: 19:07 - loss: 0.8987 - regression_loss: 0.5759 - classification_loss: 0.3228
 127/3000 [>.............................] - ETA: 19:07 - loss: 0.8989 - regression_loss: 0.5768 - classification_loss: 0.3220
 128/3000 [>.............................] - ETA: 19:07 - loss: 0.8984 - regression_loss: 0.5761 - classification_loss: 0.3223
 129/3000 [>.............................] - ETA: 19:07 - loss: 0.8969 - regression_loss: 0.5744 - classification_loss: 0.3226
 130/3000 [>.............................] - ETA: 19:07 - loss: 0.8974 - regression_loss: 0.5743 - classification_loss: 0.3230
 131/3000 [>.............................] - ETA: 19:07 - loss: 0.8979 - regression_loss: 0.5754 - classification_loss: 0.3225
 132/3000 [>.............................] - ETA: 19:07 - loss: 0.8968 - regression_loss: 0.5743 - classification_loss: 0.3225
 133/3000 [>.............................] - ETA: 19:07 - loss: 0.8949 - regression_loss: 0.5731 - classification_loss: 0.3218
 134/3000 [>.............................] - ETA: 19:07 - loss: 0.8928 - regression_loss: 0.5712 - classification_loss: 0.3216
 135/3000 [>.............................] - ETA: 19:07 - loss: 0.8951 - regression_loss: 0.5734 - classification_loss: 0.3216
 136/3000 [>.............................] - ETA: 19:07 - loss: 0.8953 - regression_loss: 0.5742 - classification_loss: 0.3211
 137/3000 [>.............................] - ETA: 19:06 - loss: 0.8950 - regression_loss: 0.5742 - classification_loss: 0.3208
 138/3000 [>.............................] - ETA: 19:06 - loss: 0.8953 - regression_loss: 0.5750 - classification_loss: 0.3203
 139/3000 [>.............................] - ETA: 19:06 - loss: 0.8942 - regression_loss: 0.5744 - classification_loss: 0.3199
 140/3000 [>.............................] - ETA: 19:06 - loss: 0.8946 - regression_loss: 0.5754 - classification_loss: 0.3192
 141/3000 [>.............................] - ETA: 19:05 - loss: 0.8938 - regression_loss: 0.5748 - classification_loss: 0.3191
 142/3000 [>.............................] - ETA: 19:05 - loss: 0.8915 - regression_loss: 0.5736 - classification_loss: 0.3180
 143/3000 [>.............................] - ETA: 19:05 - loss: 0.8919 - regression_loss: 0.5733 - classification_loss: 0.3187
 144/3000 [>.............................] - ETA: 19:05 - loss: 0.8907 - regression_loss: 0.5719 - classification_loss: 0.3188
 145/3000 [>.............................] - ETA: 19:05 - loss: 0.8923 - regression_loss: 0.5736 - classification_loss: 0.3187
 146/3000 [>.............................] - ETA: 19:05 - loss: 0.8920 - regression_loss: 0.5733 - classification_loss: 0.3188
 147/3000 [>.............................] - ETA: 19:05 - loss: 0.8930 - regression_loss: 0.5749 - classification_loss: 0.3181
 148/3000 [>.............................] - ETA: 19:05 - loss: 0.8919 - regression_loss: 0.5737 - classification_loss: 0.3182
 149/3000 [>.............................] - ETA: 19:05 - loss: 0.8924 - regression_loss: 0.5740 - classification_loss: 0.3184
 150/3000 [>.............................] - ETA: 19:04 - loss: 0.8911 - regression_loss: 0.5726 - classification_loss: 0.3185
 151/3000 [>.............................] - ETA: 19:05 - loss: 0.8905 - regression_loss: 0.5719 - classification_loss: 0.3186
 152/3000 [>.............................] - ETA: 19:05 - loss: 0.8926 - regression_loss: 0.5737 - classification_loss: 0.3189
 153/3000 [>.............................] - ETA: 19:04 - loss: 0.8927 - regression_loss: 0.5735 - classification_loss: 0.3192
 154/3000 [>.............................] - ETA: 19:04 - loss: 0.8951 - regression_loss: 0.5749 - classification_loss: 0.3202
 155/3000 [>.............................] - ETA: 19:04 - loss: 0.8934 - regression_loss: 0.5735 - classification_loss: 0.3198
 156/3000 [>.............................] - ETA: 19:04 - loss: 0.8921 - regression_loss: 0.5721 - classification_loss: 0.3201
 157/3000 [>.............................] - ETA: 19:04 - loss: 0.8934 - regression_loss: 0.5738 - classification_loss: 0.3195
 158/3000 [>.............................] - ETA: 19:04 - loss: 0.8942 - regression_loss: 0.5751 - classification_loss: 0.3191
 159/3000 [>.............................] - ETA: 19:04 - loss: 0.8936 - regression_loss: 0.5753 - classification_loss: 0.3183
 160/3000 [>.............................] - ETA: 19:04 - loss: 0.8950 - regression_loss: 0.5762 - classification_loss: 0.3188
 161/3000 [>.............................] - ETA: 19:03 - loss: 0.8953 - regression_loss: 0.5762 - classification_loss: 0.3191
 162/3000 [>.............................] - ETA: 19:03 - loss: 0.8966 - regression_loss: 0.5778 - classification_loss: 0.3188
 163/3000 [>.............................] - ETA: 19:03 - loss: 0.8969 - regression_loss: 0.5785 - classification_loss: 0.3184
 164/3000 [>.............................] - ETA: 19:03 - loss: 0.8955 - regression_loss: 0.5778 - classification_loss: 0.3177
 165/3000 [>.............................] - ETA: 19:03 - loss: 0.8961 - regression_loss: 0.5783 - classification_loss: 0.3178
 166/3000 [>.............................] - ETA: 19:02 - loss: 0.8960 - regression_loss: 0.5780 - classification_loss: 0.3179
 167/3000 [>.............................] - ETA: 19:02 - loss: 0.8959 - regression_loss: 0.5777 - classification_loss: 0.3182
 168/3000 [>.............................] - ETA: 19:02 - loss: 0.8979 - regression_loss: 0.5796 - classification_loss: 0.3183
 169/3000 [>.............................] - ETA: 19:01 - loss: 0.8982 - regression_loss: 0.5806 - classification_loss: 0.3177
 170/3000 [>.............................] - ETA: 19:01 - loss: 0.8981 - regression_loss: 0.5803 - classification_loss: 0.3179
 171/3000 [>.............................] - ETA: 19:01 - loss: 0.8998 - regression_loss: 0.5812 - classification_loss: 0.3185
 172/3000 [>.............................] - ETA: 19:01 - loss: 0.9002 - regression_loss: 0.5814 - classification_loss: 0.3188
 173/3000 [>.............................] - ETA: 19:01 - loss: 0.9001 - regression_loss: 0.5814 - classification_loss: 0.3187
 174/3000 [>.............................] - ETA: 19:01 - loss: 0.8993 - regression_loss: 0.5808 - classification_loss: 0.3185
 175/3000 [>.............................] - ETA: 19:01 - loss: 0.9006 - regression_loss: 0.5819 - classification_loss: 0.3186
 176/3000 [>.............................] - ETA: 19:01 - loss: 0.8990 - regression_loss: 0.5804 - classification_loss: 0.3186
 177/3000 [>.............................] - ETA: 19:01 - loss: 0.8970 - regression_loss: 0.5786 - classification_loss: 0.3185
 178/3000 [>.............................] - ETA: 19:00 - loss: 0.8967 - regression_loss: 0.5787 - classification_loss: 0.3180
 179/3000 [>.............................] - ETA: 19:00 - loss: 0.8958 - regression_loss: 0.5777 - classification_loss: 0.3181
 180/3000 [>.............................] - ETA: 19:00 - loss: 0.8973 - regression_loss: 0.5789 - classification_loss: 0.3184
 181/3000 [>.............................] - ETA: 19:00 - loss: 0.8971 - regression_loss: 0.5782 - classification_loss: 0.3189
 182/3000 [>.............................] - ETA: 19:00 - loss: 0.8986 - regression_loss: 0.5796 - classification_loss: 0.3190
 183/3000 [>.............................] - ETA: 18:59 - loss: 0.8977 - regression_loss: 0.5789 - classification_loss: 0.3188
 184/3000 [>.............................] - ETA: 18:59 - loss: 0.8976 - regression_loss: 0.5790 - classification_loss: 0.3186
 185/3000 [>.............................] - ETA: 18:58 - loss: 0.8970 - regression_loss: 0.5786 - classification_loss: 0.3184
 186/3000 [>.............................] - ETA: 18:58 - loss: 0.8972 - regression_loss: 0.5798 - classification_loss: 0.3174
 187/3000 [>.............................] - ETA: 18:58 - loss: 0.8960 - regression_loss: 0.5785 - classification_loss: 0.3176
 188/3000 [>.............................] - ETA: 18:58 - loss: 0.8954 - regression_loss: 0.5778 - classification_loss: 0.3177
 189/3000 [>.............................] - ETA: 18:57 - loss: 0.8938 - regression_loss: 0.5765 - classification_loss: 0.3173
 190/3000 [>.............................] - ETA: 18:57 - loss: 0.8919 - regression_loss: 0.5752 - classification_loss: 0.3167
 191/3000 [>.............................] - ETA: 18:57 - loss: 0.8925 - regression_loss: 0.5758 - classification_loss: 0.3166
 192/3000 [>.............................] - ETA: 18:56 - loss: 0.8942 - regression_loss: 0.5777 - classification_loss: 0.3166
 193/3000 [>.............................] - ETA: 18:56 - loss: 0.8943 - regression_loss: 0.5776 - classification_loss: 0.3167
 194/3000 [>.............................] - ETA: 18:56 - loss: 0.8945 - regression_loss: 0.5779 - classification_loss: 0.3166
 195/3000 [>.............................] - ETA: 18:56 - loss: 0.8951 - regression_loss: 0.5788 - classification_loss: 0.3163
 196/3000 [>.............................] - ETA: 18:56 - loss: 0.8944 - regression_loss: 0.5782 - classification_loss: 0.3161
 197/3000 [>.............................] - ETA: 18:55 - loss: 0.8928 - regression_loss: 0.5768 - classification_loss: 0.3161
 198/3000 [>.............................] - ETA: 18:55 - loss: 0.8919 - regression_loss: 0.5757 - classification_loss: 0.3162
 199/3000 [>.............................] - ETA: 18:55 - loss: 0.8926 - regression_loss: 0.5758 - classification_loss: 0.3168
 200/3000 [=>............................] - ETA: 18:55 - loss: 0.8919 - regression_loss: 0.5753 - classification_loss: 0.3166
 201/3000 [=>............................] - ETA: 18:55 - loss: 0.8916 - regression_loss: 0.5743 - classification_loss: 0.3173
 202/3000 [=>............................] - ETA: 18:54 - loss: 0.8913 - regression_loss: 0.5740 - classification_loss: 0.3174
 203/3000 [=>............................] - ETA: 18:54 - loss: 0.8903 - regression_loss: 0.5732 - classification_loss: 0.3171
 204/3000 [=>............................] - ETA: 18:54 - loss: 0.8896 - regression_loss: 0.5724 - classification_loss: 0.3171
 205/3000 [=>............................] - ETA: 18:54 - loss: 0.8887 - regression_loss: 0.5719 - classification_loss: 0.3168
 206/3000 [=>............................] - ETA: 18:53 - loss: 0.8880 - regression_loss: 0.5713 - classification_loss: 0.3167
 207/3000 [=>............................] - ETA: 18:53 - loss: 0.8870 - regression_loss: 0.5701 - classification_loss: 0.3169
 208/3000 [=>............................] - ETA: 18:53 - loss: 0.8857 - regression_loss: 0.5694 - classification_loss: 0.3163
 209/3000 [=>............................] - ETA: 18:52 - loss: 0.8854 - regression_loss: 0.5693 - classification_loss: 0.3160
 210/3000 [=>............................] - ETA: 18:52 - loss: 0.8860 - regression_loss: 0.5696 - classification_loss: 0.3164
 211/3000 [=>............................] - ETA: 18:51 - loss: 0.8877 - regression_loss: 0.5713 - classification_loss: 0.3164
 212/3000 [=>............................] - ETA: 18:51 - loss: 0.8865 - regression_loss: 0.5696 - classification_loss: 0.3168
 213/3000 [=>............................] - ETA: 18:51 - loss: 0.8851 - regression_loss: 0.5686 - classification_loss: 0.3165
 214/3000 [=>............................] - ETA: 18:51 - loss: 0.8851 - regression_loss: 0.5683 - classification_loss: 0.3168
 215/3000 [=>............................] - ETA: 18:50 - loss: 0.8857 - regression_loss: 0.5691 - classification_loss: 0.3166
 216/3000 [=>............................] - ETA: 18:50 - loss: 0.8862 - regression_loss: 0.5694 - classification_loss: 0.3167
 217/3000 [=>............................] - ETA: 18:50 - loss: 0.8849 - regression_loss: 0.5691 - classification_loss: 0.3159
 218/3000 [=>............................] - ETA: 18:49 - loss: 0.8839 - regression_loss: 0.5680 - classification_loss: 0.3159
 219/3000 [=>............................] - ETA: 18:49 - loss: 0.8866 - regression_loss: 0.5705 - classification_loss: 0.3161
 220/3000 [=>............................] - ETA: 18:49 - loss: 0.8859 - regression_loss: 0.5696 - classification_loss: 0.3163
 221/3000 [=>............................] - ETA: 18:48 - loss: 0.8852 - regression_loss: 0.5688 - classification_loss: 0.3164
 222/3000 [=>............................] - ETA: 18:48 - loss: 0.8847 - regression_loss: 0.5683 - classification_loss: 0.3164
 223/3000 [=>............................] - ETA: 18:48 - loss: 0.8838 - regression_loss: 0.5675 - classification_loss: 0.3163
 224/3000 [=>............................] - ETA: 18:47 - loss: 0.8845 - regression_loss: 0.5678 - classification_loss: 0.3167
 225/3000 [=>............................] - ETA: 18:47 - loss: 0.8847 - regression_loss: 0.5677 - classification_loss: 0.3170
 226/3000 [=>............................] - ETA: 18:47 - loss: 0.8845 - regression_loss: 0.5674 - classification_loss: 0.3171
 227/3000 [=>............................] - ETA: 18:46 - loss: 0.8839 - regression_loss: 0.5667 - classification_loss: 0.3172
 228/3000 [=>............................] - ETA: 18:46 - loss: 0.8842 - regression_loss: 0.5669 - classification_loss: 0.3173
 229/3000 [=>............................] - ETA: 18:46 - loss: 0.8857 - regression_loss: 0.5682 - classification_loss: 0.3174
 230/3000 [=>............................] - ETA: 18:45 - loss: 0.8847 - regression_loss: 0.5672 - classification_loss: 0.3175
 231/3000 [=>............................] - ETA: 18:45 - loss: 0.8844 - regression_loss: 0.5669 - classification_loss: 0.3176
 232/3000 [=>............................] - ETA: 18:45 - loss: 0.8845 - regression_loss: 0.5671 - classification_loss: 0.3174
 233/3000 [=>............................] - ETA: 18:44 - loss: 0.8836 - regression_loss: 0.5666 - classification_loss: 0.3170
 234/3000 [=>............................] - ETA: 18:44 - loss: 0.8831 - regression_loss: 0.5662 - classification_loss: 0.3170
 235/3000 [=>............................] - ETA: 18:44 - loss: 0.8829 - regression_loss: 0.5657 - classification_loss: 0.3172
 236/3000 [=>............................] - ETA: 18:43 - loss: 0.8832 - regression_loss: 0.5659 - classification_loss: 0.3173
 237/3000 [=>............................] - ETA: 18:43 - loss: 0.8823 - regression_loss: 0.5651 - classification_loss: 0.3172
 238/3000 [=>............................] - ETA: 18:43 - loss: 0.8837 - regression_loss: 0.5662 - classification_loss: 0.3175
 239/3000 [=>............................] - ETA: 18:42 - loss: 0.8839 - regression_loss: 0.5661 - classification_loss: 0.3178
 240/3000 [=>............................] - ETA: 18:42 - loss: 0.8833 - regression_loss: 0.5653 - classification_loss: 0.3180
 241/3000 [=>............................] - ETA: 18:42 - loss: 0.8852 - regression_loss: 0.5667 - classification_loss: 0.3185
 242/3000 [=>............................] - ETA: 18:42 - loss: 0.8846 - regression_loss: 0.5660 - classification_loss: 0.3186
 243/3000 [=>............................] - ETA: 18:41 - loss: 0.8836 - regression_loss: 0.5649 - classification_loss: 0.3186
 244/3000 [=>............................] - ETA: 18:41 - loss: 0.8842 - regression_loss: 0.5653 - classification_loss: 0.3189
 245/3000 [=>............................] - ETA: 18:40 - loss: 0.8834 - regression_loss: 0.5646 - classification_loss: 0.3188
 246/3000 [=>............................] - ETA: 18:40 - loss: 0.8849 - regression_loss: 0.5663 - classification_loss: 0.3186
 247/3000 [=>............................] - ETA: 18:40 - loss: 0.8850 - regression_loss: 0.5664 - classification_loss: 0.3187
 248/3000 [=>............................] - ETA: 18:39 - loss: 0.8850 - regression_loss: 0.5671 - classification_loss: 0.3179
 249/3000 [=>............................] - ETA: 18:39 - loss: 0.8856 - regression_loss: 0.5676 - classification_loss: 0.3180
 250/3000 [=>............................] - ETA: 18:38 - loss: 0.8843 - regression_loss: 0.5667 - classification_loss: 0.3177
 251/3000 [=>............................] - ETA: 18:38 - loss: 0.8844 - regression_loss: 0.5668 - classification_loss: 0.3176
 252/3000 [=>............................] - ETA: 18:37 - loss: 0.8838 - regression_loss: 0.5663 - classification_loss: 0.3176
 253/3000 [=>............................] - ETA: 18:37 - loss: 0.8840 - regression_loss: 0.5666 - classification_loss: 0.3174
 254/3000 [=>............................] - ETA: 18:37 - loss: 0.8849 - regression_loss: 0.5675 - classification_loss: 0.3174
 255/3000 [=>............................] - ETA: 18:36 - loss: 0.8837 - regression_loss: 0.5665 - classification_loss: 0.3172
 256/3000 [=>............................] - ETA: 18:36 - loss: 0.8853 - regression_loss: 0.5678 - classification_loss: 0.3175
 257/3000 [=>............................] - ETA: 18:35 - loss: 0.8844 - regression_loss: 0.5669 - classification_loss: 0.3176
 258/3000 [=>............................] - ETA: 18:35 - loss: 0.8836 - regression_loss: 0.5658 - classification_loss: 0.3177
 259/3000 [=>............................] - ETA: 18:35 - loss: 0.8851 - regression_loss: 0.5675 - classification_loss: 0.3176
 260/3000 [=>............................] - ETA: 18:34 - loss: 0.8845 - regression_loss: 0.5668 - classification_loss: 0.3177
 261/3000 [=>............................] - ETA: 18:34 - loss: 0.8852 - regression_loss: 0.5672 - classification_loss: 0.3180
 262/3000 [=>............................] - ETA: 18:33 - loss: 0.8842 - regression_loss: 0.5664 - classification_loss: 0.3177
 263/3000 [=>............................] - ETA: 18:33 - loss: 0.8843 - regression_loss: 0.5665 - classification_loss: 0.3178
 264/3000 [=>............................] - ETA: 18:32 - loss: 0.8836 - regression_loss: 0.5655 - classification_loss: 0.3180
 265/3000 [=>............................] - ETA: 18:32 - loss: 0.8829 - regression_loss: 0.5650 - classification_loss: 0.3179
 266/3000 [=>............................] - ETA: 18:32 - loss: 0.8839 - regression_loss: 0.5656 - classification_loss: 0.3184
 267/3000 [=>............................] - ETA: 18:31 - loss: 0.8829 - regression_loss: 0.5646 - classification_loss: 0.3183
 268/3000 [=>............................] - ETA: 18:31 - loss: 0.8823 - regression_loss: 0.5639 - classification_loss: 0.3184
 269/3000 [=>............................] - ETA: 18:31 - loss: 0.8819 - regression_loss: 0.5635 - classification_loss: 0.3184
 270/3000 [=>............................] - ETA: 18:30 - loss: 0.8809 - regression_loss: 0.5625 - classification_loss: 0.3184
 271/3000 [=>............................] - ETA: 18:30 - loss: 0.8805 - regression_loss: 0.5622 - classification_loss: 0.3183
 272/3000 [=>............................] - ETA: 18:30 - loss: 0.8810 - regression_loss: 0.5627 - classification_loss: 0.3183
 273/3000 [=>............................] - ETA: 18:29 - loss: 0.8816 - regression_loss: 0.5633 - classification_loss: 0.3183
 274/3000 [=>............................] - ETA: 18:29 - loss: 0.8805 - regression_loss: 0.5623 - classification_loss: 0.3181
 275/3000 [=>............................] - ETA: 18:29 - loss: 0.8802 - regression_loss: 0.5619 - classification_loss: 0.3183
 276/3000 [=>............................] - ETA: 18:28 - loss: 0.8799 - regression_loss: 0.5612 - classification_loss: 0.3186
 277/3000 [=>............................] - ETA: 18:28 - loss: 0.8794 - regression_loss: 0.5608 - classification_loss: 0.3186
 278/3000 [=>............................] - ETA: 18:28 - loss: 0.8796 - regression_loss: 0.5609 - classification_loss: 0.3186
 279/3000 [=>............................] - ETA: 18:27 - loss: 0.8808 - regression_loss: 0.5623 - classification_loss: 0.3185
 280/3000 [=>............................] - ETA: 18:27 - loss: 0.8801 - regression_loss: 0.5616 - classification_loss: 0.3185
 281/3000 [=>............................] - ETA: 18:27 - loss: 0.8794 - regression_loss: 0.5610 - classification_loss: 0.3184
 282/3000 [=>............................] - ETA: 18:26 - loss: 0.8810 - regression_loss: 0.5627 - classification_loss: 0.3183
 283/3000 [=>............................] - ETA: 18:26 - loss: 0.8808 - regression_loss: 0.5627 - classification_loss: 0.3181
 284/3000 [=>............................] - ETA: 18:26 - loss: 0.8811 - regression_loss: 0.5629 - classification_loss: 0.3182
 285/3000 [=>............................] - ETA: 18:25 - loss: 0.8804 - regression_loss: 0.5624 - classification_loss: 0.3180
 286/3000 [=>............................] - ETA: 18:25 - loss: 0.8799 - regression_loss: 0.5623 - classification_loss: 0.3175
 287/3000 [=>............................] - ETA: 18:25 - loss: 0.8800 - regression_loss: 0.5628 - classification_loss: 0.3173
 288/3000 [=>............................] - ETA: 18:24 - loss: 0.8794 - regression_loss: 0.5626 - classification_loss: 0.3168
 289/3000 [=>............................] - ETA: 18:24 - loss: 0.8789 - regression_loss: 0.5621 - classification_loss: 0.3168
 290/3000 [=>............................] - ETA: 18:24 - loss: 0.8794 - regression_loss: 0.5623 - classification_loss: 0.3171
 291/3000 [=>............................] - ETA: 18:23 - loss: 0.8802 - regression_loss: 0.5631 - classification_loss: 0.3171
 292/3000 [=>............................] - ETA: 18:23 - loss: 0.8799 - regression_loss: 0.5628 - classification_loss: 0.3171
 293/3000 [=>............................] - ETA: 18:23 - loss: 0.8809 - regression_loss: 0.5638 - classification_loss: 0.3172
 294/3000 [=>............................] - ETA: 18:22 - loss: 0.8798 - regression_loss: 0.5629 - classification_loss: 0.3169
 295/3000 [=>............................] - ETA: 18:22 - loss: 0.8793 - regression_loss: 0.5624 - classification_loss: 0.3170
 296/3000 [=>............................] - ETA: 18:21 - loss: 0.8788 - regression_loss: 0.5618 - classification_loss: 0.3170
 297/3000 [=>............................] - ETA: 18:21 - loss: 0.8779 - regression_loss: 0.5608 - classification_loss: 0.3171
 298/3000 [=>............................] - ETA: 18:21 - loss: 0.8767 - regression_loss: 0.5599 - classification_loss: 0.3168
 299/3000 [=>............................] - ETA: 18:21 - loss: 0.8767 - regression_loss: 0.5599 - classification_loss: 0.3169
 300/3000 [==>...........................] - ETA: 18:20 - loss: 0.8758 - regression_loss: 0.5590 - classification_loss: 0.3168
 301/3000 [==>...........................] - ETA: 18:20 - loss: 0.8752 - regression_loss: 0.5583 - classification_loss: 0.3169
 302/3000 [==>...........................] - ETA: 18:20 - loss: 0.8744 - regression_loss: 0.5580 - classification_loss: 0.3164
 303/3000 [==>...........................] - ETA: 18:20 - loss: 0.8761 - regression_loss: 0.5596 - classification_loss: 0.3165
 304/3000 [==>...........................] - ETA: 18:19 - loss: 0.8762 - regression_loss: 0.5594 - classification_loss: 0.3168
 305/3000 [==>...........................] - ETA: 18:19 - loss: 0.8757 - regression_loss: 0.5590 - classification_loss: 0.3167
 306/3000 [==>...........................] - ETA: 18:18 - loss: 0.8757 - regression_loss: 0.5588 - classification_loss: 0.3169
 307/3000 [==>...........................] - ETA: 18:18 - loss: 0.8753 - regression_loss: 0.5581 - classification_loss: 0.3172
 308/3000 [==>...........................] - ETA: 18:17 - loss: 0.8757 - regression_loss: 0.5590 - classification_loss: 0.3167
 309/3000 [==>...........................] - ETA: 18:17 - loss: 0.8758 - regression_loss: 0.5592 - classification_loss: 0.3166
 310/3000 [==>...........................] - ETA: 18:17 - loss: 0.8766 - regression_loss: 0.5596 - classification_loss: 0.3170
 311/3000 [==>...........................] - ETA: 18:16 - loss: 0.8760 - regression_loss: 0.5589 - classification_loss: 0.3171
 312/3000 [==>...........................] - ETA: 18:16 - loss: 0.8764 - regression_loss: 0.5586 - classification_loss: 0.3178
 313/3000 [==>...........................] - ETA: 18:16 - loss: 0.8759 - regression_loss: 0.5579 - classification_loss: 0.3179
 314/3000 [==>...........................] - ETA: 18:15 - loss: 0.8748 - regression_loss: 0.5572 - classification_loss: 0.3177
 315/3000 [==>...........................] - ETA: 18:15 - loss: 0.8752 - regression_loss: 0.5578 - classification_loss: 0.3173
 316/3000 [==>...........................] - ETA: 18:15 - loss: 0.8745 - regression_loss: 0.5576 - classification_loss: 0.3169
 317/3000 [==>...........................] - ETA: 18:14 - loss: 0.8744 - regression_loss: 0.5574 - classification_loss: 0.3170
 318/3000 [==>...........................] - ETA: 18:14 - loss: 0.8738 - regression_loss: 0.5567 - classification_loss: 0.3170
 319/3000 [==>...........................] - ETA: 18:14 - loss: 0.8747 - regression_loss: 0.5577 - classification_loss: 0.3170
 320/3000 [==>...........................] - ETA: 18:13 - loss: 0.8738 - regression_loss: 0.5567 - classification_loss: 0.3171
 321/3000 [==>...........................] - ETA: 18:13 - loss: 0.8733 - regression_loss: 0.5560 - classification_loss: 0.3173
 322/3000 [==>...........................] - ETA: 18:12 - loss: 0.8730 - regression_loss: 0.5556 - classification_loss: 0.3174
 323/3000 [==>...........................] - ETA: 18:12 - loss: 0.8741 - regression_loss: 0.5566 - classification_loss: 0.3174
 324/3000 [==>...........................] - ETA: 18:11 - loss: 0.8738 - regression_loss: 0.5559 - classification_loss: 0.3179
 325/3000 [==>...........................] - ETA: 18:11 - loss: 0.8735 - regression_loss: 0.5554 - classification_loss: 0.3181
 326/3000 [==>...........................] - ETA: 18:10 - loss: 0.8725 - regression_loss: 0.5549 - classification_loss: 0.3176
 327/3000 [==>...........................] - ETA: 18:10 - loss: 0.8718 - regression_loss: 0.5543 - classification_loss: 0.3175
 328/3000 [==>...........................] - ETA: 18:10 - loss: 0.8722 - regression_loss: 0.5545 - classification_loss: 0.3178
 329/3000 [==>...........................] - ETA: 18:09 - loss: 0.8722 - regression_loss: 0.5547 - classification_loss: 0.3175
 330/3000 [==>...........................] - ETA: 18:09 - loss: 0.8729 - regression_loss: 0.5554 - classification_loss: 0.3175
 331/3000 [==>...........................] - ETA: 18:09 - loss: 0.8729 - regression_loss: 0.5555 - classification_loss: 0.3174
 332/3000 [==>...........................] - ETA: 18:08 - loss: 0.8724 - regression_loss: 0.5549 - classification_loss: 0.3175
 333/3000 [==>...........................] - ETA: 18:08 - loss: 0.8712 - regression_loss: 0.5541 - classification_loss: 0.3170
 334/3000 [==>...........................] - ETA: 18:07 - loss: 0.8708 - regression_loss: 0.5539 - classification_loss: 0.3169
 335/3000 [==>...........................] - ETA: 18:07 - loss: 0.8708 - regression_loss: 0.5541 - classification_loss: 0.3167
 336/3000 [==>...........................] - ETA: 18:06 - loss: 0.8711 - regression_loss: 0.5543 - classification_loss: 0.3168
 337/3000 [==>...........................] - ETA: 18:06 - loss: 0.8706 - regression_loss: 0.5539 - classification_loss: 0.3167
 338/3000 [==>...........................] - ETA: 18:06 - loss: 0.8716 - regression_loss: 0.5547 - classification_loss: 0.3169
 339/3000 [==>...........................] - ETA: 18:05 - loss: 0.8707 - regression_loss: 0.5539 - classification_loss: 0.3168
 340/3000 [==>...........................] - ETA: 18:05 - loss: 0.8698 - regression_loss: 0.5532 - classification_loss: 0.3166
 341/3000 [==>...........................] - ETA: 18:05 - loss: 0.8700 - regression_loss: 0.5532 - classification_loss: 0.3168
 342/3000 [==>...........................] - ETA: 18:04 - loss: 0.8693 - regression_loss: 0.5525 - classification_loss: 0.3168
 343/3000 [==>...........................] - ETA: 18:04 - loss: 0.8688 - regression_loss: 0.5519 - classification_loss: 0.3168
 344/3000 [==>...........................] - ETA: 18:03 - loss: 0.8690 - regression_loss: 0.5518 - classification_loss: 0.3172
 345/3000 [==>...........................] - ETA: 18:03 - loss: 0.8688 - regression_loss: 0.5513 - classification_loss: 0.3175
 346/3000 [==>...........................] - ETA: 18:03 - loss: 0.8690 - regression_loss: 0.5517 - classification_loss: 0.3173
 347/3000 [==>...........................] - ETA: 18:02 - loss: 0.8688 - regression_loss: 0.5515 - classification_loss: 0.3174
 348/3000 [==>...........................] - ETA: 18:02 - loss: 0.8677 - regression_loss: 0.5509 - classification_loss: 0.3168
 349/3000 [==>...........................] - ETA: 18:01 - loss: 0.8674 - regression_loss: 0.5506 - classification_loss: 0.3168
 350/3000 [==>...........................] - ETA: 18:01 - loss: 0.8671 - regression_loss: 0.5502 - classification_loss: 0.3169
 351/3000 [==>...........................] - ETA: 18:01 - loss: 0.8674 - regression_loss: 0.5507 - classification_loss: 0.3167
 352/3000 [==>...........................] - ETA: 18:00 - loss: 0.8665 - regression_loss: 0.5499 - classification_loss: 0.3167
 353/3000 [==>...........................] - ETA: 18:00 - loss: 0.8664 - regression_loss: 0.5500 - classification_loss: 0.3164
 354/3000 [==>...........................] - ETA: 17:59 - loss: 0.8659 - regression_loss: 0.5493 - classification_loss: 0.3166
 355/3000 [==>...........................] - ETA: 17:59 - loss: 0.8661 - regression_loss: 0.5493 - classification_loss: 0.3168
 356/3000 [==>...........................] - ETA: 17:59 - loss: 0.8659 - regression_loss: 0.5493 - classification_loss: 0.3167
 357/3000 [==>...........................] - ETA: 17:58 - loss: 0.8662 - regression_loss: 0.5497 - classification_loss: 0.3165
 358/3000 [==>...........................] - ETA: 17:58 - loss: 0.8661 - regression_loss: 0.5497 - classification_loss: 0.3163
 359/3000 [==>...........................] - ETA: 17:58 - loss: 0.8655 - regression_loss: 0.5492 - classification_loss: 0.3163
 360/3000 [==>...........................] - ETA: 17:57 - loss: 0.8649 - regression_loss: 0.5488 - classification_loss: 0.3161
 361/3000 [==>...........................] - ETA: 17:57 - loss: 0.8643 - regression_loss: 0.5485 - classification_loss: 0.3158
 362/3000 [==>...........................] - ETA: 17:57 - loss: 0.8647 - regression_loss: 0.5488 - classification_loss: 0.3158
 363/3000 [==>...........................] - ETA: 17:56 - loss: 0.8647 - regression_loss: 0.5487 - classification_loss: 0.3160
 364/3000 [==>...........................] - ETA: 17:56 - loss: 0.8642 - regression_loss: 0.5483 - classification_loss: 0.3159
 365/3000 [==>...........................] - ETA: 17:55 - loss: 0.8637 - regression_loss: 0.5479 - classification_loss: 0.3158
 366/3000 [==>...........................] - ETA: 17:55 - loss: 0.8650 - regression_loss: 0.5492 - classification_loss: 0.3158
 367/3000 [==>...........................] - ETA: 17:55 - loss: 0.8649 - regression_loss: 0.5490 - classification_loss: 0.3159
 368/3000 [==>...........................] - ETA: 17:54 - loss: 0.8637 - regression_loss: 0.5482 - classification_loss: 0.3155
 369/3000 [==>...........................] - ETA: 17:54 - loss: 0.8634 - regression_loss: 0.5479 - classification_loss: 0.3155
 370/3000 [==>...........................] - ETA: 17:54 - loss: 0.8633 - regression_loss: 0.5478 - classification_loss: 0.3155
 371/3000 [==>...........................] - ETA: 17:54 - loss: 0.8630 - regression_loss: 0.5473 - classification_loss: 0.3158
 372/3000 [==>...........................] - ETA: 17:53 - loss: 0.8625 - regression_loss: 0.5468 - classification_loss: 0.3157
 373/3000 [==>...........................] - ETA: 17:53 - loss: 0.8623 - regression_loss: 0.5468 - classification_loss: 0.3154
 374/3000 [==>...........................] - ETA: 17:52 - loss: 0.8621 - regression_loss: 0.5469 - classification_loss: 0.3152
 375/3000 [==>...........................] - ETA: 17:52 - loss: 0.8619 - regression_loss: 0.5469 - classification_loss: 0.3150
 376/3000 [==>...........................] - ETA: 17:52 - loss: 0.8621 - regression_loss: 0.5470 - classification_loss: 0.3151
 377/3000 [==>...........................] - ETA: 17:52 - loss: 0.8618 - regression_loss: 0.5465 - classification_loss: 0.3153
 378/3000 [==>...........................] - ETA: 17:51 - loss: 0.8618 - regression_loss: 0.5466 - classification_loss: 0.3151
 379/3000 [==>...........................] - ETA: 17:51 - loss: 0.8607 - regression_loss: 0.5457 - classification_loss: 0.3150
 380/3000 [==>...........................] - ETA: 17:51 - loss: 0.8601 - regression_loss: 0.5452 - classification_loss: 0.3150
 381/3000 [==>...........................] - ETA: 17:50 - loss: 0.8597 - regression_loss: 0.5449 - classification_loss: 0.3148
 382/3000 [==>...........................] - ETA: 17:50 - loss: 0.8596 - regression_loss: 0.5450 - classification_loss: 0.3146
 383/3000 [==>...........................] - ETA: 17:49 - loss: 0.8605 - regression_loss: 0.5457 - classification_loss: 0.3148
 384/3000 [==>...........................] - ETA: 17:49 - loss: 0.8601 - regression_loss: 0.5451 - classification_loss: 0.3149
 385/3000 [==>...........................] - ETA: 17:48 - loss: 0.8595 - regression_loss: 0.5445 - classification_loss: 0.3150
 386/3000 [==>...........................] - ETA: 17:48 - loss: 0.8584 - regression_loss: 0.5438 - classification_loss: 0.3146
 387/3000 [==>...........................] - ETA: 17:47 - loss: 0.8578 - regression_loss: 0.5432 - classification_loss: 0.3146
 388/3000 [==>...........................] - ETA: 17:47 - loss: 0.8575 - regression_loss: 0.5429 - classification_loss: 0.3146
 389/3000 [==>...........................] - ETA: 17:47 - loss: 0.8573 - regression_loss: 0.5430 - classification_loss: 0.3143
 390/3000 [==>...........................] - ETA: 17:46 - loss: 0.8568 - regression_loss: 0.5425 - classification_loss: 0.3143
 391/3000 [==>...........................] - ETA: 17:46 - loss: 0.8557 - regression_loss: 0.5417 - classification_loss: 0.3140
 392/3000 [==>...........................] - ETA: 17:45 - loss: 0.8553 - regression_loss: 0.5413 - classification_loss: 0.3140
 393/3000 [==>...........................] - ETA: 17:45 - loss: 0.8549 - regression_loss: 0.5408 - classification_loss: 0.3141
 394/3000 [==>...........................] - ETA: 17:45 - loss: 0.8546 - regression_loss: 0.5404 - classification_loss: 0.3142
 395/3000 [==>...........................] - ETA: 17:44 - loss: 0.8550 - regression_loss: 0.5409 - classification_loss: 0.3142
 396/3000 [==>...........................] - ETA: 17:44 - loss: 0.8542 - regression_loss: 0.5404 - classification_loss: 0.3138
 397/3000 [==>...........................] - ETA: 17:44 - loss: 0.8538 - regression_loss: 0.5401 - classification_loss: 0.3137
 398/3000 [==>...........................] - ETA: 17:43 - loss: 0.8535 - regression_loss: 0.5398 - classification_loss: 0.3136
 399/3000 [==>...........................] - ETA: 17:43 - loss: 0.8533 - regression_loss: 0.5400 - classification_loss: 0.3134
 400/3000 [===>..........................] - ETA: 17:42 - loss: 0.8528 - regression_loss: 0.5395 - classification_loss: 0.3134
 401/3000 [===>..........................] - ETA: 17:42 - loss: 0.8538 - regression_loss: 0.5404 - classification_loss: 0.3134
 402/3000 [===>..........................] - ETA: 17:42 - loss: 0.8532 - regression_loss: 0.5398 - classification_loss: 0.3134
 403/3000 [===>..........................] - ETA: 17:41 - loss: 0.8532 - regression_loss: 0.5398 - classification_loss: 0.3134
 404/3000 [===>..........................] - ETA: 17:41 - loss: 0.8527 - regression_loss: 0.5392 - classification_loss: 0.3135
 405/3000 [===>..........................] - ETA: 17:40 - loss: 0.8525 - regression_loss: 0.5390 - classification_loss: 0.3136
 406/3000 [===>..........................] - ETA: 17:40 - loss: 0.8518 - regression_loss: 0.5384 - classification_loss: 0.3134
 407/3000 [===>..........................] - ETA: 17:40 - loss: 0.8513 - regression_loss: 0.5380 - classification_loss: 0.3133
 408/3000 [===>..........................] - ETA: 17:39 - loss: 0.8513 - regression_loss: 0.5381 - classification_loss: 0.3132
 409/3000 [===>..........................] - ETA: 17:39 - loss: 0.8516 - regression_loss: 0.5385 - classification_loss: 0.3131
 410/3000 [===>..........................] - ETA: 17:39 - loss: 0.8514 - regression_loss: 0.5384 - classification_loss: 0.3130
 411/3000 [===>..........................] - ETA: 17:38 - loss: 0.8523 - regression_loss: 0.5392 - classification_loss: 0.3131
 412/3000 [===>..........................] - ETA: 17:38 - loss: 0.8519 - regression_loss: 0.5389 - classification_loss: 0.3130
 413/3000 [===>..........................] - ETA: 17:37 - loss: 0.8525 - regression_loss: 0.5393 - classification_loss: 0.3132
 414/3000 [===>..........................] - ETA: 17:37 - loss: 0.8535 - regression_loss: 0.5404 - classification_loss: 0.3131
 415/3000 [===>..........................] - ETA: 17:37 - loss: 0.8530 - regression_loss: 0.5404 - classification_loss: 0.3127
 416/3000 [===>..........................] - ETA: 17:36 - loss: 0.8529 - regression_loss: 0.5400 - classification_loss: 0.3129
 417/3000 [===>..........................] - ETA: 17:36 - loss: 0.8529 - regression_loss: 0.5398 - classification_loss: 0.3131
 418/3000 [===>..........................] - ETA: 17:35 - loss: 0.8532 - regression_loss: 0.5400 - classification_loss: 0.3132
 419/3000 [===>..........................] - ETA: 17:35 - loss: 0.8536 - regression_loss: 0.5402 - classification_loss: 0.3134
 420/3000 [===>..........................] - ETA: 17:34 - loss: 0.8543 - regression_loss: 0.5409 - classification_loss: 0.3135
 421/3000 [===>..........................] - ETA: 17:34 - loss: 0.8550 - regression_loss: 0.5415 - classification_loss: 0.3134
 422/3000 [===>..........................] - ETA: 17:33 - loss: 0.8552 - regression_loss: 0.5418 - classification_loss: 0.3134
 423/3000 [===>..........................] - ETA: 17:33 - loss: 0.8550 - regression_loss: 0.5417 - classification_loss: 0.3134
 424/3000 [===>..........................] - ETA: 17:33 - loss: 0.8547 - regression_loss: 0.5413 - classification_loss: 0.3135
 425/3000 [===>..........................] - ETA: 17:32 - loss: 0.8542 - regression_loss: 0.5407 - classification_loss: 0.3135
 426/3000 [===>..........................] - ETA: 17:32 - loss: 0.8542 - regression_loss: 0.5407 - classification_loss: 0.3135
 427/3000 [===>..........................] - ETA: 17:31 - loss: 0.8541 - regression_loss: 0.5408 - classification_loss: 0.3133
 428/3000 [===>..........................] - ETA: 17:31 - loss: 0.8544 - regression_loss: 0.5413 - classification_loss: 0.3131
 429/3000 [===>..........................] - ETA: 17:31 - loss: 0.8548 - regression_loss: 0.5414 - classification_loss: 0.3134
 430/3000 [===>..........................] - ETA: 17:30 - loss: 0.8549 - regression_loss: 0.5414 - classification_loss: 0.3135
 431/3000 [===>..........................] - ETA: 17:30 - loss: 0.8562 - regression_loss: 0.5426 - classification_loss: 0.3136
 432/3000 [===>..........................] - ETA: 17:30 - loss: 0.8558 - regression_loss: 0.5422 - classification_loss: 0.3135
 433/3000 [===>..........................] - ETA: 17:29 - loss: 0.8560 - regression_loss: 0.5423 - classification_loss: 0.3137
 434/3000 [===>..........................] - ETA: 17:29 - loss: 0.8557 - regression_loss: 0.5421 - classification_loss: 0.3136
 435/3000 [===>..........................] - ETA: 17:28 - loss: 0.8565 - regression_loss: 0.5428 - classification_loss: 0.3137
 436/3000 [===>..........................] - ETA: 17:28 - loss: 0.8561 - regression_loss: 0.5426 - classification_loss: 0.3135
 437/3000 [===>..........................] - ETA: 17:27 - loss: 0.8565 - regression_loss: 0.5431 - classification_loss: 0.3134
 438/3000 [===>..........................] - ETA: 17:27 - loss: 0.8567 - regression_loss: 0.5435 - classification_loss: 0.3132
 439/3000 [===>..........................] - ETA: 17:27 - loss: 0.8568 - regression_loss: 0.5438 - classification_loss: 0.3130
 440/3000 [===>..........................] - ETA: 17:26 - loss: 0.8567 - regression_loss: 0.5436 - classification_loss: 0.3131
 441/3000 [===>..........................] - ETA: 17:26 - loss: 0.8562 - regression_loss: 0.5432 - classification_loss: 0.3130
 442/3000 [===>..........................] - ETA: 17:25 - loss: 0.8561 - regression_loss: 0.5430 - classification_loss: 0.3131
 443/3000 [===>..........................] - ETA: 17:25 - loss: 0.8561 - regression_loss: 0.5430 - classification_loss: 0.3131
 444/3000 [===>..........................] - ETA: 17:25 - loss: 0.8563 - regression_loss: 0.5432 - classification_loss: 0.3132
 445/3000 [===>..........................] - ETA: 17:24 - loss: 0.8558 - regression_loss: 0.5427 - classification_loss: 0.3131
 446/3000 [===>..........................] - ETA: 17:24 - loss: 0.8558 - regression_loss: 0.5429 - classification_loss: 0.3129
 447/3000 [===>..........................] - ETA: 17:23 - loss: 0.8555 - regression_loss: 0.5427 - classification_loss: 0.3128
 448/3000 [===>..........................] - ETA: 17:23 - loss: 0.8555 - regression_loss: 0.5425 - classification_loss: 0.3130
 449/3000 [===>..........................] - ETA: 17:23 - loss: 0.8554 - regression_loss: 0.5424 - classification_loss: 0.3130
 450/3000 [===>..........................] - ETA: 17:22 - loss: 0.8547 - regression_loss: 0.5419 - classification_loss: 0.3128
 451/3000 [===>..........................] - ETA: 17:22 - loss: 0.8545 - regression_loss: 0.5416 - classification_loss: 0.3128
 452/3000 [===>..........................] - ETA: 17:22 - loss: 0.8540 - regression_loss: 0.5414 - classification_loss: 0.3125
 453/3000 [===>..........................] - ETA: 17:21 - loss: 0.8546 - regression_loss: 0.5422 - classification_loss: 0.3124
 454/3000 [===>..........................] - ETA: 17:21 - loss: 0.8541 - regression_loss: 0.5420 - classification_loss: 0.3122
 455/3000 [===>..........................] - ETA: 17:20 - loss: 0.8536 - regression_loss: 0.5415 - classification_loss: 0.3121
 456/3000 [===>..........................] - ETA: 17:20 - loss: 0.8536 - regression_loss: 0.5416 - classification_loss: 0.3120
 457/3000 [===>..........................] - ETA: 17:19 - loss: 0.8533 - regression_loss: 0.5412 - classification_loss: 0.3121
 458/3000 [===>..........................] - ETA: 17:19 - loss: 0.8532 - regression_loss: 0.5410 - classification_loss: 0.3122
 459/3000 [===>..........................] - ETA: 17:19 - loss: 0.8532 - regression_loss: 0.5407 - classification_loss: 0.3125
 460/3000 [===>..........................] - ETA: 17:18 - loss: 0.8536 - regression_loss: 0.5411 - classification_loss: 0.3125
 461/3000 [===>..........................] - ETA: 17:18 - loss: 0.8533 - regression_loss: 0.5409 - classification_loss: 0.3124
 462/3000 [===>..........................] - ETA: 17:17 - loss: 0.8533 - regression_loss: 0.5411 - classification_loss: 0.3122
 463/3000 [===>..........................] - ETA: 17:17 - loss: 0.8531 - regression_loss: 0.5409 - classification_loss: 0.3122
 464/3000 [===>..........................] - ETA: 17:17 - loss: 0.8533 - regression_loss: 0.5412 - classification_loss: 0.3121
 465/3000 [===>..........................] - ETA: 17:16 - loss: 0.8530 - regression_loss: 0.5409 - classification_loss: 0.3121
 466/3000 [===>..........................] - ETA: 17:16 - loss: 0.8526 - regression_loss: 0.5405 - classification_loss: 0.3122
 467/3000 [===>..........................] - ETA: 17:15 - loss: 0.8522 - regression_loss: 0.5400 - classification_loss: 0.3122
 468/3000 [===>..........................] - ETA: 17:15 - loss: 0.8520 - regression_loss: 0.5401 - classification_loss: 0.3119
 469/3000 [===>..........................] - ETA: 17:15 - loss: 0.8516 - regression_loss: 0.5397 - classification_loss: 0.3119
 470/3000 [===>..........................] - ETA: 17:14 - loss: 0.8509 - regression_loss: 0.5392 - classification_loss: 0.3117
 471/3000 [===>..........................] - ETA: 17:14 - loss: 0.8507 - regression_loss: 0.5389 - classification_loss: 0.3117
 472/3000 [===>..........................] - ETA: 17:13 - loss: 0.8500 - regression_loss: 0.5385 - classification_loss: 0.3115
 473/3000 [===>..........................] - ETA: 17:13 - loss: 0.8493 - regression_loss: 0.5380 - classification_loss: 0.3114
 474/3000 [===>..........................] - ETA: 17:13 - loss: 0.8489 - regression_loss: 0.5377 - classification_loss: 0.3112
 475/3000 [===>..........................] - ETA: 17:12 - loss: 0.8488 - regression_loss: 0.5375 - classification_loss: 0.3113
 476/3000 [===>..........................] - ETA: 17:12 - loss: 0.8488 - regression_loss: 0.5374 - classification_loss: 0.3114
 477/3000 [===>..........................] - ETA: 17:12 - loss: 0.8490 - regression_loss: 0.5378 - classification_loss: 0.3113
 478/3000 [===>..........................] - ETA: 17:11 - loss: 0.8487 - regression_loss: 0.5374 - classification_loss: 0.3113
 479/3000 [===>..........................] - ETA: 17:11 - loss: 0.8483 - regression_loss: 0.5373 - classification_loss: 0.3110
 480/3000 [===>..........................] - ETA: 17:11 - loss: 0.8478 - regression_loss: 0.5368 - classification_loss: 0.3110
 481/3000 [===>..........................] - ETA: 17:10 - loss: 0.8474 - regression_loss: 0.5364 - classification_loss: 0.3110
 482/3000 [===>..........................] - ETA: 17:10 - loss: 0.8474 - regression_loss: 0.5363 - classification_loss: 0.3111
 483/3000 [===>..........................] - ETA: 17:09 - loss: 0.8469 - regression_loss: 0.5362 - classification_loss: 0.3106
 484/3000 [===>..........................] - ETA: 17:09 - loss: 0.8465 - regression_loss: 0.5359 - classification_loss: 0.3106
 485/3000 [===>..........................] - ETA: 17:09 - loss: 0.8465 - regression_loss: 0.5360 - classification_loss: 0.3104
 486/3000 [===>..........................] - ETA: 17:08 - loss: 0.8460 - regression_loss: 0.5357 - classification_loss: 0.3103
 487/3000 [===>..........................] - ETA: 17:08 - loss: 0.8455 - regression_loss: 0.5351 - classification_loss: 0.3104
 488/3000 [===>..........................] - ETA: 17:08 - loss: 0.8461 - regression_loss: 0.5357 - classification_loss: 0.3104
 489/3000 [===>..........................] - ETA: 17:07 - loss: 0.8463 - regression_loss: 0.5360 - classification_loss: 0.3103
 490/3000 [===>..........................] - ETA: 17:07 - loss: 0.8459 - regression_loss: 0.5356 - classification_loss: 0.3104
 491/3000 [===>..........................] - ETA: 17:06 - loss: 0.8455 - regression_loss: 0.5350 - classification_loss: 0.3105
 492/3000 [===>..........................] - ETA: 17:06 - loss: 0.8456 - regression_loss: 0.5351 - classification_loss: 0.3105
 493/3000 [===>..........................] - ETA: 17:06 - loss: 0.8460 - regression_loss: 0.5355 - classification_loss: 0.3105
 494/3000 [===>..........................] - ETA: 17:06 - loss: 0.8453 - regression_loss: 0.5348 - classification_loss: 0.3105
 495/3000 [===>..........................] - ETA: 17:05 - loss: 0.8447 - regression_loss: 0.5344 - classification_loss: 0.3103
 496/3000 [===>..........................] - ETA: 17:05 - loss: 0.8449 - regression_loss: 0.5347 - classification_loss: 0.3102
 497/3000 [===>..........................] - ETA: 17:04 - loss: 0.8447 - regression_loss: 0.5345 - classification_loss: 0.3102
 498/3000 [===>..........................] - ETA: 17:04 - loss: 0.8445 - regression_loss: 0.5343 - classification_loss: 0.3102
 499/3000 [===>..........................] - ETA: 17:04 - loss: 0.8442 - regression_loss: 0.5340 - classification_loss: 0.3102
 500/3000 [====>.........................] - ETA: 17:03 - loss: 0.8443 - regression_loss: 0.5342 - classification_loss: 0.3101
 501/3000 [====>.........................] - ETA: 17:03 - loss: 0.8438 - regression_loss: 0.5338 - classification_loss: 0.3100
 502/3000 [====>.........................] - ETA: 17:02 - loss: 0.8435 - regression_loss: 0.5335 - classification_loss: 0.3100
 503/3000 [====>.........................] - ETA: 17:02 - loss: 0.8439 - regression_loss: 0.5339 - classification_loss: 0.3101
 504/3000 [====>.........................] - ETA: 17:02 - loss: 0.8441 - regression_loss: 0.5342 - classification_loss: 0.3099
 505/3000 [====>.........................] - ETA: 17:01 - loss: 0.8446 - regression_loss: 0.5346 - classification_loss: 0.3100
 506/3000 [====>.........................] - ETA: 17:01 - loss: 0.8444 - regression_loss: 0.5343 - classification_loss: 0.3101
 507/3000 [====>.........................] - ETA: 17:00 - loss: 0.8439 - regression_loss: 0.5339 - classification_loss: 0.3101
 508/3000 [====>.........................] - ETA: 17:00 - loss: 0.8445 - regression_loss: 0.5345 - classification_loss: 0.3101
 509/3000 [====>.........................] - ETA: 17:00 - loss: 0.8450 - regression_loss: 0.5349 - classification_loss: 0.3101
 510/3000 [====>.........................] - ETA: 16:59 - loss: 0.8446 - regression_loss: 0.5344 - classification_loss: 0.3102
 511/3000 [====>.........................] - ETA: 16:59 - loss: 0.8440 - regression_loss: 0.5339 - classification_loss: 0.3101
 512/3000 [====>.........................] - ETA: 16:58 - loss: 0.8437 - regression_loss: 0.5338 - classification_loss: 0.3099
 513/3000 [====>.........................] - ETA: 16:58 - loss: 0.8439 - regression_loss: 0.5341 - classification_loss: 0.3097
 514/3000 [====>.........................] - ETA: 16:58 - loss: 0.8436 - regression_loss: 0.5337 - classification_loss: 0.3099
 515/3000 [====>.........................] - ETA: 16:57 - loss: 0.8433 - regression_loss: 0.5337 - classification_loss: 0.3096
 516/3000 [====>.........................] - ETA: 16:57 - loss: 0.8432 - regression_loss: 0.5337 - classification_loss: 0.3095
 517/3000 [====>.........................] - ETA: 16:56 - loss: 0.8431 - regression_loss: 0.5336 - classification_loss: 0.3094
 518/3000 [====>.........................] - ETA: 16:56 - loss: 0.8432 - regression_loss: 0.5337 - classification_loss: 0.3094
 519/3000 [====>.........................] - ETA: 16:55 - loss: 0.8426 - regression_loss: 0.5332 - classification_loss: 0.3094
 520/3000 [====>.........................] - ETA: 16:55 - loss: 0.8425 - regression_loss: 0.5331 - classification_loss: 0.3094
 521/3000 [====>.........................] - ETA: 16:55 - loss: 0.8421 - regression_loss: 0.5327 - classification_loss: 0.3094
 522/3000 [====>.........................] - ETA: 16:54 - loss: 0.8421 - regression_loss: 0.5327 - classification_loss: 0.3095
 523/3000 [====>.........................] - ETA: 16:54 - loss: 0.8418 - regression_loss: 0.5323 - classification_loss: 0.3095
 524/3000 [====>.........................] - ETA: 16:53 - loss: 0.8411 - regression_loss: 0.5318 - classification_loss: 0.3093
 525/3000 [====>.........................] - ETA: 16:53 - loss: 0.8406 - regression_loss: 0.5313 - classification_loss: 0.3093
 526/3000 [====>.........................] - ETA: 16:53 - loss: 0.8404 - regression_loss: 0.5311 - classification_loss: 0.3092
 527/3000 [====>.........................] - ETA: 16:52 - loss: 0.8399 - regression_loss: 0.5310 - classification_loss: 0.3088
 528/3000 [====>.........................] - ETA: 16:52 - loss: 0.8397 - regression_loss: 0.5309 - classification_loss: 0.3088
 529/3000 [====>.........................] - ETA: 16:52 - loss: 0.8395 - regression_loss: 0.5308 - classification_loss: 0.3087
 530/3000 [====>.........................] - ETA: 16:51 - loss: 0.8393 - regression_loss: 0.5307 - classification_loss: 0.3086
 531/3000 [====>.........................] - ETA: 16:51 - loss: 0.8391 - regression_loss: 0.5306 - classification_loss: 0.3084
 532/3000 [====>.........................] - ETA: 16:50 - loss: 0.8389 - regression_loss: 0.5306 - classification_loss: 0.3083
 533/3000 [====>.........................] - ETA: 16:50 - loss: 0.8383 - regression_loss: 0.5302 - classification_loss: 0.3081
 534/3000 [====>.........................] - ETA: 16:50 - loss: 0.8379 - regression_loss: 0.5299 - classification_loss: 0.3080
 535/3000 [====>.........................] - ETA: 16:49 - loss: 0.8379 - regression_loss: 0.5297 - classification_loss: 0.3082
 536/3000 [====>.........................] - ETA: 16:49 - loss: 0.8377 - regression_loss: 0.5296 - classification_loss: 0.3082
 537/3000 [====>.........................] - ETA: 16:49 - loss: 0.8374 - regression_loss: 0.5292 - classification_loss: 0.3082
 538/3000 [====>.........................] - ETA: 16:48 - loss: 0.8389 - regression_loss: 0.5307 - classification_loss: 0.3082
 539/3000 [====>.........................] - ETA: 16:48 - loss: 0.8387 - regression_loss: 0.5306 - classification_loss: 0.3080
 540/3000 [====>.........................] - ETA: 16:47 - loss: 0.8399 - regression_loss: 0.5318 - classification_loss: 0.3081
 541/3000 [====>.........................] - ETA: 16:47 - loss: 0.8405 - regression_loss: 0.5322 - classification_loss: 0.3083
 542/3000 [====>.........................] - ETA: 16:46 - loss: 0.8407 - regression_loss: 0.5325 - classification_loss: 0.3082
 543/3000 [====>.........................] - ETA: 16:46 - loss: 0.8405 - regression_loss: 0.5324 - classification_loss: 0.3081
 544/3000 [====>.........................] - ETA: 16:46 - loss: 0.8399 - regression_loss: 0.5320 - classification_loss: 0.3079
 545/3000 [====>.........................] - ETA: 16:45 - loss: 0.8397 - regression_loss: 0.5316 - classification_loss: 0.3081
 546/3000 [====>.........................] - ETA: 16:45 - loss: 0.8393 - regression_loss: 0.5312 - classification_loss: 0.3082
 547/3000 [====>.........................] - ETA: 16:44 - loss: 0.8401 - regression_loss: 0.5318 - classification_loss: 0.3083
 548/3000 [====>.........................] - ETA: 16:44 - loss: 0.8404 - regression_loss: 0.5322 - classification_loss: 0.3081
 549/3000 [====>.........................] - ETA: 16:44 - loss: 0.8406 - regression_loss: 0.5323 - classification_loss: 0.3083
 550/3000 [====>.........................] - ETA: 16:43 - loss: 0.8408 - regression_loss: 0.5324 - classification_loss: 0.3084
 551/3000 [====>.........................] - ETA: 16:43 - loss: 0.8409 - regression_loss: 0.5324 - classification_loss: 0.3085
 552/3000 [====>.........................] - ETA: 16:42 - loss: 0.8406 - regression_loss: 0.5320 - classification_loss: 0.3087
 553/3000 [====>.........................] - ETA: 16:42 - loss: 0.8409 - regression_loss: 0.5322 - classification_loss: 0.3086
 554/3000 [====>.........................] - ETA: 16:42 - loss: 0.8404 - regression_loss: 0.5318 - classification_loss: 0.3086
 555/3000 [====>.........................] - ETA: 16:41 - loss: 0.8401 - regression_loss: 0.5315 - classification_loss: 0.3086
 556/3000 [====>.........................] - ETA: 16:41 - loss: 0.8400 - regression_loss: 0.5313 - classification_loss: 0.3087
 557/3000 [====>.........................] - ETA: 16:40 - loss: 0.8402 - regression_loss: 0.5315 - classification_loss: 0.3087
 558/3000 [====>.........................] - ETA: 16:40 - loss: 0.8406 - regression_loss: 0.5319 - classification_loss: 0.3087
 559/3000 [====>.........................] - ETA: 16:39 - loss: 0.8401 - regression_loss: 0.5318 - classification_loss: 0.3084
 560/3000 [====>.........................] - ETA: 16:39 - loss: 0.8394 - regression_loss: 0.5312 - classification_loss: 0.3082
 561/3000 [====>.........................] - ETA: 16:39 - loss: 0.8394 - regression_loss: 0.5311 - classification_loss: 0.3083
 562/3000 [====>.........................] - ETA: 16:38 - loss: 0.8395 - regression_loss: 0.5312 - classification_loss: 0.3084
 563/3000 [====>.........................] - ETA: 16:38 - loss: 0.8393 - regression_loss: 0.5309 - classification_loss: 0.3084
 564/3000 [====>.........................] - ETA: 16:38 - loss: 0.8393 - regression_loss: 0.5309 - classification_loss: 0.3084
 565/3000 [====>.........................] - ETA: 16:37 - loss: 0.8398 - regression_loss: 0.5315 - classification_loss: 0.3084
 566/3000 [====>.........................] - ETA: 16:37 - loss: 0.8396 - regression_loss: 0.5312 - classification_loss: 0.3085
 567/3000 [====>.........................] - ETA: 16:36 - loss: 0.8395 - regression_loss: 0.5310 - classification_loss: 0.3086
 568/3000 [====>.........................] - ETA: 16:36 - loss: 0.8395 - regression_loss: 0.5308 - classification_loss: 0.3087
 569/3000 [====>.........................] - ETA: 16:36 - loss: 0.8392 - regression_loss: 0.5306 - classification_loss: 0.3086
 570/3000 [====>.........................] - ETA: 16:35 - loss: 0.8392 - regression_loss: 0.5305 - classification_loss: 0.3087
 571/3000 [====>.........................] - ETA: 16:35 - loss: 0.8395 - regression_loss: 0.5309 - classification_loss: 0.3086
 572/3000 [====>.........................] - ETA: 16:34 - loss: 0.8396 - regression_loss: 0.5311 - classification_loss: 0.3084
 573/3000 [====>.........................] - ETA: 16:34 - loss: 0.8391 - regression_loss: 0.5307 - classification_loss: 0.3084
 574/3000 [====>.........................] - ETA: 16:33 - loss: 0.8393 - regression_loss: 0.5309 - classification_loss: 0.3084
 575/3000 [====>.........................] - ETA: 16:33 - loss: 0.8395 - regression_loss: 0.5311 - classification_loss: 0.3083
 576/3000 [====>.........................] - ETA: 16:33 - loss: 0.8394 - regression_loss: 0.5310 - classification_loss: 0.3085
 577/3000 [====>.........................] - ETA: 16:32 - loss: 0.8392 - regression_loss: 0.5308 - classification_loss: 0.3084
 578/3000 [====>.........................] - ETA: 16:32 - loss: 0.8389 - regression_loss: 0.5305 - classification_loss: 0.3084
 579/3000 [====>.........................] - ETA: 16:32 - loss: 0.8386 - regression_loss: 0.5303 - classification_loss: 0.3084
 580/3000 [====>.........................] - ETA: 16:31 - loss: 0.8390 - regression_loss: 0.5308 - classification_loss: 0.3083
 581/3000 [====>.........................] - ETA: 16:31 - loss: 0.8394 - regression_loss: 0.5312 - classification_loss: 0.3082
 582/3000 [====>.........................] - ETA: 16:31 - loss: 0.8387 - regression_loss: 0.5307 - classification_loss: 0.3081
 583/3000 [====>.........................] - ETA: 16:30 - loss: 0.8385 - regression_loss: 0.5305 - classification_loss: 0.3080
 584/3000 [====>.........................] - ETA: 16:30 - loss: 0.8388 - regression_loss: 0.5307 - classification_loss: 0.3081
 585/3000 [====>.........................] - ETA: 16:29 - loss: 0.8388 - regression_loss: 0.5307 - classification_loss: 0.3080
 586/3000 [====>.........................] - ETA: 16:29 - loss: 0.8384 - regression_loss: 0.5306 - classification_loss: 0.3079
 587/3000 [====>.........................] - ETA: 16:29 - loss: 0.8381 - regression_loss: 0.5303 - classification_loss: 0.3078
 588/3000 [====>.........................] - ETA: 16:28 - loss: 0.8382 - regression_loss: 0.5305 - classification_loss: 0.3077
 589/3000 [====>.........................] - ETA: 16:28 - loss: 0.8381 - regression_loss: 0.5303 - classification_loss: 0.3077
 590/3000 [====>.........................] - ETA: 16:27 - loss: 0.8376 - regression_loss: 0.5302 - classification_loss: 0.3074
 591/3000 [====>.........................] - ETA: 16:27 - loss: 0.8373 - regression_loss: 0.5301 - classification_loss: 0.3072
 592/3000 [====>.........................] - ETA: 16:27 - loss: 0.8373 - regression_loss: 0.5301 - classification_loss: 0.3072
 593/3000 [====>.........................] - ETA: 16:26 - loss: 0.8375 - regression_loss: 0.5303 - classification_loss: 0.3072
 594/3000 [====>.........................] - ETA: 16:26 - loss: 0.8375 - regression_loss: 0.5305 - classification_loss: 0.3070
 595/3000 [====>.........................] - ETA: 16:25 - loss: 0.8372 - regression_loss: 0.5303 - classification_loss: 0.3070
 596/3000 [====>.........................] - ETA: 16:25 - loss: 0.8376 - regression_loss: 0.5305 - classification_loss: 0.3070
 597/3000 [====>.........................] - ETA: 16:24 - loss: 0.8370 - regression_loss: 0.5302 - classification_loss: 0.3068
 598/3000 [====>.........................] - ETA: 16:24 - loss: 0.8367 - regression_loss: 0.5300 - classification_loss: 0.3067
 599/3000 [====>.........................] - ETA: 16:24 - loss: 0.8366 - regression_loss: 0.5298 - classification_loss: 0.3068
 600/3000 [=====>........................] - ETA: 16:23 - loss: 0.8368 - regression_loss: 0.5301 - classification_loss: 0.3068
 601/3000 [=====>........................] - ETA: 16:23 - loss: 0.8366 - regression_loss: 0.5299 - classification_loss: 0.3067
 602/3000 [=====>........................] - ETA: 16:22 - loss: 0.8365 - regression_loss: 0.5297 - classification_loss: 0.3068
 603/3000 [=====>........................] - ETA: 16:22 - loss: 0.8363 - regression_loss: 0.5295 - classification_loss: 0.3067
 604/3000 [=====>........................] - ETA: 16:22 - loss: 0.8360 - regression_loss: 0.5295 - classification_loss: 0.3065
 605/3000 [=====>........................] - ETA: 16:21 - loss: 0.8355 - regression_loss: 0.5291 - classification_loss: 0.3064
 606/3000 [=====>........................] - ETA: 16:21 - loss: 0.8349 - regression_loss: 0.5285 - classification_loss: 0.3063
 607/3000 [=====>........................] - ETA: 16:21 - loss: 0.8351 - regression_loss: 0.5286 - classification_loss: 0.3065
 608/3000 [=====>........................] - ETA: 16:20 - loss: 0.8352 - regression_loss: 0.5287 - classification_loss: 0.3065
 609/3000 [=====>........................] - ETA: 16:20 - loss: 0.8347 - regression_loss: 0.5284 - classification_loss: 0.3064
 610/3000 [=====>........................] - ETA: 16:19 - loss: 0.8351 - regression_loss: 0.5288 - classification_loss: 0.3063
 611/3000 [=====>........................] - ETA: 16:19 - loss: 0.8352 - regression_loss: 0.5290 - classification_loss: 0.3062
 612/3000 [=====>........................] - ETA: 16:19 - loss: 0.8348 - regression_loss: 0.5285 - classification_loss: 0.3063
 613/3000 [=====>........................] - ETA: 16:18 - loss: 0.8350 - regression_loss: 0.5288 - classification_loss: 0.3063
 614/3000 [=====>........................] - ETA: 16:18 - loss: 0.8351 - regression_loss: 0.5290 - classification_loss: 0.3061
 615/3000 [=====>........................] - ETA: 16:17 - loss: 0.8350 - regression_loss: 0.5287 - classification_loss: 0.3062
 616/3000 [=====>........................] - ETA: 16:17 - loss: 0.8348 - regression_loss: 0.5287 - classification_loss: 0.3062
 617/3000 [=====>........................] - ETA: 16:16 - loss: 0.8346 - regression_loss: 0.5283 - classification_loss: 0.3063
 618/3000 [=====>........................] - ETA: 16:16 - loss: 0.8340 - regression_loss: 0.5278 - classification_loss: 0.3062
 619/3000 [=====>........................] - ETA: 16:16 - loss: 0.8338 - regression_loss: 0.5276 - classification_loss: 0.3062
 620/3000 [=====>........................] - ETA: 16:15 - loss: 0.8336 - regression_loss: 0.5275 - classification_loss: 0.3061
 621/3000 [=====>........................] - ETA: 16:15 - loss: 0.8335 - regression_loss: 0.5276 - classification_loss: 0.3059
 622/3000 [=====>........................] - ETA: 16:14 - loss: 0.8333 - regression_loss: 0.5273 - classification_loss: 0.3059
 623/3000 [=====>........................] - ETA: 16:14 - loss: 0.8327 - regression_loss: 0.5271 - classification_loss: 0.3057
 624/3000 [=====>........................] - ETA: 16:14 - loss: 0.8327 - regression_loss: 0.5272 - classification_loss: 0.3056
 625/3000 [=====>........................] - ETA: 16:13 - loss: 0.8323 - regression_loss: 0.5268 - classification_loss: 0.3055
 626/3000 [=====>........................] - ETA: 16:13 - loss: 0.8323 - regression_loss: 0.5270 - classification_loss: 0.3053
 627/3000 [=====>........................] - ETA: 16:12 - loss: 0.8325 - regression_loss: 0.5272 - classification_loss: 0.3053
 628/3000 [=====>........................] - ETA: 16:12 - loss: 0.8326 - regression_loss: 0.5272 - classification_loss: 0.3054
 629/3000 [=====>........................] - ETA: 16:12 - loss: 0.8323 - regression_loss: 0.5271 - classification_loss: 0.3052
 630/3000 [=====>........................] - ETA: 16:11 - loss: 0.8322 - regression_loss: 0.5269 - classification_loss: 0.3053
 631/3000 [=====>........................] - ETA: 16:11 - loss: 0.8319 - regression_loss: 0.5265 - classification_loss: 0.3054
 632/3000 [=====>........................] - ETA: 16:11 - loss: 0.8314 - regression_loss: 0.5261 - classification_loss: 0.3053
 633/3000 [=====>........................] - ETA: 16:10 - loss: 0.8314 - regression_loss: 0.5261 - classification_loss: 0.3053
 634/3000 [=====>........................] - ETA: 16:10 - loss: 0.8316 - regression_loss: 0.5264 - classification_loss: 0.3052
 635/3000 [=====>........................] - ETA: 16:09 - loss: 0.8315 - regression_loss: 0.5264 - classification_loss: 0.3051
 636/3000 [=====>........................] - ETA: 16:09 - loss: 0.8317 - regression_loss: 0.5265 - classification_loss: 0.3052
 637/3000 [=====>........................] - ETA: 16:09 - loss: 0.8316 - regression_loss: 0.5265 - classification_loss: 0.3051
 638/3000 [=====>........................] - ETA: 16:08 - loss: 0.8320 - regression_loss: 0.5267 - classification_loss: 0.3052
 639/3000 [=====>........................] - ETA: 16:08 - loss: 0.8322 - regression_loss: 0.5269 - classification_loss: 0.3053
 640/3000 [=====>........................] - ETA: 16:07 - loss: 0.8320 - regression_loss: 0.5267 - classification_loss: 0.3053
 641/3000 [=====>........................] - ETA: 16:07 - loss: 0.8321 - regression_loss: 0.5269 - classification_loss: 0.3052
 642/3000 [=====>........................] - ETA: 16:06 - loss: 0.8316 - regression_loss: 0.5265 - classification_loss: 0.3051
 643/3000 [=====>........................] - ETA: 16:06 - loss: 0.8311 - regression_loss: 0.5262 - classification_loss: 0.3049
 644/3000 [=====>........................] - ETA: 16:06 - loss: 0.8306 - regression_loss: 0.5259 - classification_loss: 0.3047
 645/3000 [=====>........................] - ETA: 16:05 - loss: 0.8307 - regression_loss: 0.5261 - classification_loss: 0.3046
 646/3000 [=====>........................] - ETA: 16:05 - loss: 0.8305 - regression_loss: 0.5259 - classification_loss: 0.3045
 647/3000 [=====>........................] - ETA: 16:04 - loss: 0.8311 - regression_loss: 0.5266 - classification_loss: 0.3045
 648/3000 [=====>........................] - ETA: 16:04 - loss: 0.8314 - regression_loss: 0.5269 - classification_loss: 0.3046
 649/3000 [=====>........................] - ETA: 16:04 - loss: 0.8311 - regression_loss: 0.5265 - classification_loss: 0.3046
 650/3000 [=====>........................] - ETA: 16:03 - loss: 0.8313 - regression_loss: 0.5267 - classification_loss: 0.3046
 651/3000 [=====>........................] - ETA: 16:03 - loss: 0.8310 - regression_loss: 0.5264 - classification_loss: 0.3046
 652/3000 [=====>........................] - ETA: 16:02 - loss: 0.8312 - regression_loss: 0.5267 - classification_loss: 0.3045
 653/3000 [=====>........................] - ETA: 16:02 - loss: 0.8308 - regression_loss: 0.5263 - classification_loss: 0.3045
 654/3000 [=====>........................] - ETA: 16:02 - loss: 0.8306 - regression_loss: 0.5260 - classification_loss: 0.3046
 655/3000 [=====>........................] - ETA: 16:01 - loss: 0.8308 - regression_loss: 0.5263 - classification_loss: 0.3045
 656/3000 [=====>........................] - ETA: 16:01 - loss: 0.8305 - regression_loss: 0.5264 - classification_loss: 0.3042
 657/3000 [=====>........................] - ETA: 16:01 - loss: 0.8307 - regression_loss: 0.5265 - classification_loss: 0.3042
 658/3000 [=====>........................] - ETA: 16:00 - loss: 0.8302 - regression_loss: 0.5263 - classification_loss: 0.3039
 659/3000 [=====>........................] - ETA: 16:00 - loss: 0.8301 - regression_loss: 0.5262 - classification_loss: 0.3039
 660/3000 [=====>........................] - ETA: 15:59 - loss: 0.8297 - regression_loss: 0.5259 - classification_loss: 0.3038
 661/3000 [=====>........................] - ETA: 15:59 - loss: 0.8295 - regression_loss: 0.5257 - classification_loss: 0.3038
 662/3000 [=====>........................] - ETA: 15:59 - loss: 0.8299 - regression_loss: 0.5261 - classification_loss: 0.3038
 663/3000 [=====>........................] - ETA: 15:58 - loss: 0.8296 - regression_loss: 0.5261 - classification_loss: 0.3035
 664/3000 [=====>........................] - ETA: 15:58 - loss: 0.8301 - regression_loss: 0.5266 - classification_loss: 0.3035
 665/3000 [=====>........................] - ETA: 15:57 - loss: 0.8302 - regression_loss: 0.5269 - classification_loss: 0.3033
 666/3000 [=====>........................] - ETA: 15:57 - loss: 0.8302 - regression_loss: 0.5270 - classification_loss: 0.3031
 667/3000 [=====>........................] - ETA: 15:57 - loss: 0.8304 - regression_loss: 0.5271 - classification_loss: 0.3033
 668/3000 [=====>........................] - ETA: 15:56 - loss: 0.8304 - regression_loss: 0.5272 - classification_loss: 0.3032
 669/3000 [=====>........................] - ETA: 15:56 - loss: 0.8307 - regression_loss: 0.5276 - classification_loss: 0.3031
 670/3000 [=====>........................] - ETA: 15:55 - loss: 0.8305 - regression_loss: 0.5276 - classification_loss: 0.3029
 671/3000 [=====>........................] - ETA: 15:55 - loss: 0.8309 - regression_loss: 0.5281 - classification_loss: 0.3028
 672/3000 [=====>........................] - ETA: 15:55 - loss: 0.8302 - regression_loss: 0.5277 - classification_loss: 0.3025
 673/3000 [=====>........................] - ETA: 15:54 - loss: 0.8299 - regression_loss: 0.5274 - classification_loss: 0.3025
 674/3000 [=====>........................] - ETA: 15:54 - loss: 0.8294 - regression_loss: 0.5270 - classification_loss: 0.3024
 675/3000 [=====>........................] - ETA: 15:54 - loss: 0.8294 - regression_loss: 0.5269 - classification_loss: 0.3026
 676/3000 [=====>........................] - ETA: 15:53 - loss: 0.8290 - regression_loss: 0.5265 - classification_loss: 0.3026
 677/3000 [=====>........................] - ETA: 15:53 - loss: 0.8286 - regression_loss: 0.5262 - classification_loss: 0.3024
 678/3000 [=====>........................] - ETA: 15:52 - loss: 0.8285 - regression_loss: 0.5260 - classification_loss: 0.3025
 679/3000 [=====>........................] - ETA: 15:52 - loss: 0.8278 - regression_loss: 0.5255 - classification_loss: 0.3023
 680/3000 [=====>........................] - ETA: 15:51 - loss: 0.8276 - regression_loss: 0.5254 - classification_loss: 0.3022
 681/3000 [=====>........................] - ETA: 15:51 - loss: 0.8279 - regression_loss: 0.5257 - classification_loss: 0.3022
 682/3000 [=====>........................] - ETA: 15:51 - loss: 0.8281 - regression_loss: 0.5260 - classification_loss: 0.3021
 683/3000 [=====>........................] - ETA: 15:50 - loss: 0.8283 - regression_loss: 0.5263 - classification_loss: 0.3020
 684/3000 [=====>........................] - ETA: 15:50 - loss: 0.8282 - regression_loss: 0.5261 - classification_loss: 0.3022
 685/3000 [=====>........................] - ETA: 15:49 - loss: 0.8285 - regression_loss: 0.5263 - classification_loss: 0.3022
 686/3000 [=====>........................] - ETA: 15:49 - loss: 0.8288 - regression_loss: 0.5267 - classification_loss: 0.3021
 687/3000 [=====>........................] - ETA: 15:49 - loss: 0.8291 - regression_loss: 0.5271 - classification_loss: 0.3020
 688/3000 [=====>........................] - ETA: 15:48 - loss: 0.8292 - regression_loss: 0.5273 - classification_loss: 0.3019
 689/3000 [=====>........................] - ETA: 15:48 - loss: 0.8287 - regression_loss: 0.5269 - classification_loss: 0.3017
 690/3000 [=====>........................] - ETA: 15:47 - loss: 0.8286 - regression_loss: 0.5268 - classification_loss: 0.3018
 691/3000 [=====>........................] - ETA: 15:47 - loss: 0.8282 - regression_loss: 0.5266 - classification_loss: 0.3016
 692/3000 [=====>........................] - ETA: 15:47 - loss: 0.8282 - regression_loss: 0.5267 - classification_loss: 0.3014
 693/3000 [=====>........................] - ETA: 15:46 - loss: 0.8281 - regression_loss: 0.5268 - classification_loss: 0.3013
 694/3000 [=====>........................] - ETA: 15:46 - loss: 0.8282 - regression_loss: 0.5268 - classification_loss: 0.3014
 695/3000 [=====>........................] - ETA: 15:45 - loss: 0.8279 - regression_loss: 0.5266 - classification_loss: 0.3013
 696/3000 [=====>........................] - ETA: 15:45 - loss: 0.8273 - regression_loss: 0.5262 - classification_loss: 0.3010
 697/3000 [=====>........................] - ETA: 15:45 - loss: 0.8271 - regression_loss: 0.5260 - classification_loss: 0.3011
 698/3000 [=====>........................] - ETA: 15:44 - loss: 0.8269 - regression_loss: 0.5258 - classification_loss: 0.3011
 699/3000 [=====>........................] - ETA: 15:44 - loss: 0.8265 - regression_loss: 0.5256 - classification_loss: 0.3009
 700/3000 [======>.......................] - ETA: 15:43 - loss: 0.8260 - regression_loss: 0.5253 - classification_loss: 0.3007
 701/3000 [======>.......................] - ETA: 15:43 - loss: 0.8258 - regression_loss: 0.5250 - classification_loss: 0.3008
 702/3000 [======>.......................] - ETA: 15:42 - loss: 0.8261 - regression_loss: 0.5254 - classification_loss: 0.3006
 703/3000 [======>.......................] - ETA: 15:42 - loss: 0.8268 - regression_loss: 0.5261 - classification_loss: 0.3006
 704/3000 [======>.......................] - ETA: 15:42 - loss: 0.8266 - regression_loss: 0.5259 - classification_loss: 0.3006
 705/3000 [======>.......................] - ETA: 15:41 - loss: 0.8267 - regression_loss: 0.5262 - classification_loss: 0.3005
 706/3000 [======>.......................] - ETA: 15:41 - loss: 0.8261 - regression_loss: 0.5257 - classification_loss: 0.3004
 707/3000 [======>.......................] - ETA: 15:40 - loss: 0.8256 - regression_loss: 0.5253 - classification_loss: 0.3003
 708/3000 [======>.......................] - ETA: 15:40 - loss: 0.8254 - regression_loss: 0.5251 - classification_loss: 0.3003
 709/3000 [======>.......................] - ETA: 15:40 - loss: 0.8258 - regression_loss: 0.5255 - classification_loss: 0.3003
 710/3000 [======>.......................] - ETA: 15:39 - loss: 0.8259 - regression_loss: 0.5257 - classification_loss: 0.3002
 711/3000 [======>.......................] - ETA: 15:39 - loss: 0.8252 - regression_loss: 0.5254 - classification_loss: 0.2999
 712/3000 [======>.......................] - ETA: 15:38 - loss: 0.8249 - regression_loss: 0.5251 - classification_loss: 0.2998
 713/3000 [======>.......................] - ETA: 15:38 - loss: 0.8248 - regression_loss: 0.5250 - classification_loss: 0.2998
 714/3000 [======>.......................] - ETA: 15:37 - loss: 0.8246 - regression_loss: 0.5249 - classification_loss: 0.2997
 715/3000 [======>.......................] - ETA: 15:37 - loss: 0.8243 - regression_loss: 0.5246 - classification_loss: 0.2996
 716/3000 [======>.......................] - ETA: 15:37 - loss: 0.8244 - regression_loss: 0.5247 - classification_loss: 0.2997
 717/3000 [======>.......................] - ETA: 15:36 - loss: 0.8242 - regression_loss: 0.5245 - classification_loss: 0.2997
 718/3000 [======>.......................] - ETA: 15:36 - loss: 0.8240 - regression_loss: 0.5242 - classification_loss: 0.2998
 719/3000 [======>.......................] - ETA: 15:35 - loss: 0.8240 - regression_loss: 0.5243 - classification_loss: 0.2997
 720/3000 [======>.......................] - ETA: 15:35 - loss: 0.8242 - regression_loss: 0.5246 - classification_loss: 0.2996
 721/3000 [======>.......................] - ETA: 15:35 - loss: 0.8240 - regression_loss: 0.5245 - classification_loss: 0.2995
 722/3000 [======>.......................] - ETA: 15:34 - loss: 0.8239 - regression_loss: 0.5244 - classification_loss: 0.2995
 723/3000 [======>.......................] - ETA: 15:34 - loss: 0.8241 - regression_loss: 0.5246 - classification_loss: 0.2995
 724/3000 [======>.......................] - ETA: 15:33 - loss: 0.8240 - regression_loss: 0.5245 - classification_loss: 0.2995
 725/3000 [======>.......................] - ETA: 15:33 - loss: 0.8239 - regression_loss: 0.5245 - classification_loss: 0.2994
 726/3000 [======>.......................] - ETA: 15:33 - loss: 0.8238 - regression_loss: 0.5244 - classification_loss: 0.2994
 727/3000 [======>.......................] - ETA: 15:32 - loss: 0.8236 - regression_loss: 0.5243 - classification_loss: 0.2992
 728/3000 [======>.......................] - ETA: 15:32 - loss: 0.8231 - regression_loss: 0.5240 - classification_loss: 0.2991
 729/3000 [======>.......................] - ETA: 15:31 - loss: 0.8227 - regression_loss: 0.5237 - classification_loss: 0.2990
 730/3000 [======>.......................] - ETA: 15:31 - loss: 0.8226 - regression_loss: 0.5238 - classification_loss: 0.2988
 731/3000 [======>.......................] - ETA: 15:31 - loss: 0.8227 - regression_loss: 0.5239 - classification_loss: 0.2987
 732/3000 [======>.......................] - ETA: 15:30 - loss: 0.8226 - regression_loss: 0.5238 - classification_loss: 0.2988
 733/3000 [======>.......................] - ETA: 15:30 - loss: 0.8225 - regression_loss: 0.5239 - classification_loss: 0.2986
 734/3000 [======>.......................] - ETA: 15:29 - loss: 0.8228 - regression_loss: 0.5240 - classification_loss: 0.2987
 735/3000 [======>.......................] - ETA: 15:29 - loss: 0.8223 - regression_loss: 0.5237 - classification_loss: 0.2986
 736/3000 [======>.......................] - ETA: 15:29 - loss: 0.8224 - regression_loss: 0.5238 - classification_loss: 0.2986
 737/3000 [======>.......................] - ETA: 15:28 - loss: 0.8222 - regression_loss: 0.5237 - classification_loss: 0.2985
 738/3000 [======>.......................] - ETA: 15:28 - loss: 0.8221 - regression_loss: 0.5237 - classification_loss: 0.2984
 739/3000 [======>.......................] - ETA: 15:27 - loss: 0.8221 - regression_loss: 0.5237 - classification_loss: 0.2985
 740/3000 [======>.......................] - ETA: 15:27 - loss: 0.8220 - regression_loss: 0.5237 - classification_loss: 0.2983
 741/3000 [======>.......................] - ETA: 15:27 - loss: 0.8218 - regression_loss: 0.5235 - classification_loss: 0.2983
 742/3000 [======>.......................] - ETA: 15:26 - loss: 0.8216 - regression_loss: 0.5234 - classification_loss: 0.2982
 743/3000 [======>.......................] - ETA: 15:26 - loss: 0.8214 - regression_loss: 0.5234 - classification_loss: 0.2980
 744/3000 [======>.......................] - ETA: 15:26 - loss: 0.8212 - regression_loss: 0.5232 - classification_loss: 0.2980
 745/3000 [======>.......................] - ETA: 15:25 - loss: 0.8211 - regression_loss: 0.5231 - classification_loss: 0.2980
 746/3000 [======>.......................] - ETA: 15:25 - loss: 0.8209 - regression_loss: 0.5231 - classification_loss: 0.2978
 747/3000 [======>.......................] - ETA: 15:24 - loss: 0.8206 - regression_loss: 0.5230 - classification_loss: 0.2976
 748/3000 [======>.......................] - ETA: 15:24 - loss: 0.8206 - regression_loss: 0.5231 - classification_loss: 0.2975
 749/3000 [======>.......................] - ETA: 15:24 - loss: 0.8205 - regression_loss: 0.5231 - classification_loss: 0.2974
 750/3000 [======>.......................] - ETA: 15:23 - loss: 0.8204 - regression_loss: 0.5231 - classification_loss: 0.2973
 751/3000 [======>.......................] - ETA: 15:23 - loss: 0.8205 - regression_loss: 0.5231 - classification_loss: 0.2974
 752/3000 [======>.......................] - ETA: 15:22 - loss: 0.8201 - regression_loss: 0.5228 - classification_loss: 0.2973
 753/3000 [======>.......................] - ETA: 15:22 - loss: 0.8199 - regression_loss: 0.5225 - classification_loss: 0.2974
 754/3000 [======>.......................] - ETA: 15:22 - loss: 0.8195 - regression_loss: 0.5221 - classification_loss: 0.2974
 755/3000 [======>.......................] - ETA: 15:21 - loss: 0.8192 - regression_loss: 0.5219 - classification_loss: 0.2973
 756/3000 [======>.......................] - ETA: 15:21 - loss: 0.8198 - regression_loss: 0.5224 - classification_loss: 0.2974
 757/3000 [======>.......................] - ETA: 15:20 - loss: 0.8202 - regression_loss: 0.5227 - classification_loss: 0.2975
 758/3000 [======>.......................] - ETA: 15:20 - loss: 0.8203 - regression_loss: 0.5228 - classification_loss: 0.2975
 759/3000 [======>.......................] - ETA: 15:20 - loss: 0.8202 - regression_loss: 0.5227 - classification_loss: 0.2975
 760/3000 [======>.......................] - ETA: 15:19 - loss: 0.8208 - regression_loss: 0.5234 - classification_loss: 0.2974
 761/3000 [======>.......................] - ETA: 15:19 - loss: 0.8207 - regression_loss: 0.5233 - classification_loss: 0.2974
 762/3000 [======>.......................] - ETA: 15:18 - loss: 0.8204 - regression_loss: 0.5232 - classification_loss: 0.2972
 763/3000 [======>.......................] - ETA: 15:18 - loss: 0.8202 - regression_loss: 0.5231 - classification_loss: 0.2971
 764/3000 [======>.......................] - ETA: 15:18 - loss: 0.8203 - regression_loss: 0.5232 - classification_loss: 0.2971
 765/3000 [======>.......................] - ETA: 15:17 - loss: 0.8198 - regression_loss: 0.5228 - classification_loss: 0.2970
 766/3000 [======>.......................] - ETA: 15:17 - loss: 0.8201 - regression_loss: 0.5231 - classification_loss: 0.2970
 767/3000 [======>.......................] - ETA: 15:16 - loss: 0.8202 - regression_loss: 0.5232 - classification_loss: 0.2970
 768/3000 [======>.......................] - ETA: 15:16 - loss: 0.8203 - regression_loss: 0.5233 - classification_loss: 0.2970
 769/3000 [======>.......................] - ETA: 15:16 - loss: 0.8206 - regression_loss: 0.5236 - classification_loss: 0.2970
 770/3000 [======>.......................] - ETA: 15:15 - loss: 0.8202 - regression_loss: 0.5235 - classification_loss: 0.2967
 771/3000 [======>.......................] - ETA: 15:15 - loss: 0.8202 - regression_loss: 0.5236 - classification_loss: 0.2966
 772/3000 [======>.......................] - ETA: 15:14 - loss: 0.8200 - regression_loss: 0.5234 - classification_loss: 0.2966
 773/3000 [======>.......................] - ETA: 15:14 - loss: 0.8196 - regression_loss: 0.5231 - classification_loss: 0.2965
 774/3000 [======>.......................] - ETA: 15:14 - loss: 0.8194 - regression_loss: 0.5227 - classification_loss: 0.2967
 775/3000 [======>.......................] - ETA: 15:13 - loss: 0.8194 - regression_loss: 0.5227 - classification_loss: 0.2967
 776/3000 [======>.......................] - ETA: 15:13 - loss: 0.8191 - regression_loss: 0.5225 - classification_loss: 0.2967
 777/3000 [======>.......................] - ETA: 15:12 - loss: 0.8190 - regression_loss: 0.5225 - classification_loss: 0.2965
 778/3000 [======>.......................] - ETA: 15:12 - loss: 0.8186 - regression_loss: 0.5222 - classification_loss: 0.2964
 779/3000 [======>.......................] - ETA: 15:11 - loss: 0.8188 - regression_loss: 0.5223 - classification_loss: 0.2964
 780/3000 [======>.......................] - ETA: 15:11 - loss: 0.8185 - regression_loss: 0.5221 - classification_loss: 0.2964
 781/3000 [======>.......................] - ETA: 15:11 - loss: 0.8182 - regression_loss: 0.5217 - classification_loss: 0.2965
 782/3000 [======>.......................] - ETA: 15:10 - loss: 0.8179 - regression_loss: 0.5214 - classification_loss: 0.2966
 783/3000 [======>.......................] - ETA: 15:10 - loss: 0.8180 - regression_loss: 0.5216 - classification_loss: 0.2964
 784/3000 [======>.......................] - ETA: 15:09 - loss: 0.8178 - regression_loss: 0.5214 - classification_loss: 0.2963
 785/3000 [======>.......................] - ETA: 15:09 - loss: 0.8175 - regression_loss: 0.5211 - classification_loss: 0.2964
 786/3000 [======>.......................] - ETA: 15:09 - loss: 0.8173 - regression_loss: 0.5209 - classification_loss: 0.2964
 787/3000 [======>.......................] - ETA: 15:08 - loss: 0.8172 - regression_loss: 0.5208 - classification_loss: 0.2964
 788/3000 [======>.......................] - ETA: 15:08 - loss: 0.8172 - regression_loss: 0.5209 - classification_loss: 0.2964
 789/3000 [======>.......................] - ETA: 15:07 - loss: 0.8169 - regression_loss: 0.5206 - classification_loss: 0.2963
 790/3000 [======>.......................] - ETA: 15:07 - loss: 0.8169 - regression_loss: 0.5205 - classification_loss: 0.2964
 791/3000 [======>.......................] - ETA: 15:06 - loss: 0.8165 - regression_loss: 0.5202 - classification_loss: 0.2963
 792/3000 [======>.......................] - ETA: 15:06 - loss: 0.8166 - regression_loss: 0.5203 - classification_loss: 0.2962
 793/3000 [======>.......................] - ETA: 15:06 - loss: 0.8164 - regression_loss: 0.5201 - classification_loss: 0.2963
 794/3000 [======>.......................] - ETA: 15:05 - loss: 0.8161 - regression_loss: 0.5199 - classification_loss: 0.2963
 795/3000 [======>.......................] - ETA: 15:05 - loss: 0.8159 - regression_loss: 0.5197 - classification_loss: 0.2962
 796/3000 [======>.......................] - ETA: 15:04 - loss: 0.8158 - regression_loss: 0.5196 - classification_loss: 0.2961
 797/3000 [======>.......................] - ETA: 15:04 - loss: 0.8159 - regression_loss: 0.5197 - classification_loss: 0.2963
 798/3000 [======>.......................] - ETA: 15:04 - loss: 0.8156 - regression_loss: 0.5193 - classification_loss: 0.2963
 799/3000 [======>.......................] - ETA: 15:03 - loss: 0.8159 - regression_loss: 0.5197 - classification_loss: 0.2962
 800/3000 [=======>......................] - ETA: 15:03 - loss: 0.8158 - regression_loss: 0.5195 - classification_loss: 0.2963
 801/3000 [=======>......................] - ETA: 15:02 - loss: 0.8157 - regression_loss: 0.5194 - classification_loss: 0.2963
 802/3000 [=======>......................] - ETA: 15:02 - loss: 0.8158 - regression_loss: 0.5195 - classification_loss: 0.2963
 803/3000 [=======>......................] - ETA: 15:02 - loss: 0.8158 - regression_loss: 0.5195 - classification_loss: 0.2963
 804/3000 [=======>......................] - ETA: 15:01 - loss: 0.8155 - regression_loss: 0.5193 - classification_loss: 0.2962
 805/3000 [=======>......................] - ETA: 15:01 - loss: 0.8153 - regression_loss: 0.5191 - classification_loss: 0.2962
 806/3000 [=======>......................] - ETA: 15:00 - loss: 0.8156 - regression_loss: 0.5195 - classification_loss: 0.2961
 807/3000 [=======>......................] - ETA: 15:00 - loss: 0.8157 - regression_loss: 0.5195 - classification_loss: 0.2961
 808/3000 [=======>......................] - ETA: 15:00 - loss: 0.8159 - regression_loss: 0.5198 - classification_loss: 0.2961
 809/3000 [=======>......................] - ETA: 14:59 - loss: 0.8154 - regression_loss: 0.5195 - classification_loss: 0.2959
 810/3000 [=======>......................] - ETA: 14:59 - loss: 0.8151 - regression_loss: 0.5191 - classification_loss: 0.2959
 811/3000 [=======>......................] - ETA: 14:58 - loss: 0.8155 - regression_loss: 0.5196 - classification_loss: 0.2959
 812/3000 [=======>......................] - ETA: 14:58 - loss: 0.8149 - regression_loss: 0.5192 - classification_loss: 0.2957
 813/3000 [=======>......................] - ETA: 14:58 - loss: 0.8150 - regression_loss: 0.5194 - classification_loss: 0.2956
 814/3000 [=======>......................] - ETA: 14:57 - loss: 0.8147 - regression_loss: 0.5192 - classification_loss: 0.2955
 815/3000 [=======>......................] - ETA: 14:57 - loss: 0.8145 - regression_loss: 0.5191 - classification_loss: 0.2954
 816/3000 [=======>......................] - ETA: 14:57 - loss: 0.8144 - regression_loss: 0.5190 - classification_loss: 0.2953
 817/3000 [=======>......................] - ETA: 14:56 - loss: 0.8141 - regression_loss: 0.5189 - classification_loss: 0.2952
 818/3000 [=======>......................] - ETA: 14:56 - loss: 0.8138 - regression_loss: 0.5189 - classification_loss: 0.2949
 819/3000 [=======>......................] - ETA: 14:55 - loss: 0.8133 - regression_loss: 0.5186 - classification_loss: 0.2947
 820/3000 [=======>......................] - ETA: 14:55 - loss: 0.8130 - regression_loss: 0.5183 - classification_loss: 0.2947
 821/3000 [=======>......................] - ETA: 14:54 - loss: 0.8131 - regression_loss: 0.5185 - classification_loss: 0.2946
 822/3000 [=======>......................] - ETA: 14:54 - loss: 0.8133 - regression_loss: 0.5188 - classification_loss: 0.2945
 823/3000 [=======>......................] - ETA: 14:54 - loss: 0.8134 - regression_loss: 0.5189 - classification_loss: 0.2945
 824/3000 [=======>......................] - ETA: 14:53 - loss: 0.8132 - regression_loss: 0.5188 - classification_loss: 0.2944
 825/3000 [=======>......................] - ETA: 14:53 - loss: 0.8133 - regression_loss: 0.5189 - classification_loss: 0.2944
 826/3000 [=======>......................] - ETA: 14:52 - loss: 0.8132 - regression_loss: 0.5188 - classification_loss: 0.2944
 827/3000 [=======>......................] - ETA: 14:52 - loss: 0.8135 - regression_loss: 0.5190 - classification_loss: 0.2945
 828/3000 [=======>......................] - ETA: 14:52 - loss: 0.8130 - regression_loss: 0.5188 - classification_loss: 0.2942
 829/3000 [=======>......................] - ETA: 14:51 - loss: 0.8127 - regression_loss: 0.5185 - classification_loss: 0.2942
 830/3000 [=======>......................] - ETA: 14:51 - loss: 0.8124 - regression_loss: 0.5182 - classification_loss: 0.2942
 831/3000 [=======>......................] - ETA: 14:50 - loss: 0.8121 - regression_loss: 0.5179 - classification_loss: 0.2942
 832/3000 [=======>......................] - ETA: 14:50 - loss: 0.8116 - regression_loss: 0.5177 - classification_loss: 0.2939
 833/3000 [=======>......................] - ETA: 14:49 - loss: 0.8114 - regression_loss: 0.5177 - classification_loss: 0.2937
 834/3000 [=======>......................] - ETA: 14:49 - loss: 0.8117 - regression_loss: 0.5180 - classification_loss: 0.2937
 835/3000 [=======>......................] - ETA: 14:49 - loss: 0.8117 - regression_loss: 0.5179 - classification_loss: 0.2938
 836/3000 [=======>......................] - ETA: 14:48 - loss: 0.8118 - regression_loss: 0.5181 - classification_loss: 0.2938
 837/3000 [=======>......................] - ETA: 14:48 - loss: 0.8114 - regression_loss: 0.5177 - classification_loss: 0.2937
 838/3000 [=======>......................] - ETA: 14:47 - loss: 0.8114 - regression_loss: 0.5176 - classification_loss: 0.2938
 839/3000 [=======>......................] - ETA: 14:47 - loss: 0.8114 - regression_loss: 0.5175 - classification_loss: 0.2939
 840/3000 [=======>......................] - ETA: 14:46 - loss: 0.8115 - regression_loss: 0.5176 - classification_loss: 0.2939
 841/3000 [=======>......................] - ETA: 14:46 - loss: 0.8115 - regression_loss: 0.5174 - classification_loss: 0.2941
 842/3000 [=======>......................] - ETA: 14:46 - loss: 0.8114 - regression_loss: 0.5172 - classification_loss: 0.2943
 843/3000 [=======>......................] - ETA: 14:45 - loss: 0.8114 - regression_loss: 0.5172 - classification_loss: 0.2943
 844/3000 [=======>......................] - ETA: 14:45 - loss: 0.8114 - regression_loss: 0.5171 - classification_loss: 0.2943
 845/3000 [=======>......................] - ETA: 14:44 - loss: 0.8112 - regression_loss: 0.5168 - classification_loss: 0.2944
 846/3000 [=======>......................] - ETA: 14:44 - loss: 0.8113 - regression_loss: 0.5170 - classification_loss: 0.2944
 847/3000 [=======>......................] - ETA: 14:44 - loss: 0.8112 - regression_loss: 0.5169 - classification_loss: 0.2943
 848/3000 [=======>......................] - ETA: 14:43 - loss: 0.8112 - regression_loss: 0.5169 - classification_loss: 0.2943
 849/3000 [=======>......................] - ETA: 14:43 - loss: 0.8114 - regression_loss: 0.5171 - classification_loss: 0.2943
 850/3000 [=======>......................] - ETA: 14:42 - loss: 0.8114 - regression_loss: 0.5170 - classification_loss: 0.2944
 851/3000 [=======>......................] - ETA: 14:42 - loss: 0.8118 - regression_loss: 0.5174 - classification_loss: 0.2944
 852/3000 [=======>......................] - ETA: 14:42 - loss: 0.8118 - regression_loss: 0.5175 - classification_loss: 0.2943
 853/3000 [=======>......................] - ETA: 14:41 - loss: 0.8117 - regression_loss: 0.5176 - classification_loss: 0.2942
 854/3000 [=======>......................] - ETA: 14:41 - loss: 0.8116 - regression_loss: 0.5174 - classification_loss: 0.2942
 855/3000 [=======>......................] - ETA: 14:40 - loss: 0.8115 - regression_loss: 0.5173 - classification_loss: 0.2942
 856/3000 [=======>......................] - ETA: 14:40 - loss: 0.8116 - regression_loss: 0.5175 - classification_loss: 0.2941
 857/3000 [=======>......................] - ETA: 14:40 - loss: 0.8114 - regression_loss: 0.5173 - classification_loss: 0.2941
 858/3000 [=======>......................] - ETA: 14:39 - loss: 0.8114 - regression_loss: 0.5174 - classification_loss: 0.2940
 859/3000 [=======>......................] - ETA: 14:39 - loss: 0.8113 - regression_loss: 0.5173 - classification_loss: 0.2940
 860/3000 [=======>......................] - ETA: 14:38 - loss: 0.8111 - regression_loss: 0.5171 - classification_loss: 0.2940
 861/3000 [=======>......................] - ETA: 14:38 - loss: 0.8109 - regression_loss: 0.5170 - classification_loss: 0.2938
 862/3000 [=======>......................] - ETA: 14:37 - loss: 0.8108 - regression_loss: 0.5168 - classification_loss: 0.2940
 863/3000 [=======>......................] - ETA: 14:37 - loss: 0.8105 - regression_loss: 0.5167 - classification_loss: 0.2938
 864/3000 [=======>......................] - ETA: 14:37 - loss: 0.8103 - regression_loss: 0.5164 - classification_loss: 0.2938
 865/3000 [=======>......................] - ETA: 14:36 - loss: 0.8102 - regression_loss: 0.5165 - classification_loss: 0.2937
 866/3000 [=======>......................] - ETA: 14:36 - loss: 0.8103 - regression_loss: 0.5167 - classification_loss: 0.2936
 867/3000 [=======>......................] - ETA: 14:35 - loss: 0.8103 - regression_loss: 0.5168 - classification_loss: 0.2935
 868/3000 [=======>......................] - ETA: 14:35 - loss: 0.8107 - regression_loss: 0.5172 - classification_loss: 0.2935
 869/3000 [=======>......................] - ETA: 14:35 - loss: 0.8107 - regression_loss: 0.5171 - classification_loss: 0.2936
 870/3000 [=======>......................] - ETA: 14:34 - loss: 0.8106 - regression_loss: 0.5169 - classification_loss: 0.2937
 871/3000 [=======>......................] - ETA: 14:34 - loss: 0.8103 - regression_loss: 0.5167 - classification_loss: 0.2936
 872/3000 [=======>......................] - ETA: 14:33 - loss: 0.8102 - regression_loss: 0.5165 - classification_loss: 0.2937
 873/3000 [=======>......................] - ETA: 14:33 - loss: 0.8107 - regression_loss: 0.5169 - classification_loss: 0.2937
 874/3000 [=======>......................] - ETA: 14:32 - loss: 0.8108 - regression_loss: 0.5171 - classification_loss: 0.2937
 875/3000 [=======>......................] - ETA: 14:32 - loss: 0.8106 - regression_loss: 0.5171 - classification_loss: 0.2936
 876/3000 [=======>......................] - ETA: 14:32 - loss: 0.8105 - regression_loss: 0.5169 - classification_loss: 0.2936
 877/3000 [=======>......................] - ETA: 14:31 - loss: 0.8103 - regression_loss: 0.5167 - classification_loss: 0.2936
 878/3000 [=======>......................] - ETA: 14:31 - loss: 0.8102 - regression_loss: 0.5166 - classification_loss: 0.2936
 879/3000 [=======>......................] - ETA: 14:30 - loss: 0.8103 - regression_loss: 0.5166 - classification_loss: 0.2937
 880/3000 [=======>......................] - ETA: 14:30 - loss: 0.8101 - regression_loss: 0.5166 - classification_loss: 0.2935
 881/3000 [=======>......................] - ETA: 14:30 - loss: 0.8100 - regression_loss: 0.5165 - classification_loss: 0.2935
 882/3000 [=======>......................] - ETA: 14:29 - loss: 0.8099 - regression_loss: 0.5165 - classification_loss: 0.2934
 883/3000 [=======>......................] - ETA: 14:29 - loss: 0.8097 - regression_loss: 0.5163 - classification_loss: 0.2934
 884/3000 [=======>......................] - ETA: 14:28 - loss: 0.8094 - regression_loss: 0.5161 - classification_loss: 0.2933
 885/3000 [=======>......................] - ETA: 14:28 - loss: 0.8091 - regression_loss: 0.5159 - classification_loss: 0.2933
 886/3000 [=======>......................] - ETA: 14:28 - loss: 0.8090 - regression_loss: 0.5157 - classification_loss: 0.2933
 887/3000 [=======>......................] - ETA: 14:27 - loss: 0.8087 - regression_loss: 0.5154 - classification_loss: 0.2933
 888/3000 [=======>......................] - ETA: 14:27 - loss: 0.8090 - regression_loss: 0.5157 - classification_loss: 0.2933
 889/3000 [=======>......................] - ETA: 14:26 - loss: 0.8090 - regression_loss: 0.5157 - classification_loss: 0.2933
 890/3000 [=======>......................] - ETA: 14:26 - loss: 0.8088 - regression_loss: 0.5155 - classification_loss: 0.2932
 891/3000 [=======>......................] - ETA: 14:25 - loss: 0.8088 - regression_loss: 0.5155 - classification_loss: 0.2933
 892/3000 [=======>......................] - ETA: 14:25 - loss: 0.8087 - regression_loss: 0.5153 - classification_loss: 0.2934
 893/3000 [=======>......................] - ETA: 14:25 - loss: 0.8085 - regression_loss: 0.5150 - classification_loss: 0.2935
 894/3000 [=======>......................] - ETA: 14:24 - loss: 0.8081 - regression_loss: 0.5147 - classification_loss: 0.2934
 895/3000 [=======>......................] - ETA: 14:24 - loss: 0.8079 - regression_loss: 0.5145 - classification_loss: 0.2934
 896/3000 [=======>......................] - ETA: 14:23 - loss: 0.8080 - regression_loss: 0.5147 - classification_loss: 0.2933
 897/3000 [=======>......................] - ETA: 14:23 - loss: 0.8080 - regression_loss: 0.5146 - classification_loss: 0.2934
 898/3000 [=======>......................] - ETA: 14:23 - loss: 0.8075 - regression_loss: 0.5143 - classification_loss: 0.2933
 899/3000 [=======>......................] - ETA: 14:22 - loss: 0.8076 - regression_loss: 0.5144 - classification_loss: 0.2932
 900/3000 [========>.....................] - ETA: 14:22 - loss: 0.8077 - regression_loss: 0.5143 - classification_loss: 0.2934
 901/3000 [========>.....................] - ETA: 14:21 - loss: 0.8078 - regression_loss: 0.5145 - classification_loss: 0.2933
 902/3000 [========>.....................] - ETA: 14:21 - loss: 0.8075 - regression_loss: 0.5143 - classification_loss: 0.2932
 903/3000 [========>.....................] - ETA: 14:21 - loss: 0.8079 - regression_loss: 0.5147 - classification_loss: 0.2932
 904/3000 [========>.....................] - ETA: 14:20 - loss: 0.8077 - regression_loss: 0.5146 - classification_loss: 0.2932
 905/3000 [========>.....................] - ETA: 14:20 - loss: 0.8080 - regression_loss: 0.5149 - classification_loss: 0.2931
 906/3000 [========>.....................] - ETA: 14:19 - loss: 0.8081 - regression_loss: 0.5149 - classification_loss: 0.2932
 907/3000 [========>.....................] - ETA: 14:19 - loss: 0.8077 - regression_loss: 0.5146 - classification_loss: 0.2931
 908/3000 [========>.....................] - ETA: 14:19 - loss: 0.8075 - regression_loss: 0.5144 - classification_loss: 0.2931
 909/3000 [========>.....................] - ETA: 14:18 - loss: 0.8076 - regression_loss: 0.5145 - classification_loss: 0.2931
 910/3000 [========>.....................] - ETA: 14:18 - loss: 0.8077 - regression_loss: 0.5145 - classification_loss: 0.2932
 911/3000 [========>.....................] - ETA: 14:17 - loss: 0.8076 - regression_loss: 0.5144 - classification_loss: 0.2932
 912/3000 [========>.....................] - ETA: 14:17 - loss: 0.8079 - regression_loss: 0.5148 - classification_loss: 0.2932
 913/3000 [========>.....................] - ETA: 14:16 - loss: 0.8077 - regression_loss: 0.5146 - classification_loss: 0.2931
 914/3000 [========>.....................] - ETA: 14:16 - loss: 0.8076 - regression_loss: 0.5144 - classification_loss: 0.2932
 915/3000 [========>.....................] - ETA: 14:16 - loss: 0.8074 - regression_loss: 0.5143 - classification_loss: 0.2931
 916/3000 [========>.....................] - ETA: 14:15 - loss: 0.8073 - regression_loss: 0.5141 - classification_loss: 0.2931
 917/3000 [========>.....................] - ETA: 14:15 - loss: 0.8075 - regression_loss: 0.5145 - classification_loss: 0.2931
 918/3000 [========>.....................] - ETA: 14:14 - loss: 0.8078 - regression_loss: 0.5147 - classification_loss: 0.2931
 919/3000 [========>.....................] - ETA: 14:14 - loss: 0.8075 - regression_loss: 0.5145 - classification_loss: 0.2930
 920/3000 [========>.....................] - ETA: 14:14 - loss: 0.8073 - regression_loss: 0.5143 - classification_loss: 0.2930
 921/3000 [========>.....................] - ETA: 14:13 - loss: 0.8073 - regression_loss: 0.5143 - classification_loss: 0.2930
 922/3000 [========>.....................] - ETA: 14:13 - loss: 0.8074 - regression_loss: 0.5145 - classification_loss: 0.2929
 923/3000 [========>.....................] - ETA: 14:12 - loss: 0.8074 - regression_loss: 0.5144 - classification_loss: 0.2929
 924/3000 [========>.....................] - ETA: 14:12 - loss: 0.8071 - regression_loss: 0.5142 - classification_loss: 0.2928
 925/3000 [========>.....................] - ETA: 14:12 - loss: 0.8069 - regression_loss: 0.5141 - classification_loss: 0.2927
 926/3000 [========>.....................] - ETA: 14:11 - loss: 0.8065 - regression_loss: 0.5138 - classification_loss: 0.2927
 927/3000 [========>.....................] - ETA: 14:11 - loss: 0.8062 - regression_loss: 0.5135 - classification_loss: 0.2927
 928/3000 [========>.....................] - ETA: 14:10 - loss: 0.8059 - regression_loss: 0.5132 - classification_loss: 0.2927
 929/3000 [========>.....................] - ETA: 14:10 - loss: 0.8057 - regression_loss: 0.5131 - classification_loss: 0.2927
 930/3000 [========>.....................] - ETA: 14:10 - loss: 0.8055 - regression_loss: 0.5129 - classification_loss: 0.2925
 931/3000 [========>.....................] - ETA: 14:09 - loss: 0.8052 - regression_loss: 0.5127 - classification_loss: 0.2925
 932/3000 [========>.....................] - ETA: 14:09 - loss: 0.8051 - regression_loss: 0.5125 - classification_loss: 0.2926
 933/3000 [========>.....................] - ETA: 14:08 - loss: 0.8050 - regression_loss: 0.5123 - classification_loss: 0.2926
 934/3000 [========>.....................] - ETA: 14:08 - loss: 0.8049 - regression_loss: 0.5122 - classification_loss: 0.2927
 935/3000 [========>.....................] - ETA: 14:08 - loss: 0.8049 - regression_loss: 0.5121 - classification_loss: 0.2927
 936/3000 [========>.....................] - ETA: 14:07 - loss: 0.8045 - regression_loss: 0.5118 - classification_loss: 0.2926
 937/3000 [========>.....................] - ETA: 14:07 - loss: 0.8042 - regression_loss: 0.5116 - classification_loss: 0.2925
 938/3000 [========>.....................] - ETA: 14:06 - loss: 0.8041 - regression_loss: 0.5115 - classification_loss: 0.2925
 939/3000 [========>.....................] - ETA: 14:06 - loss: 0.8042 - regression_loss: 0.5117 - classification_loss: 0.2924
 940/3000 [========>.....................] - ETA: 14:06 - loss: 0.8041 - regression_loss: 0.5119 - classification_loss: 0.2923
 941/3000 [========>.....................] - ETA: 14:05 - loss: 0.8042 - regression_loss: 0.5120 - classification_loss: 0.2922
 942/3000 [========>.....................] - ETA: 14:05 - loss: 0.8038 - regression_loss: 0.5117 - classification_loss: 0.2921
 943/3000 [========>.....................] - ETA: 14:04 - loss: 0.8038 - regression_loss: 0.5117 - classification_loss: 0.2920
 944/3000 [========>.....................] - ETA: 14:04 - loss: 0.8034 - regression_loss: 0.5115 - classification_loss: 0.2919
 945/3000 [========>.....................] - ETA: 14:04 - loss: 0.8032 - regression_loss: 0.5113 - classification_loss: 0.2919
 946/3000 [========>.....................] - ETA: 14:03 - loss: 0.8029 - regression_loss: 0.5110 - classification_loss: 0.2919
 947/3000 [========>.....................] - ETA: 14:03 - loss: 0.8029 - regression_loss: 0.5111 - classification_loss: 0.2918
 948/3000 [========>.....................] - ETA: 14:02 - loss: 0.8029 - regression_loss: 0.5112 - classification_loss: 0.2917
 949/3000 [========>.....................] - ETA: 14:02 - loss: 0.8028 - regression_loss: 0.5112 - classification_loss: 0.2917
 950/3000 [========>.....................] - ETA: 14:02 - loss: 0.8029 - regression_loss: 0.5112 - classification_loss: 0.2917
 951/3000 [========>.....................] - ETA: 14:01 - loss: 0.8028 - regression_loss: 0.5113 - classification_loss: 0.2915
 952/3000 [========>.....................] - ETA: 14:01 - loss: 0.8030 - regression_loss: 0.5115 - classification_loss: 0.2914
 953/3000 [========>.....................] - ETA: 14:00 - loss: 0.8028 - regression_loss: 0.5116 - classification_loss: 0.2912
 954/3000 [========>.....................] - ETA: 14:00 - loss: 0.8030 - regression_loss: 0.5118 - classification_loss: 0.2913
 955/3000 [========>.....................] - ETA: 14:00 - loss: 0.8028 - regression_loss: 0.5116 - classification_loss: 0.2912
 956/3000 [========>.....................] - ETA: 13:59 - loss: 0.8028 - regression_loss: 0.5117 - classification_loss: 0.2910
 957/3000 [========>.....................] - ETA: 13:59 - loss: 0.8027 - regression_loss: 0.5118 - classification_loss: 0.2909
 958/3000 [========>.....................] - ETA: 13:58 - loss: 0.8026 - regression_loss: 0.5118 - classification_loss: 0.2908
 959/3000 [========>.....................] - ETA: 13:58 - loss: 0.8027 - regression_loss: 0.5120 - classification_loss: 0.2907
 960/3000 [========>.....................] - ETA: 13:58 - loss: 0.8028 - regression_loss: 0.5121 - classification_loss: 0.2907
 961/3000 [========>.....................] - ETA: 13:57 - loss: 0.8026 - regression_loss: 0.5119 - classification_loss: 0.2906
 962/3000 [========>.....................] - ETA: 13:57 - loss: 0.8026 - regression_loss: 0.5119 - classification_loss: 0.2907
 963/3000 [========>.....................] - ETA: 13:56 - loss: 0.8024 - regression_loss: 0.5117 - classification_loss: 0.2907
 964/3000 [========>.....................] - ETA: 13:56 - loss: 0.8026 - regression_loss: 0.5120 - classification_loss: 0.2906
 965/3000 [========>.....................] - ETA: 13:55 - loss: 0.8024 - regression_loss: 0.5119 - classification_loss: 0.2905
 966/3000 [========>.....................] - ETA: 13:55 - loss: 0.8025 - regression_loss: 0.5119 - classification_loss: 0.2905
 967/3000 [========>.....................] - ETA: 13:55 - loss: 0.8027 - regression_loss: 0.5122 - classification_loss: 0.2905
 968/3000 [========>.....................] - ETA: 13:54 - loss: 0.8024 - regression_loss: 0.5119 - classification_loss: 0.2905
 969/3000 [========>.....................] - ETA: 13:54 - loss: 0.8023 - regression_loss: 0.5118 - classification_loss: 0.2905
 970/3000 [========>.....................] - ETA: 13:53 - loss: 0.8020 - regression_loss: 0.5116 - classification_loss: 0.2904
 971/3000 [========>.....................] - ETA: 13:53 - loss: 0.8019 - regression_loss: 0.5114 - classification_loss: 0.2904
 972/3000 [========>.....................] - ETA: 13:52 - loss: 0.8019 - regression_loss: 0.5116 - classification_loss: 0.2904
 973/3000 [========>.....................] - ETA: 13:52 - loss: 0.8016 - regression_loss: 0.5114 - classification_loss: 0.2902
 974/3000 [========>.....................] - ETA: 13:52 - loss: 0.8013 - regression_loss: 0.5112 - classification_loss: 0.2902
 975/3000 [========>.....................] - ETA: 13:51 - loss: 0.8011 - regression_loss: 0.5110 - classification_loss: 0.2901
 976/3000 [========>.....................] - ETA: 13:51 - loss: 0.8011 - regression_loss: 0.5110 - classification_loss: 0.2902
 977/3000 [========>.....................] - ETA: 13:51 - loss: 0.8008 - regression_loss: 0.5107 - classification_loss: 0.2902
 978/3000 [========>.....................] - ETA: 13:50 - loss: 0.8009 - regression_loss: 0.5107 - classification_loss: 0.2902
 979/3000 [========>.....................] - ETA: 13:50 - loss: 0.8009 - regression_loss: 0.5107 - classification_loss: 0.2902
 980/3000 [========>.....................] - ETA: 13:49 - loss: 0.8006 - regression_loss: 0.5105 - classification_loss: 0.2901
 981/3000 [========>.....................] - ETA: 13:49 - loss: 0.8003 - regression_loss: 0.5102 - classification_loss: 0.2901
 982/3000 [========>.....................] - ETA: 13:48 - loss: 0.8001 - regression_loss: 0.5101 - classification_loss: 0.2901
 983/3000 [========>.....................] - ETA: 13:48 - loss: 0.7999 - regression_loss: 0.5099 - classification_loss: 0.2900
 984/3000 [========>.....................] - ETA: 13:48 - loss: 0.7999 - regression_loss: 0.5099 - classification_loss: 0.2901
 985/3000 [========>.....................] - ETA: 13:47 - loss: 0.8003 - regression_loss: 0.5102 - classification_loss: 0.2901
 986/3000 [========>.....................] - ETA: 13:47 - loss: 0.8000 - regression_loss: 0.5100 - classification_loss: 0.2900
 987/3000 [========>.....................] - ETA: 13:46 - loss: 0.8001 - regression_loss: 0.5101 - classification_loss: 0.2899
 988/3000 [========>.....................] - ETA: 13:46 - loss: 0.7999 - regression_loss: 0.5101 - classification_loss: 0.2898
 989/3000 [========>.....................] - ETA: 13:45 - loss: 0.7996 - regression_loss: 0.5099 - classification_loss: 0.2897
 990/3000 [========>.....................] - ETA: 13:45 - loss: 0.7995 - regression_loss: 0.5098 - classification_loss: 0.2897
 991/3000 [========>.....................] - ETA: 13:45 - loss: 0.7999 - regression_loss: 0.5102 - classification_loss: 0.2897
 992/3000 [========>.....................] - ETA: 13:44 - loss: 0.7998 - regression_loss: 0.5103 - classification_loss: 0.2896
 993/3000 [========>.....................] - ETA: 13:44 - loss: 0.7994 - regression_loss: 0.5100 - classification_loss: 0.2894
 994/3000 [========>.....................] - ETA: 13:43 - loss: 0.7990 - regression_loss: 0.5097 - classification_loss: 0.2893
 995/3000 [========>.....................] - ETA: 13:43 - loss: 0.7989 - regression_loss: 0.5097 - classification_loss: 0.2892
 996/3000 [========>.....................] - ETA: 13:43 - loss: 0.7986 - regression_loss: 0.5095 - classification_loss: 0.2891
 997/3000 [========>.....................] - ETA: 13:42 - loss: 0.7988 - regression_loss: 0.5097 - classification_loss: 0.2891
 998/3000 [========>.....................] - ETA: 13:42 - loss: 0.7986 - regression_loss: 0.5095 - classification_loss: 0.2891
 999/3000 [========>.....................] - ETA: 13:41 - loss: 0.7983 - regression_loss: 0.5093 - classification_loss: 0.2890
1000/3000 [=========>....................] - ETA: 13:41 - loss: 0.7985 - regression_loss: 0.5096 - classification_loss: 0.2889
1001/3000 [=========>....................] - ETA: 13:41 - loss: 0.7986 - regression_loss: 0.5097 - classification_loss: 0.2889
1002/3000 [=========>....................] - ETA: 13:40 - loss: 0.7984 - regression_loss: 0.5095 - classification_loss: 0.2890
1003/3000 [=========>....................] - ETA: 13:40 - loss: 0.7983 - regression_loss: 0.5093 - classification_loss: 0.2891
1004/3000 [=========>....................] - ETA: 13:39 - loss: 0.7980 - regression_loss: 0.5091 - classification_loss: 0.2889
1005/3000 [=========>....................] - ETA: 13:39 - loss: 0.7980 - regression_loss: 0.5090 - classification_loss: 0.2890
1006/3000 [=========>....................] - ETA: 13:39 - loss: 0.7980 - regression_loss: 0.5090 - classification_loss: 0.2890
1007/3000 [=========>....................] - ETA: 13:38 - loss: 0.7982 - regression_loss: 0.5092 - classification_loss: 0.2889
1008/3000 [=========>....................] - ETA: 13:38 - loss: 0.7979 - regression_loss: 0.5090 - classification_loss: 0.2889
1009/3000 [=========>....................] - ETA: 13:37 - loss: 0.7977 - regression_loss: 0.5088 - classification_loss: 0.2888
1010/3000 [=========>....................] - ETA: 13:37 - loss: 0.7973 - regression_loss: 0.5085 - classification_loss: 0.2888
1011/3000 [=========>....................] - ETA: 13:36 - loss: 0.7973 - regression_loss: 0.5086 - classification_loss: 0.2887
1012/3000 [=========>....................] - ETA: 13:36 - loss: 0.7971 - regression_loss: 0.5086 - classification_loss: 0.2885
1013/3000 [=========>....................] - ETA: 13:36 - loss: 0.7970 - regression_loss: 0.5085 - classification_loss: 0.2884
1014/3000 [=========>....................] - ETA: 13:35 - loss: 0.7969 - regression_loss: 0.5084 - classification_loss: 0.2884
1015/3000 [=========>....................] - ETA: 13:35 - loss: 0.7967 - regression_loss: 0.5082 - classification_loss: 0.2884
1016/3000 [=========>....................] - ETA: 13:34 - loss: 0.7963 - regression_loss: 0.5079 - classification_loss: 0.2883
1017/3000 [=========>....................] - ETA: 13:34 - loss: 0.7965 - regression_loss: 0.5082 - classification_loss: 0.2883
1018/3000 [=========>....................] - ETA: 13:34 - loss: 0.7966 - regression_loss: 0.5084 - classification_loss: 0.2882
1019/3000 [=========>....................] - ETA: 13:33 - loss: 0.7969 - regression_loss: 0.5087 - classification_loss: 0.2882
1020/3000 [=========>....................] - ETA: 13:33 - loss: 0.7969 - regression_loss: 0.5088 - classification_loss: 0.2881
1021/3000 [=========>....................] - ETA: 13:32 - loss: 0.7967 - regression_loss: 0.5086 - classification_loss: 0.2881
1022/3000 [=========>....................] - ETA: 13:32 - loss: 0.7965 - regression_loss: 0.5084 - classification_loss: 0.2881
1023/3000 [=========>....................] - ETA: 13:32 - loss: 0.7963 - regression_loss: 0.5082 - classification_loss: 0.2881
1024/3000 [=========>....................] - ETA: 13:31 - loss: 0.7960 - regression_loss: 0.5080 - classification_loss: 0.2881
1025/3000 [=========>....................] - ETA: 13:31 - loss: 0.7957 - regression_loss: 0.5077 - classification_loss: 0.2880
1026/3000 [=========>....................] - ETA: 13:30 - loss: 0.7955 - regression_loss: 0.5076 - classification_loss: 0.2880
1027/3000 [=========>....................] - ETA: 13:30 - loss: 0.7954 - regression_loss: 0.5076 - classification_loss: 0.2878
1028/3000 [=========>....................] - ETA: 13:30 - loss: 0.7955 - regression_loss: 0.5078 - classification_loss: 0.2878
1029/3000 [=========>....................] - ETA: 13:29 - loss: 0.7954 - regression_loss: 0.5076 - classification_loss: 0.2878
1030/3000 [=========>....................] - ETA: 13:29 - loss: 0.7955 - regression_loss: 0.5076 - classification_loss: 0.2878
1031/3000 [=========>....................] - ETA: 13:29 - loss: 0.7954 - regression_loss: 0.5075 - classification_loss: 0.2879
1032/3000 [=========>....................] - ETA: 13:28 - loss: 0.7954 - regression_loss: 0.5076 - classification_loss: 0.2878
1033/3000 [=========>....................] - ETA: 13:28 - loss: 0.7953 - regression_loss: 0.5074 - classification_loss: 0.2879
1034/3000 [=========>....................] - ETA: 13:27 - loss: 0.7951 - regression_loss: 0.5073 - classification_loss: 0.2878
1035/3000 [=========>....................] - ETA: 13:27 - loss: 0.7953 - regression_loss: 0.5074 - classification_loss: 0.2879
1036/3000 [=========>....................] - ETA: 13:27 - loss: 0.7949 - regression_loss: 0.5072 - classification_loss: 0.2877
1037/3000 [=========>....................] - ETA: 13:26 - loss: 0.7949 - regression_loss: 0.5072 - classification_loss: 0.2876
1038/3000 [=========>....................] - ETA: 13:26 - loss: 0.7947 - regression_loss: 0.5072 - classification_loss: 0.2875
1039/3000 [=========>....................] - ETA: 13:25 - loss: 0.7947 - regression_loss: 0.5072 - classification_loss: 0.2875
1040/3000 [=========>....................] - ETA: 13:25 - loss: 0.7942 - regression_loss: 0.5069 - classification_loss: 0.2874
1041/3000 [=========>....................] - ETA: 13:24 - loss: 0.7944 - regression_loss: 0.5070 - classification_loss: 0.2874
1042/3000 [=========>....................] - ETA: 13:24 - loss: 0.7939 - regression_loss: 0.5067 - classification_loss: 0.2872
1043/3000 [=========>....................] - ETA: 13:24 - loss: 0.7936 - regression_loss: 0.5065 - classification_loss: 0.2871
1044/3000 [=========>....................] - ETA: 13:23 - loss: 0.7937 - regression_loss: 0.5067 - classification_loss: 0.2870
1045/3000 [=========>....................] - ETA: 13:23 - loss: 0.7934 - regression_loss: 0.5065 - classification_loss: 0.2869
1046/3000 [=========>....................] - ETA: 13:22 - loss: 0.7931 - regression_loss: 0.5062 - classification_loss: 0.2869
1047/3000 [=========>....................] - ETA: 13:22 - loss: 0.7929 - regression_loss: 0.5061 - classification_loss: 0.2868
1048/3000 [=========>....................] - ETA: 13:22 - loss: 0.7928 - regression_loss: 0.5061 - classification_loss: 0.2867
1049/3000 [=========>....................] - ETA: 13:21 - loss: 0.7926 - regression_loss: 0.5058 - classification_loss: 0.2867
1050/3000 [=========>....................] - ETA: 13:21 - loss: 0.7923 - regression_loss: 0.5056 - classification_loss: 0.2867
1051/3000 [=========>....................] - ETA: 13:20 - loss: 0.7921 - regression_loss: 0.5056 - classification_loss: 0.2865
1052/3000 [=========>....................] - ETA: 13:20 - loss: 0.7919 - regression_loss: 0.5054 - classification_loss: 0.2865
1053/3000 [=========>....................] - ETA: 13:19 - loss: 0.7920 - regression_loss: 0.5055 - classification_loss: 0.2865
1054/3000 [=========>....................] - ETA: 13:19 - loss: 0.7918 - regression_loss: 0.5053 - classification_loss: 0.2865
1055/3000 [=========>....................] - ETA: 13:19 - loss: 0.7918 - regression_loss: 0.5052 - classification_loss: 0.2866
1056/3000 [=========>....................] - ETA: 13:18 - loss: 0.7921 - regression_loss: 0.5056 - classification_loss: 0.2865
1057/3000 [=========>....................] - ETA: 13:18 - loss: 0.7919 - regression_loss: 0.5056 - classification_loss: 0.2863
1058/3000 [=========>....................] - ETA: 13:17 - loss: 0.7919 - regression_loss: 0.5056 - classification_loss: 0.2863
1059/3000 [=========>....................] - ETA: 13:17 - loss: 0.7917 - regression_loss: 0.5054 - classification_loss: 0.2863
1060/3000 [=========>....................] - ETA: 13:17 - loss: 0.7915 - regression_loss: 0.5053 - classification_loss: 0.2863
1061/3000 [=========>....................] - ETA: 13:16 - loss: 0.7914 - regression_loss: 0.5051 - classification_loss: 0.2862
1062/3000 [=========>....................] - ETA: 13:16 - loss: 0.7913 - regression_loss: 0.5050 - classification_loss: 0.2863
1063/3000 [=========>....................] - ETA: 13:15 - loss: 0.7909 - regression_loss: 0.5048 - classification_loss: 0.2862
1064/3000 [=========>....................] - ETA: 13:15 - loss: 0.7909 - regression_loss: 0.5048 - classification_loss: 0.2861
1065/3000 [=========>....................] - ETA: 13:15 - loss: 0.7908 - regression_loss: 0.5047 - classification_loss: 0.2861
1066/3000 [=========>....................] - ETA: 13:14 - loss: 0.7907 - regression_loss: 0.5045 - classification_loss: 0.2862
1067/3000 [=========>....................] - ETA: 13:14 - loss: 0.7908 - regression_loss: 0.5046 - classification_loss: 0.2862
1068/3000 [=========>....................] - ETA: 13:13 - loss: 0.7907 - regression_loss: 0.5045 - classification_loss: 0.2862
1069/3000 [=========>....................] - ETA: 13:13 - loss: 0.7908 - regression_loss: 0.5046 - classification_loss: 0.2861
1070/3000 [=========>....................] - ETA: 13:13 - loss: 0.7907 - regression_loss: 0.5047 - classification_loss: 0.2860
1071/3000 [=========>....................] - ETA: 13:12 - loss: 0.7906 - regression_loss: 0.5047 - classification_loss: 0.2859
1072/3000 [=========>....................] - ETA: 13:12 - loss: 0.7907 - regression_loss: 0.5048 - classification_loss: 0.2859
1073/3000 [=========>....................] - ETA: 13:11 - loss: 0.7904 - regression_loss: 0.5046 - classification_loss: 0.2858
1074/3000 [=========>....................] - ETA: 13:11 - loss: 0.7901 - regression_loss: 0.5044 - classification_loss: 0.2858
1075/3000 [=========>....................] - ETA: 13:10 - loss: 0.7899 - regression_loss: 0.5042 - classification_loss: 0.2857
1076/3000 [=========>....................] - ETA: 13:10 - loss: 0.7896 - regression_loss: 0.5040 - classification_loss: 0.2856
1077/3000 [=========>....................] - ETA: 13:10 - loss: 0.7898 - regression_loss: 0.5042 - classification_loss: 0.2856
1078/3000 [=========>....................] - ETA: 13:09 - loss: 0.7899 - regression_loss: 0.5042 - classification_loss: 0.2857
1079/3000 [=========>....................] - ETA: 13:09 - loss: 0.7897 - regression_loss: 0.5041 - classification_loss: 0.2857
1080/3000 [=========>....................] - ETA: 13:08 - loss: 0.7895 - regression_loss: 0.5039 - classification_loss: 0.2856
1081/3000 [=========>....................] - ETA: 13:08 - loss: 0.7893 - regression_loss: 0.5038 - classification_loss: 0.2856
1082/3000 [=========>....................] - ETA: 13:07 - loss: 0.7890 - regression_loss: 0.5036 - classification_loss: 0.2854
1083/3000 [=========>....................] - ETA: 13:07 - loss: 0.7889 - regression_loss: 0.5033 - classification_loss: 0.2856
1084/3000 [=========>....................] - ETA: 13:07 - loss: 0.7886 - regression_loss: 0.5031 - classification_loss: 0.2855
1085/3000 [=========>....................] - ETA: 13:06 - loss: 0.7883 - regression_loss: 0.5028 - classification_loss: 0.2855
1086/3000 [=========>....................] - ETA: 13:06 - loss: 0.7880 - regression_loss: 0.5025 - classification_loss: 0.2855
1087/3000 [=========>....................] - ETA: 13:05 - loss: 0.7877 - regression_loss: 0.5024 - classification_loss: 0.2853
1088/3000 [=========>....................] - ETA: 13:05 - loss: 0.7878 - regression_loss: 0.5024 - classification_loss: 0.2854
1089/3000 [=========>....................] - ETA: 13:05 - loss: 0.7875 - regression_loss: 0.5022 - classification_loss: 0.2853
1090/3000 [=========>....................] - ETA: 13:04 - loss: 0.7873 - regression_loss: 0.5021 - classification_loss: 0.2852
1091/3000 [=========>....................] - ETA: 13:04 - loss: 0.7872 - regression_loss: 0.5019 - classification_loss: 0.2852
1092/3000 [=========>....................] - ETA: 13:03 - loss: 0.7873 - regression_loss: 0.5022 - classification_loss: 0.2851
1093/3000 [=========>....................] - ETA: 13:03 - loss: 0.7871 - regression_loss: 0.5021 - classification_loss: 0.2849
1094/3000 [=========>....................] - ETA: 13:02 - loss: 0.7874 - regression_loss: 0.5025 - classification_loss: 0.2849
1095/3000 [=========>....................] - ETA: 13:02 - loss: 0.7875 - regression_loss: 0.5028 - classification_loss: 0.2848
1096/3000 [=========>....................] - ETA: 13:02 - loss: 0.7873 - regression_loss: 0.5027 - classification_loss: 0.2846
1097/3000 [=========>....................] - ETA: 13:01 - loss: 0.7873 - regression_loss: 0.5027 - classification_loss: 0.2846
1098/3000 [=========>....................] - ETA: 13:01 - loss: 0.7872 - regression_loss: 0.5026 - classification_loss: 0.2846
1099/3000 [=========>....................] - ETA: 13:00 - loss: 0.7867 - regression_loss: 0.5023 - classification_loss: 0.2844
1100/3000 [==========>...................] - ETA: 13:00 - loss: 0.7868 - regression_loss: 0.5024 - classification_loss: 0.2844
1101/3000 [==========>...................] - ETA: 13:00 - loss: 0.7866 - regression_loss: 0.5023 - classification_loss: 0.2843
1102/3000 [==========>...................] - ETA: 12:59 - loss: 0.7866 - regression_loss: 0.5023 - classification_loss: 0.2843
1103/3000 [==========>...................] - ETA: 12:59 - loss: 0.7864 - regression_loss: 0.5022 - classification_loss: 0.2842
1104/3000 [==========>...................] - ETA: 12:58 - loss: 0.7862 - regression_loss: 0.5021 - classification_loss: 0.2841
1105/3000 [==========>...................] - ETA: 12:58 - loss: 0.7863 - regression_loss: 0.5022 - classification_loss: 0.2840
1106/3000 [==========>...................] - ETA: 12:57 - loss: 0.7862 - regression_loss: 0.5022 - classification_loss: 0.2840
1107/3000 [==========>...................] - ETA: 12:57 - loss: 0.7862 - regression_loss: 0.5021 - classification_loss: 0.2841
1108/3000 [==========>...................] - ETA: 12:57 - loss: 0.7860 - regression_loss: 0.5020 - classification_loss: 0.2840
1109/3000 [==========>...................] - ETA: 12:56 - loss: 0.7858 - regression_loss: 0.5019 - classification_loss: 0.2839
1110/3000 [==========>...................] - ETA: 12:56 - loss: 0.7858 - regression_loss: 0.5019 - classification_loss: 0.2839
1111/3000 [==========>...................] - ETA: 12:55 - loss: 0.7857 - regression_loss: 0.5017 - classification_loss: 0.2840
1112/3000 [==========>...................] - ETA: 12:55 - loss: 0.7856 - regression_loss: 0.5017 - classification_loss: 0.2839
1113/3000 [==========>...................] - ETA: 12:55 - loss: 0.7855 - regression_loss: 0.5016 - classification_loss: 0.2839
1114/3000 [==========>...................] - ETA: 12:54 - loss: 0.7855 - regression_loss: 0.5016 - classification_loss: 0.2838
1115/3000 [==========>...................] - ETA: 12:54 - loss: 0.7857 - regression_loss: 0.5018 - classification_loss: 0.2839
1116/3000 [==========>...................] - ETA: 12:53 - loss: 0.7856 - regression_loss: 0.5017 - classification_loss: 0.2839
1117/3000 [==========>...................] - ETA: 12:53 - loss: 0.7853 - regression_loss: 0.5015 - classification_loss: 0.2837
1118/3000 [==========>...................] - ETA: 12:53 - loss: 0.7852 - regression_loss: 0.5015 - classification_loss: 0.2837
1119/3000 [==========>...................] - ETA: 12:52 - loss: 0.7850 - regression_loss: 0.5013 - classification_loss: 0.2837
1120/3000 [==========>...................] - ETA: 12:52 - loss: 0.7850 - regression_loss: 0.5013 - classification_loss: 0.2836
1121/3000 [==========>...................] - ETA: 12:51 - loss: 0.7847 - regression_loss: 0.5011 - classification_loss: 0.2836
1122/3000 [==========>...................] - ETA: 12:51 - loss: 0.7846 - regression_loss: 0.5012 - classification_loss: 0.2834
1123/3000 [==========>...................] - ETA: 12:51 - loss: 0.7848 - regression_loss: 0.5013 - classification_loss: 0.2835
1124/3000 [==========>...................] - ETA: 12:50 - loss: 0.7847 - regression_loss: 0.5012 - classification_loss: 0.2835
1125/3000 [==========>...................] - ETA: 12:50 - loss: 0.7848 - regression_loss: 0.5013 - classification_loss: 0.2835
1126/3000 [==========>...................] - ETA: 12:49 - loss: 0.7845 - regression_loss: 0.5011 - classification_loss: 0.2834
1127/3000 [==========>...................] - ETA: 12:49 - loss: 0.7843 - regression_loss: 0.5010 - classification_loss: 0.2833
1128/3000 [==========>...................] - ETA: 12:49 - loss: 0.7841 - regression_loss: 0.5009 - classification_loss: 0.2832
1129/3000 [==========>...................] - ETA: 12:48 - loss: 0.7839 - regression_loss: 0.5008 - classification_loss: 0.2832
1130/3000 [==========>...................] - ETA: 12:48 - loss: 0.7837 - regression_loss: 0.5005 - classification_loss: 0.2832
1131/3000 [==========>...................] - ETA: 12:47 - loss: 0.7836 - regression_loss: 0.5006 - classification_loss: 0.2830
1132/3000 [==========>...................] - ETA: 12:47 - loss: 0.7837 - regression_loss: 0.5007 - classification_loss: 0.2830
1133/3000 [==========>...................] - ETA: 12:46 - loss: 0.7836 - regression_loss: 0.5007 - classification_loss: 0.2829
1134/3000 [==========>...................] - ETA: 12:46 - loss: 0.7835 - regression_loss: 0.5007 - classification_loss: 0.2828
1135/3000 [==========>...................] - ETA: 12:46 - loss: 0.7835 - regression_loss: 0.5007 - classification_loss: 0.2828
1136/3000 [==========>...................] - ETA: 12:45 - loss: 0.7835 - regression_loss: 0.5006 - classification_loss: 0.2829
1137/3000 [==========>...................] - ETA: 12:45 - loss: 0.7835 - regression_loss: 0.5007 - classification_loss: 0.2828
1138/3000 [==========>...................] - ETA: 12:44 - loss: 0.7835 - regression_loss: 0.5007 - classification_loss: 0.2828
1139/3000 [==========>...................] - ETA: 12:44 - loss: 0.7833 - regression_loss: 0.5005 - classification_loss: 0.2829
1140/3000 [==========>...................] - ETA: 12:44 - loss: 0.7832 - regression_loss: 0.5004 - classification_loss: 0.2828
1141/3000 [==========>...................] - ETA: 12:43 - loss: 0.7832 - regression_loss: 0.5005 - classification_loss: 0.2827
1142/3000 [==========>...................] - ETA: 12:43 - loss: 0.7831 - regression_loss: 0.5004 - classification_loss: 0.2827
1143/3000 [==========>...................] - ETA: 12:42 - loss: 0.7829 - regression_loss: 0.5003 - classification_loss: 0.2826
1144/3000 [==========>...................] - ETA: 12:42 - loss: 0.7830 - regression_loss: 0.5004 - classification_loss: 0.2826
1145/3000 [==========>...................] - ETA: 12:42 - loss: 0.7828 - regression_loss: 0.5003 - classification_loss: 0.2825
1146/3000 [==========>...................] - ETA: 12:41 - loss: 0.7826 - regression_loss: 0.5001 - classification_loss: 0.2825
1147/3000 [==========>...................] - ETA: 12:41 - loss: 0.7825 - regression_loss: 0.5000 - classification_loss: 0.2825
1148/3000 [==========>...................] - ETA: 12:40 - loss: 0.7823 - regression_loss: 0.4999 - classification_loss: 0.2824
1149/3000 [==========>...................] - ETA: 12:40 - loss: 0.7821 - regression_loss: 0.4997 - classification_loss: 0.2824
1150/3000 [==========>...................] - ETA: 12:39 - loss: 0.7820 - regression_loss: 0.4996 - classification_loss: 0.2824
1151/3000 [==========>...................] - ETA: 12:39 - loss: 0.7819 - regression_loss: 0.4994 - classification_loss: 0.2825
1152/3000 [==========>...................] - ETA: 12:39 - loss: 0.7815 - regression_loss: 0.4991 - classification_loss: 0.2824
1153/3000 [==========>...................] - ETA: 12:38 - loss: 0.7818 - regression_loss: 0.4994 - classification_loss: 0.2823
1154/3000 [==========>...................] - ETA: 12:38 - loss: 0.7816 - regression_loss: 0.4993 - classification_loss: 0.2823
1155/3000 [==========>...................] - ETA: 12:37 - loss: 0.7817 - regression_loss: 0.4994 - classification_loss: 0.2823
1156/3000 [==========>...................] - ETA: 12:37 - loss: 0.7817 - regression_loss: 0.4995 - classification_loss: 0.2822
1157/3000 [==========>...................] - ETA: 12:37 - loss: 0.7815 - regression_loss: 0.4993 - classification_loss: 0.2822
1158/3000 [==========>...................] - ETA: 12:36 - loss: 0.7814 - regression_loss: 0.4993 - classification_loss: 0.2822
1159/3000 [==========>...................] - ETA: 12:36 - loss: 0.7812 - regression_loss: 0.4991 - classification_loss: 0.2821
1160/3000 [==========>...................] - ETA: 12:35 - loss: 0.7812 - regression_loss: 0.4991 - classification_loss: 0.2821
1161/3000 [==========>...................] - ETA: 12:35 - loss: 0.7809 - regression_loss: 0.4989 - classification_loss: 0.2820
1162/3000 [==========>...................] - ETA: 12:34 - loss: 0.7807 - regression_loss: 0.4987 - classification_loss: 0.2820
1163/3000 [==========>...................] - ETA: 12:34 - loss: 0.7809 - regression_loss: 0.4989 - classification_loss: 0.2819
1164/3000 [==========>...................] - ETA: 12:34 - loss: 0.7808 - regression_loss: 0.4988 - classification_loss: 0.2820
1165/3000 [==========>...................] - ETA: 12:33 - loss: 0.7808 - regression_loss: 0.4989 - classification_loss: 0.2819
1166/3000 [==========>...................] - ETA: 12:33 - loss: 0.7809 - regression_loss: 0.4990 - classification_loss: 0.2819
1167/3000 [==========>...................] - ETA: 12:32 - loss: 0.7808 - regression_loss: 0.4989 - classification_loss: 0.2819
1168/3000 [==========>...................] - ETA: 12:32 - loss: 0.7808 - regression_loss: 0.4989 - classification_loss: 0.2819
1169/3000 [==========>...................] - ETA: 12:32 - loss: 0.7806 - regression_loss: 0.4988 - classification_loss: 0.2818
1170/3000 [==========>...................] - ETA: 12:31 - loss: 0.7805 - regression_loss: 0.4988 - classification_loss: 0.2817
1171/3000 [==========>...................] - ETA: 12:31 - loss: 0.7806 - regression_loss: 0.4989 - classification_loss: 0.2817
1172/3000 [==========>...................] - ETA: 12:30 - loss: 0.7804 - regression_loss: 0.4987 - classification_loss: 0.2817
1173/3000 [==========>...................] - ETA: 12:30 - loss: 0.7806 - regression_loss: 0.4989 - classification_loss: 0.2816
1174/3000 [==========>...................] - ETA: 12:30 - loss: 0.7806 - regression_loss: 0.4988 - classification_loss: 0.2817
1175/3000 [==========>...................] - ETA: 12:29 - loss: 0.7804 - regression_loss: 0.4987 - classification_loss: 0.2817
1176/3000 [==========>...................] - ETA: 12:29 - loss: 0.7801 - regression_loss: 0.4985 - classification_loss: 0.2816
1177/3000 [==========>...................] - ETA: 12:28 - loss: 0.7799 - regression_loss: 0.4983 - classification_loss: 0.2816
1178/3000 [==========>...................] - ETA: 12:28 - loss: 0.7796 - regression_loss: 0.4980 - classification_loss: 0.2815
1179/3000 [==========>...................] - ETA: 12:27 - loss: 0.7793 - regression_loss: 0.4978 - classification_loss: 0.2815
1180/3000 [==========>...................] - ETA: 12:27 - loss: 0.7794 - regression_loss: 0.4979 - classification_loss: 0.2815
1181/3000 [==========>...................] - ETA: 12:27 - loss: 0.7792 - regression_loss: 0.4977 - classification_loss: 0.2815
1182/3000 [==========>...................] - ETA: 12:26 - loss: 0.7790 - regression_loss: 0.4977 - classification_loss: 0.2813
1183/3000 [==========>...................] - ETA: 12:26 - loss: 0.7790 - regression_loss: 0.4977 - classification_loss: 0.2813
1184/3000 [==========>...................] - ETA: 12:25 - loss: 0.7786 - regression_loss: 0.4975 - classification_loss: 0.2812
1185/3000 [==========>...................] - ETA: 12:25 - loss: 0.7785 - regression_loss: 0.4974 - classification_loss: 0.2812
1186/3000 [==========>...................] - ETA: 12:25 - loss: 0.7784 - regression_loss: 0.4972 - classification_loss: 0.2812
1187/3000 [==========>...................] - ETA: 12:24 - loss: 0.7782 - regression_loss: 0.4971 - classification_loss: 0.2811
1188/3000 [==========>...................] - ETA: 12:24 - loss: 0.7784 - regression_loss: 0.4974 - classification_loss: 0.2810
1189/3000 [==========>...................] - ETA: 12:23 - loss: 0.7783 - regression_loss: 0.4973 - classification_loss: 0.2810
1190/3000 [==========>...................] - ETA: 12:23 - loss: 0.7786 - regression_loss: 0.4975 - classification_loss: 0.2811
1191/3000 [==========>...................] - ETA: 12:23 - loss: 0.7787 - regression_loss: 0.4976 - classification_loss: 0.2811
1192/3000 [==========>...................] - ETA: 12:22 - loss: 0.7788 - regression_loss: 0.4976 - classification_loss: 0.2812
1193/3000 [==========>...................] - ETA: 12:22 - loss: 0.7789 - regression_loss: 0.4977 - classification_loss: 0.2812
1194/3000 [==========>...................] - ETA: 12:21 - loss: 0.7787 - regression_loss: 0.4976 - classification_loss: 0.2811
1195/3000 [==========>...................] - ETA: 12:21 - loss: 0.7788 - regression_loss: 0.4977 - classification_loss: 0.2811
1196/3000 [==========>...................] - ETA: 12:20 - loss: 0.7788 - regression_loss: 0.4978 - classification_loss: 0.2810
1197/3000 [==========>...................] - ETA: 12:20 - loss: 0.7788 - regression_loss: 0.4977 - classification_loss: 0.2811
1198/3000 [==========>...................] - ETA: 12:20 - loss: 0.7786 - regression_loss: 0.4976 - classification_loss: 0.2810
1199/3000 [==========>...................] - ETA: 12:19 - loss: 0.7786 - regression_loss: 0.4976 - classification_loss: 0.2810
1200/3000 [===========>..................] - ETA: 12:19 - loss: 0.7785 - regression_loss: 0.4977 - classification_loss: 0.2808
1201/3000 [===========>..................] - ETA: 12:18 - loss: 0.7785 - regression_loss: 0.4977 - classification_loss: 0.2808
1202/3000 [===========>..................] - ETA: 12:18 - loss: 0.7784 - regression_loss: 0.4976 - classification_loss: 0.2808
1203/3000 [===========>..................] - ETA: 12:18 - loss: 0.7782 - regression_loss: 0.4973 - classification_loss: 0.2808
1204/3000 [===========>..................] - ETA: 12:17 - loss: 0.7779 - regression_loss: 0.4971 - classification_loss: 0.2808
1205/3000 [===========>..................] - ETA: 12:17 - loss: 0.7776 - regression_loss: 0.4970 - classification_loss: 0.2807
1206/3000 [===========>..................] - ETA: 12:16 - loss: 0.7778 - regression_loss: 0.4971 - classification_loss: 0.2807
1207/3000 [===========>..................] - ETA: 12:16 - loss: 0.7778 - regression_loss: 0.4970 - classification_loss: 0.2807
1208/3000 [===========>..................] - ETA: 12:16 - loss: 0.7777 - regression_loss: 0.4971 - classification_loss: 0.2806
1209/3000 [===========>..................] - ETA: 12:15 - loss: 0.7775 - regression_loss: 0.4969 - classification_loss: 0.2805
1210/3000 [===========>..................] - ETA: 12:15 - loss: 0.7776 - regression_loss: 0.4970 - classification_loss: 0.2806
1211/3000 [===========>..................] - ETA: 12:14 - loss: 0.7773 - regression_loss: 0.4969 - classification_loss: 0.2804
1212/3000 [===========>..................] - ETA: 12:14 - loss: 0.7772 - regression_loss: 0.4967 - classification_loss: 0.2805
1213/3000 [===========>..................] - ETA: 12:14 - loss: 0.7769 - regression_loss: 0.4965 - classification_loss: 0.2804
1214/3000 [===========>..................] - ETA: 12:13 - loss: 0.7767 - regression_loss: 0.4964 - classification_loss: 0.2804
1215/3000 [===========>..................] - ETA: 12:13 - loss: 0.7769 - regression_loss: 0.4966 - classification_loss: 0.2803
1216/3000 [===========>..................] - ETA: 12:12 - loss: 0.7769 - regression_loss: 0.4966 - classification_loss: 0.2803
1217/3000 [===========>..................] - ETA: 12:12 - loss: 0.7766 - regression_loss: 0.4964 - classification_loss: 0.2802
1218/3000 [===========>..................] - ETA: 12:11 - loss: 0.7766 - regression_loss: 0.4964 - classification_loss: 0.2802
1219/3000 [===========>..................] - ETA: 12:11 - loss: 0.7764 - regression_loss: 0.4963 - classification_loss: 0.2801
1220/3000 [===========>..................] - ETA: 12:11 - loss: 0.7762 - regression_loss: 0.4962 - classification_loss: 0.2800
1221/3000 [===========>..................] - ETA: 12:10 - loss: 0.7766 - regression_loss: 0.4966 - classification_loss: 0.2800
1222/3000 [===========>..................] - ETA: 12:10 - loss: 0.7763 - regression_loss: 0.4964 - classification_loss: 0.2799
1223/3000 [===========>..................] - ETA: 12:09 - loss: 0.7764 - regression_loss: 0.4964 - classification_loss: 0.2799
1224/3000 [===========>..................] - ETA: 12:09 - loss: 0.7763 - regression_loss: 0.4965 - classification_loss: 0.2798
1225/3000 [===========>..................] - ETA: 12:09 - loss: 0.7765 - regression_loss: 0.4967 - classification_loss: 0.2798
1226/3000 [===========>..................] - ETA: 12:08 - loss: 0.7766 - regression_loss: 0.4968 - classification_loss: 0.2797
1227/3000 [===========>..................] - ETA: 12:08 - loss: 0.7766 - regression_loss: 0.4969 - classification_loss: 0.2797
1228/3000 [===========>..................] - ETA: 12:07 - loss: 0.7766 - regression_loss: 0.4969 - classification_loss: 0.2797
1229/3000 [===========>..................] - ETA: 12:07 - loss: 0.7766 - regression_loss: 0.4969 - classification_loss: 0.2797
1230/3000 [===========>..................] - ETA: 12:07 - loss: 0.7763 - regression_loss: 0.4967 - classification_loss: 0.2796
1231/3000 [===========>..................] - ETA: 12:06 - loss: 0.7761 - regression_loss: 0.4965 - classification_loss: 0.2796
1232/3000 [===========>..................] - ETA: 12:06 - loss: 0.7759 - regression_loss: 0.4963 - classification_loss: 0.2796
1233/3000 [===========>..................] - ETA: 12:05 - loss: 0.7758 - regression_loss: 0.4962 - classification_loss: 0.2796
1234/3000 [===========>..................] - ETA: 12:05 - loss: 0.7756 - regression_loss: 0.4960 - classification_loss: 0.2795
1235/3000 [===========>..................] - ETA: 12:04 - loss: 0.7755 - regression_loss: 0.4960 - classification_loss: 0.2795
1236/3000 [===========>..................] - ETA: 12:04 - loss: 0.7754 - regression_loss: 0.4960 - classification_loss: 0.2794
1237/3000 [===========>..................] - ETA: 12:04 - loss: 0.7754 - regression_loss: 0.4961 - classification_loss: 0.2794
1238/3000 [===========>..................] - ETA: 12:03 - loss: 0.7752 - regression_loss: 0.4959 - classification_loss: 0.2793
1239/3000 [===========>..................] - ETA: 12:03 - loss: 0.7750 - regression_loss: 0.4957 - classification_loss: 0.2792
1240/3000 [===========>..................] - ETA: 12:02 - loss: 0.7748 - regression_loss: 0.4956 - classification_loss: 0.2792
1241/3000 [===========>..................] - ETA: 12:02 - loss: 0.7748 - regression_loss: 0.4956 - classification_loss: 0.2792
1242/3000 [===========>..................] - ETA: 12:02 - loss: 0.7749 - regression_loss: 0.4958 - classification_loss: 0.2791
1243/3000 [===========>..................] - ETA: 12:01 - loss: 0.7749 - regression_loss: 0.4959 - classification_loss: 0.2790
1244/3000 [===========>..................] - ETA: 12:01 - loss: 0.7747 - regression_loss: 0.4958 - classification_loss: 0.2790
1245/3000 [===========>..................] - ETA: 12:00 - loss: 0.7747 - regression_loss: 0.4957 - classification_loss: 0.2790
1246/3000 [===========>..................] - ETA: 12:00 - loss: 0.7747 - regression_loss: 0.4958 - classification_loss: 0.2789
1247/3000 [===========>..................] - ETA: 11:59 - loss: 0.7744 - regression_loss: 0.4957 - classification_loss: 0.2787
1248/3000 [===========>..................] - ETA: 11:59 - loss: 0.7744 - regression_loss: 0.4957 - classification_loss: 0.2787
1249/3000 [===========>..................] - ETA: 11:59 - loss: 0.7743 - regression_loss: 0.4956 - classification_loss: 0.2787
1250/3000 [===========>..................] - ETA: 11:58 - loss: 0.7742 - regression_loss: 0.4955 - classification_loss: 0.2787
1251/3000 [===========>..................] - ETA: 11:58 - loss: 0.7739 - regression_loss: 0.4954 - classification_loss: 0.2785
1252/3000 [===========>..................] - ETA: 11:57 - loss: 0.7740 - regression_loss: 0.4954 - classification_loss: 0.2786
1253/3000 [===========>..................] - ETA: 11:57 - loss: 0.7740 - regression_loss: 0.4955 - classification_loss: 0.2784
1254/3000 [===========>..................] - ETA: 11:57 - loss: 0.7739 - regression_loss: 0.4956 - classification_loss: 0.2783
1255/3000 [===========>..................] - ETA: 11:56 - loss: 0.7739 - regression_loss: 0.4957 - classification_loss: 0.2782
1256/3000 [===========>..................] - ETA: 11:56 - loss: 0.7737 - regression_loss: 0.4956 - classification_loss: 0.2781
1257/3000 [===========>..................] - ETA: 11:55 - loss: 0.7738 - regression_loss: 0.4958 - classification_loss: 0.2780
1258/3000 [===========>..................] - ETA: 11:55 - loss: 0.7736 - regression_loss: 0.4957 - classification_loss: 0.2779
1259/3000 [===========>..................] - ETA: 11:55 - loss: 0.7735 - regression_loss: 0.4957 - classification_loss: 0.2778
1260/3000 [===========>..................] - ETA: 11:54 - loss: 0.7736 - regression_loss: 0.4958 - classification_loss: 0.2778
1261/3000 [===========>..................] - ETA: 11:54 - loss: 0.7734 - regression_loss: 0.4957 - classification_loss: 0.2778
1262/3000 [===========>..................] - ETA: 11:53 - loss: 0.7733 - regression_loss: 0.4955 - classification_loss: 0.2778
1263/3000 [===========>..................] - ETA: 11:53 - loss: 0.7731 - regression_loss: 0.4953 - classification_loss: 0.2778
1264/3000 [===========>..................] - ETA: 11:53 - loss: 0.7729 - regression_loss: 0.4952 - classification_loss: 0.2777
1265/3000 [===========>..................] - ETA: 11:52 - loss: 0.7725 - regression_loss: 0.4950 - classification_loss: 0.2775
1266/3000 [===========>..................] - ETA: 11:52 - loss: 0.7726 - regression_loss: 0.4950 - classification_loss: 0.2776
1267/3000 [===========>..................] - ETA: 11:51 - loss: 0.7725 - regression_loss: 0.4949 - classification_loss: 0.2776
1268/3000 [===========>..................] - ETA: 11:51 - loss: 0.7725 - regression_loss: 0.4948 - classification_loss: 0.2776
1269/3000 [===========>..................] - ETA: 11:51 - loss: 0.7725 - regression_loss: 0.4948 - classification_loss: 0.2776
1270/3000 [===========>..................] - ETA: 11:50 - loss: 0.7721 - regression_loss: 0.4946 - classification_loss: 0.2775
1271/3000 [===========>..................] - ETA: 11:50 - loss: 0.7721 - regression_loss: 0.4947 - classification_loss: 0.2775
1272/3000 [===========>..................] - ETA: 11:49 - loss: 0.7720 - regression_loss: 0.4946 - classification_loss: 0.2774
1273/3000 [===========>..................] - ETA: 11:49 - loss: 0.7719 - regression_loss: 0.4945 - classification_loss: 0.2774
1274/3000 [===========>..................] - ETA: 11:48 - loss: 0.7717 - regression_loss: 0.4944 - classification_loss: 0.2773
1275/3000 [===========>..................] - ETA: 11:48 - loss: 0.7717 - regression_loss: 0.4944 - classification_loss: 0.2773
1276/3000 [===========>..................] - ETA: 11:48 - loss: 0.7713 - regression_loss: 0.4941 - classification_loss: 0.2772
1277/3000 [===========>..................] - ETA: 11:47 - loss: 0.7714 - regression_loss: 0.4943 - classification_loss: 0.2771
1278/3000 [===========>..................] - ETA: 11:47 - loss: 0.7714 - regression_loss: 0.4944 - classification_loss: 0.2771
1279/3000 [===========>..................] - ETA: 11:46 - loss: 0.7716 - regression_loss: 0.4945 - classification_loss: 0.2771
1280/3000 [===========>..................] - ETA: 11:46 - loss: 0.7718 - regression_loss: 0.4947 - classification_loss: 0.2771
1281/3000 [===========>..................] - ETA: 11:46 - loss: 0.7719 - regression_loss: 0.4948 - classification_loss: 0.2771
1282/3000 [===========>..................] - ETA: 11:45 - loss: 0.7718 - regression_loss: 0.4948 - classification_loss: 0.2770
1283/3000 [===========>..................] - ETA: 11:45 - loss: 0.7716 - regression_loss: 0.4946 - classification_loss: 0.2770
1284/3000 [===========>..................] - ETA: 11:44 - loss: 0.7716 - regression_loss: 0.4946 - classification_loss: 0.2770
1285/3000 [===========>..................] - ETA: 11:44 - loss: 0.7715 - regression_loss: 0.4946 - classification_loss: 0.2769
1286/3000 [===========>..................] - ETA: 11:44 - loss: 0.7713 - regression_loss: 0.4945 - classification_loss: 0.2768
1287/3000 [===========>..................] - ETA: 11:43 - loss: 0.7711 - regression_loss: 0.4943 - classification_loss: 0.2768
1288/3000 [===========>..................] - ETA: 11:43 - loss: 0.7710 - regression_loss: 0.4943 - classification_loss: 0.2768
1289/3000 [===========>..................] - ETA: 11:42 - loss: 0.7707 - regression_loss: 0.4941 - classification_loss: 0.2766
1290/3000 [===========>..................] - ETA: 11:42 - loss: 0.7705 - regression_loss: 0.4940 - classification_loss: 0.2765
1291/3000 [===========>..................] - ETA: 11:41 - loss: 0.7704 - regression_loss: 0.4939 - classification_loss: 0.2765
1292/3000 [===========>..................] - ETA: 11:41 - loss: 0.7704 - regression_loss: 0.4940 - classification_loss: 0.2764
1293/3000 [===========>..................] - ETA: 11:41 - loss: 0.7704 - regression_loss: 0.4941 - classification_loss: 0.2763
1294/3000 [===========>..................] - ETA: 11:40 - loss: 0.7703 - regression_loss: 0.4939 - classification_loss: 0.2764
1295/3000 [===========>..................] - ETA: 11:40 - loss: 0.7703 - regression_loss: 0.4939 - classification_loss: 0.2764
1296/3000 [===========>..................] - ETA: 11:39 - loss: 0.7701 - regression_loss: 0.4937 - classification_loss: 0.2764
1297/3000 [===========>..................] - ETA: 11:39 - loss: 0.7699 - regression_loss: 0.4936 - classification_loss: 0.2763
1298/3000 [===========>..................] - ETA: 11:39 - loss: 0.7700 - regression_loss: 0.4937 - classification_loss: 0.2763
1299/3000 [===========>..................] - ETA: 11:38 - loss: 0.7699 - regression_loss: 0.4936 - classification_loss: 0.2763
1300/3000 [============>.................] - ETA: 11:38 - loss: 0.7698 - regression_loss: 0.4936 - classification_loss: 0.2763
1301/3000 [============>.................] - ETA: 11:37 - loss: 0.7699 - regression_loss: 0.4937 - classification_loss: 0.2761
1302/3000 [============>.................] - ETA: 11:37 - loss: 0.7699 - regression_loss: 0.4938 - classification_loss: 0.2761
1303/3000 [============>.................] - ETA: 11:37 - loss: 0.7698 - regression_loss: 0.4937 - classification_loss: 0.2761
1304/3000 [============>.................] - ETA: 11:36 - loss: 0.7698 - regression_loss: 0.4937 - classification_loss: 0.2761
1305/3000 [============>.................] - ETA: 11:36 - loss: 0.7697 - regression_loss: 0.4936 - classification_loss: 0.2761
1306/3000 [============>.................] - ETA: 11:35 - loss: 0.7698 - regression_loss: 0.4937 - classification_loss: 0.2761
1307/3000 [============>.................] - ETA: 11:35 - loss: 0.7697 - regression_loss: 0.4936 - classification_loss: 0.2761
1308/3000 [============>.................] - ETA: 11:35 - loss: 0.7698 - regression_loss: 0.4937 - classification_loss: 0.2760
1309/3000 [============>.................] - ETA: 11:34 - loss: 0.7696 - regression_loss: 0.4936 - classification_loss: 0.2760
1310/3000 [============>.................] - ETA: 11:34 - loss: 0.7695 - regression_loss: 0.4935 - classification_loss: 0.2760
1311/3000 [============>.................] - ETA: 11:33 - loss: 0.7693 - regression_loss: 0.4934 - classification_loss: 0.2760
1312/3000 [============>.................] - ETA: 11:33 - loss: 0.7696 - regression_loss: 0.4937 - classification_loss: 0.2759
1313/3000 [============>.................] - ETA: 11:32 - loss: 0.7694 - regression_loss: 0.4936 - classification_loss: 0.2759
1314/3000 [============>.................] - ETA: 11:32 - loss: 0.7694 - regression_loss: 0.4936 - classification_loss: 0.2758
1315/3000 [============>.................] - ETA: 11:32 - loss: 0.7693 - regression_loss: 0.4936 - classification_loss: 0.2757
1316/3000 [============>.................] - ETA: 11:31 - loss: 0.7695 - regression_loss: 0.4937 - classification_loss: 0.2757
1317/3000 [============>.................] - ETA: 11:31 - loss: 0.7692 - regression_loss: 0.4935 - classification_loss: 0.2757
1318/3000 [============>.................] - ETA: 11:30 - loss: 0.7693 - regression_loss: 0.4936 - classification_loss: 0.2757
1319/3000 [============>.................] - ETA: 11:30 - loss: 0.7691 - regression_loss: 0.4934 - classification_loss: 0.2757
1320/3000 [============>.................] - ETA: 11:30 - loss: 0.7691 - regression_loss: 0.4933 - classification_loss: 0.2758
1321/3000 [============>.................] - ETA: 11:29 - loss: 0.7689 - regression_loss: 0.4931 - classification_loss: 0.2757
1322/3000 [============>.................] - ETA: 11:29 - loss: 0.7687 - regression_loss: 0.4930 - classification_loss: 0.2757
1323/3000 [============>.................] - ETA: 11:28 - loss: 0.7685 - regression_loss: 0.4928 - classification_loss: 0.2757
1324/3000 [============>.................] - ETA: 11:28 - loss: 0.7686 - regression_loss: 0.4929 - classification_loss: 0.2757
1325/3000 [============>.................] - ETA: 11:27 - loss: 0.7686 - regression_loss: 0.4928 - classification_loss: 0.2757
1326/3000 [============>.................] - ETA: 11:27 - loss: 0.7684 - regression_loss: 0.4927 - classification_loss: 0.2757
1327/3000 [============>.................] - ETA: 11:27 - loss: 0.7683 - regression_loss: 0.4926 - classification_loss: 0.2757
1328/3000 [============>.................] - ETA: 11:26 - loss: 0.7683 - regression_loss: 0.4926 - classification_loss: 0.2756
1329/3000 [============>.................] - ETA: 11:26 - loss: 0.7685 - regression_loss: 0.4929 - classification_loss: 0.2756
1330/3000 [============>.................] - ETA: 11:25 - loss: 0.7684 - regression_loss: 0.4929 - classification_loss: 0.2755
1331/3000 [============>.................] - ETA: 11:25 - loss: 0.7687 - regression_loss: 0.4932 - classification_loss: 0.2756
1332/3000 [============>.................] - ETA: 11:25 - loss: 0.7686 - regression_loss: 0.4931 - classification_loss: 0.2756
1333/3000 [============>.................] - ETA: 11:24 - loss: 0.7687 - regression_loss: 0.4931 - classification_loss: 0.2755
1334/3000 [============>.................] - ETA: 11:24 - loss: 0.7686 - regression_loss: 0.4931 - classification_loss: 0.2755
1335/3000 [============>.................] - ETA: 11:23 - loss: 0.7682 - regression_loss: 0.4929 - classification_loss: 0.2753
1336/3000 [============>.................] - ETA: 11:23 - loss: 0.7681 - regression_loss: 0.4927 - classification_loss: 0.2753
1337/3000 [============>.................] - ETA: 11:23 - loss: 0.7682 - regression_loss: 0.4929 - classification_loss: 0.2753
1338/3000 [============>.................] - ETA: 11:22 - loss: 0.7681 - regression_loss: 0.4928 - classification_loss: 0.2752
1339/3000 [============>.................] - ETA: 11:22 - loss: 0.7681 - regression_loss: 0.4929 - classification_loss: 0.2752
1340/3000 [============>.................] - ETA: 11:21 - loss: 0.7679 - regression_loss: 0.4928 - classification_loss: 0.2751
1341/3000 [============>.................] - ETA: 11:21 - loss: 0.7678 - regression_loss: 0.4926 - classification_loss: 0.2752
1342/3000 [============>.................] - ETA: 11:21 - loss: 0.7675 - regression_loss: 0.4924 - classification_loss: 0.2751
1343/3000 [============>.................] - ETA: 11:20 - loss: 0.7673 - regression_loss: 0.4923 - classification_loss: 0.2750
1344/3000 [============>.................] - ETA: 11:20 - loss: 0.7674 - regression_loss: 0.4924 - classification_loss: 0.2750
1345/3000 [============>.................] - ETA: 11:19 - loss: 0.7674 - regression_loss: 0.4924 - classification_loss: 0.2751
1346/3000 [============>.................] - ETA: 11:19 - loss: 0.7675 - regression_loss: 0.4924 - classification_loss: 0.2750
1347/3000 [============>.................] - ETA: 11:18 - loss: 0.7675 - regression_loss: 0.4925 - classification_loss: 0.2750
1348/3000 [============>.................] - ETA: 11:18 - loss: 0.7675 - regression_loss: 0.4925 - classification_loss: 0.2750
1349/3000 [============>.................] - ETA: 11:18 - loss: 0.7674 - regression_loss: 0.4925 - classification_loss: 0.2750
1350/3000 [============>.................] - ETA: 11:17 - loss: 0.7674 - regression_loss: 0.4925 - classification_loss: 0.2749
1351/3000 [============>.................] - ETA: 11:17 - loss: 0.7673 - regression_loss: 0.4924 - classification_loss: 0.2748
1352/3000 [============>.................] - ETA: 11:16 - loss: 0.7675 - regression_loss: 0.4926 - classification_loss: 0.2749
1353/3000 [============>.................] - ETA: 11:16 - loss: 0.7674 - regression_loss: 0.4924 - classification_loss: 0.2750
1354/3000 [============>.................] - ETA: 11:16 - loss: 0.7673 - regression_loss: 0.4923 - classification_loss: 0.2749
1355/3000 [============>.................] - ETA: 11:15 - loss: 0.7672 - regression_loss: 0.4923 - classification_loss: 0.2749
1356/3000 [============>.................] - ETA: 11:15 - loss: 0.7672 - regression_loss: 0.4924 - classification_loss: 0.2748
1357/3000 [============>.................] - ETA: 11:14 - loss: 0.7670 - regression_loss: 0.4923 - classification_loss: 0.2747
1358/3000 [============>.................] - ETA: 11:14 - loss: 0.7668 - regression_loss: 0.4921 - classification_loss: 0.2747
1359/3000 [============>.................] - ETA: 11:14 - loss: 0.7667 - regression_loss: 0.4921 - classification_loss: 0.2746
1360/3000 [============>.................] - ETA: 11:13 - loss: 0.7664 - regression_loss: 0.4919 - classification_loss: 0.2745
1361/3000 [============>.................] - ETA: 11:13 - loss: 0.7664 - regression_loss: 0.4920 - classification_loss: 0.2744
1362/3000 [============>.................] - ETA: 11:12 - loss: 0.7664 - regression_loss: 0.4920 - classification_loss: 0.2744
1363/3000 [============>.................] - ETA: 11:12 - loss: 0.7666 - regression_loss: 0.4922 - classification_loss: 0.2744
1364/3000 [============>.................] - ETA: 11:11 - loss: 0.7665 - regression_loss: 0.4922 - classification_loss: 0.2743
1365/3000 [============>.................] - ETA: 11:11 - loss: 0.7665 - regression_loss: 0.4922 - classification_loss: 0.2743
1366/3000 [============>.................] - ETA: 11:11 - loss: 0.7664 - regression_loss: 0.4921 - classification_loss: 0.2742
1367/3000 [============>.................] - ETA: 11:10 - loss: 0.7662 - regression_loss: 0.4921 - classification_loss: 0.2741
1368/3000 [============>.................] - ETA: 11:10 - loss: 0.7661 - regression_loss: 0.4919 - classification_loss: 0.2742
1369/3000 [============>.................] - ETA: 11:09 - loss: 0.7660 - regression_loss: 0.4918 - classification_loss: 0.2742
1370/3000 [============>.................] - ETA: 11:09 - loss: 0.7657 - regression_loss: 0.4917 - classification_loss: 0.2741
1371/3000 [============>.................] - ETA: 11:09 - loss: 0.7658 - regression_loss: 0.4917 - classification_loss: 0.2741
1372/3000 [============>.................] - ETA: 11:08 - loss: 0.7656 - regression_loss: 0.4916 - classification_loss: 0.2741
1373/3000 [============>.................] - ETA: 11:08 - loss: 0.7655 - regression_loss: 0.4915 - classification_loss: 0.2741
1374/3000 [============>.................] - ETA: 11:07 - loss: 0.7653 - regression_loss: 0.4913 - classification_loss: 0.2740
1375/3000 [============>.................] - ETA: 11:07 - loss: 0.7652 - regression_loss: 0.4913 - classification_loss: 0.2739
1376/3000 [============>.................] - ETA: 11:06 - loss: 0.7652 - regression_loss: 0.4914 - classification_loss: 0.2738
1377/3000 [============>.................] - ETA: 11:06 - loss: 0.7650 - regression_loss: 0.4912 - classification_loss: 0.2738
1378/3000 [============>.................] - ETA: 11:06 - loss: 0.7649 - regression_loss: 0.4912 - classification_loss: 0.2737
1379/3000 [============>.................] - ETA: 11:05 - loss: 0.7647 - regression_loss: 0.4910 - classification_loss: 0.2737
1380/3000 [============>.................] - ETA: 11:05 - loss: 0.7647 - regression_loss: 0.4911 - classification_loss: 0.2736
1381/3000 [============>.................] - ETA: 11:04 - loss: 0.7645 - regression_loss: 0.4909 - classification_loss: 0.2736
1382/3000 [============>.................] - ETA: 11:04 - loss: 0.7644 - regression_loss: 0.4909 - classification_loss: 0.2735
1383/3000 [============>.................] - ETA: 11:04 - loss: 0.7641 - regression_loss: 0.4907 - classification_loss: 0.2734
1384/3000 [============>.................] - ETA: 11:03 - loss: 0.7641 - regression_loss: 0.4906 - classification_loss: 0.2735
1385/3000 [============>.................] - ETA: 11:03 - loss: 0.7639 - regression_loss: 0.4904 - classification_loss: 0.2735
1386/3000 [============>.................] - ETA: 11:02 - loss: 0.7637 - regression_loss: 0.4903 - classification_loss: 0.2734
1387/3000 [============>.................] - ETA: 11:02 - loss: 0.7635 - regression_loss: 0.4901 - classification_loss: 0.2733
1388/3000 [============>.................] - ETA: 11:02 - loss: 0.7637 - regression_loss: 0.4903 - classification_loss: 0.2733
1389/3000 [============>.................] - ETA: 11:01 - loss: 0.7635 - regression_loss: 0.4902 - classification_loss: 0.2733
1390/3000 [============>.................] - ETA: 11:01 - loss: 0.7634 - regression_loss: 0.4902 - classification_loss: 0.2732
1391/3000 [============>.................] - ETA: 11:00 - loss: 0.7633 - regression_loss: 0.4901 - classification_loss: 0.2732
1392/3000 [============>.................] - ETA: 11:00 - loss: 0.7632 - regression_loss: 0.4900 - classification_loss: 0.2732
1393/3000 [============>.................] - ETA: 11:00 - loss: 0.7632 - regression_loss: 0.4900 - classification_loss: 0.2732
1394/3000 [============>.................] - ETA: 10:59 - loss: 0.7631 - regression_loss: 0.4900 - classification_loss: 0.2731
1395/3000 [============>.................] - ETA: 10:59 - loss: 0.7629 - regression_loss: 0.4898 - classification_loss: 0.2731
1396/3000 [============>.................] - ETA: 10:58 - loss: 0.7628 - regression_loss: 0.4897 - classification_loss: 0.2731
1397/3000 [============>.................] - ETA: 10:58 - loss: 0.7626 - regression_loss: 0.4896 - classification_loss: 0.2730
1398/3000 [============>.................] - ETA: 10:57 - loss: 0.7626 - regression_loss: 0.4896 - classification_loss: 0.2730
1399/3000 [============>.................] - ETA: 10:57 - loss: 0.7624 - regression_loss: 0.4895 - classification_loss: 0.2729
1400/3000 [=============>................] - ETA: 10:57 - loss: 0.7622 - regression_loss: 0.4894 - classification_loss: 0.2728
1401/3000 [=============>................] - ETA: 10:56 - loss: 0.7621 - regression_loss: 0.4893 - classification_loss: 0.2728
1402/3000 [=============>................] - ETA: 10:56 - loss: 0.7620 - regression_loss: 0.4893 - classification_loss: 0.2727
1403/3000 [=============>................] - ETA: 10:55 - loss: 0.7619 - regression_loss: 0.4892 - classification_loss: 0.2727
1404/3000 [=============>................] - ETA: 10:55 - loss: 0.7617 - regression_loss: 0.4891 - classification_loss: 0.2726
1405/3000 [=============>................] - ETA: 10:55 - loss: 0.7615 - regression_loss: 0.4889 - classification_loss: 0.2726
1406/3000 [=============>................] - ETA: 10:54 - loss: 0.7613 - regression_loss: 0.4888 - classification_loss: 0.2725
1407/3000 [=============>................] - ETA: 10:54 - loss: 0.7613 - regression_loss: 0.4887 - classification_loss: 0.2725
1408/3000 [=============>................] - ETA: 10:53 - loss: 0.7611 - regression_loss: 0.4886 - classification_loss: 0.2725
1409/3000 [=============>................] - ETA: 10:53 - loss: 0.7610 - regression_loss: 0.4885 - classification_loss: 0.2724
1410/3000 [=============>................] - ETA: 10:52 - loss: 0.7608 - regression_loss: 0.4885 - classification_loss: 0.2724
1411/3000 [=============>................] - ETA: 10:52 - loss: 0.7607 - regression_loss: 0.4883 - classification_loss: 0.2724
1412/3000 [=============>................] - ETA: 10:52 - loss: 0.7606 - regression_loss: 0.4883 - classification_loss: 0.2723
1413/3000 [=============>................] - ETA: 10:51 - loss: 0.7605 - regression_loss: 0.4882 - classification_loss: 0.2723
1414/3000 [=============>................] - ETA: 10:51 - loss: 0.7604 - regression_loss: 0.4881 - classification_loss: 0.2723
1415/3000 [=============>................] - ETA: 10:50 - loss: 0.7604 - regression_loss: 0.4882 - classification_loss: 0.2722
1416/3000 [=============>................] - ETA: 10:50 - loss: 0.7602 - regression_loss: 0.4880 - classification_loss: 0.2722
1417/3000 [=============>................] - ETA: 10:50 - loss: 0.7601 - regression_loss: 0.4879 - classification_loss: 0.2721
1418/3000 [=============>................] - ETA: 10:49 - loss: 0.7598 - regression_loss: 0.4878 - classification_loss: 0.2720
1419/3000 [=============>................] - ETA: 10:49 - loss: 0.7597 - regression_loss: 0.4877 - classification_loss: 0.2720
1420/3000 [=============>................] - ETA: 10:48 - loss: 0.7594 - regression_loss: 0.4875 - classification_loss: 0.2719
1421/3000 [=============>................] - ETA: 10:48 - loss: 0.7592 - regression_loss: 0.4875 - classification_loss: 0.2717
1422/3000 [=============>................] - ETA: 10:48 - loss: 0.7591 - regression_loss: 0.4874 - classification_loss: 0.2717
1423/3000 [=============>................] - ETA: 10:47 - loss: 0.7590 - regression_loss: 0.4873 - classification_loss: 0.2717
1424/3000 [=============>................] - ETA: 10:47 - loss: 0.7592 - regression_loss: 0.4876 - classification_loss: 0.2716
1425/3000 [=============>................] - ETA: 10:46 - loss: 0.7589 - regression_loss: 0.4874 - classification_loss: 0.2715
1426/3000 [=============>................] - ETA: 10:46 - loss: 0.7591 - regression_loss: 0.4877 - classification_loss: 0.2715
1427/3000 [=============>................] - ETA: 10:46 - loss: 0.7592 - regression_loss: 0.4877 - classification_loss: 0.2715
1428/3000 [=============>................] - ETA: 10:45 - loss: 0.7589 - regression_loss: 0.4875 - classification_loss: 0.2714
1429/3000 [=============>................] - ETA: 10:45 - loss: 0.7587 - regression_loss: 0.4874 - classification_loss: 0.2713
1430/3000 [=============>................] - ETA: 10:44 - loss: 0.7586 - regression_loss: 0.4873 - classification_loss: 0.2713
1431/3000 [=============>................] - ETA: 10:44 - loss: 0.7585 - regression_loss: 0.4873 - classification_loss: 0.2712
1432/3000 [=============>................] - ETA: 10:43 - loss: 0.7584 - regression_loss: 0.4872 - classification_loss: 0.2712
1433/3000 [=============>................] - ETA: 10:43 - loss: 0.7583 - regression_loss: 0.4871 - classification_loss: 0.2712
1434/3000 [=============>................] - ETA: 10:43 - loss: 0.7581 - regression_loss: 0.4870 - classification_loss: 0.2711
1435/3000 [=============>................] - ETA: 10:42 - loss: 0.7580 - regression_loss: 0.4870 - classification_loss: 0.2710
1436/3000 [=============>................] - ETA: 10:42 - loss: 0.7580 - regression_loss: 0.4870 - classification_loss: 0.2710
1437/3000 [=============>................] - ETA: 10:41 - loss: 0.7577 - regression_loss: 0.4869 - classification_loss: 0.2709
1438/3000 [=============>................] - ETA: 10:41 - loss: 0.7575 - regression_loss: 0.4867 - classification_loss: 0.2708
1439/3000 [=============>................] - ETA: 10:41 - loss: 0.7575 - regression_loss: 0.4867 - classification_loss: 0.2708
1440/3000 [=============>................] - ETA: 10:40 - loss: 0.7576 - regression_loss: 0.4867 - classification_loss: 0.2709
1441/3000 [=============>................] - ETA: 10:40 - loss: 0.7575 - regression_loss: 0.4867 - classification_loss: 0.2708
1442/3000 [=============>................] - ETA: 10:39 - loss: 0.7577 - regression_loss: 0.4869 - classification_loss: 0.2708
1443/3000 [=============>................] - ETA: 10:39 - loss: 0.7576 - regression_loss: 0.4869 - classification_loss: 0.2707
1444/3000 [=============>................] - ETA: 10:39 - loss: 0.7575 - regression_loss: 0.4869 - classification_loss: 0.2706
1445/3000 [=============>................] - ETA: 10:38 - loss: 0.7576 - regression_loss: 0.4870 - classification_loss: 0.2706
1446/3000 [=============>................] - ETA: 10:38 - loss: 0.7574 - regression_loss: 0.4869 - classification_loss: 0.2705
1447/3000 [=============>................] - ETA: 10:37 - loss: 0.7574 - regression_loss: 0.4869 - classification_loss: 0.2705
1448/3000 [=============>................] - ETA: 10:37 - loss: 0.7574 - regression_loss: 0.4869 - classification_loss: 0.2705
1449/3000 [=============>................] - ETA: 10:37 - loss: 0.7572 - regression_loss: 0.4868 - classification_loss: 0.2704
1450/3000 [=============>................] - ETA: 10:36 - loss: 0.7569 - regression_loss: 0.4866 - classification_loss: 0.2703
1451/3000 [=============>................] - ETA: 10:36 - loss: 0.7567 - regression_loss: 0.4865 - classification_loss: 0.2702
1452/3000 [=============>................] - ETA: 10:35 - loss: 0.7567 - regression_loss: 0.4865 - classification_loss: 0.2702
1453/3000 [=============>................] - ETA: 10:35 - loss: 0.7567 - regression_loss: 0.4864 - classification_loss: 0.2702
1454/3000 [=============>................] - ETA: 10:34 - loss: 0.7565 - regression_loss: 0.4864 - classification_loss: 0.2701
1455/3000 [=============>................] - ETA: 10:34 - loss: 0.7567 - regression_loss: 0.4866 - classification_loss: 0.2701
1456/3000 [=============>................] - ETA: 10:34 - loss: 0.7565 - regression_loss: 0.4865 - classification_loss: 0.2701
1457/3000 [=============>................] - ETA: 10:33 - loss: 0.7564 - regression_loss: 0.4865 - classification_loss: 0.2699
1458/3000 [=============>................] - ETA: 10:33 - loss: 0.7565 - regression_loss: 0.4866 - classification_loss: 0.2699
1459/3000 [=============>................] - ETA: 10:32 - loss: 0.7563 - regression_loss: 0.4864 - classification_loss: 0.2699
1460/3000 [=============>................] - ETA: 10:32 - loss: 0.7562 - regression_loss: 0.4864 - classification_loss: 0.2698
1461/3000 [=============>................] - ETA: 10:31 - loss: 0.7561 - regression_loss: 0.4863 - classification_loss: 0.2698
1462/3000 [=============>................] - ETA: 10:31 - loss: 0.7560 - regression_loss: 0.4862 - classification_loss: 0.2698
1463/3000 [=============>................] - ETA: 10:31 - loss: 0.7561 - regression_loss: 0.4864 - classification_loss: 0.2697
1464/3000 [=============>................] - ETA: 10:30 - loss: 0.7562 - regression_loss: 0.4864 - classification_loss: 0.2698
1465/3000 [=============>................] - ETA: 10:30 - loss: 0.7560 - regression_loss: 0.4863 - classification_loss: 0.2698
1466/3000 [=============>................] - ETA: 10:29 - loss: 0.7559 - regression_loss: 0.4861 - classification_loss: 0.2697
1467/3000 [=============>................] - ETA: 10:29 - loss: 0.7560 - regression_loss: 0.4863 - classification_loss: 0.2697
1468/3000 [=============>................] - ETA: 10:29 - loss: 0.7558 - regression_loss: 0.4861 - classification_loss: 0.2697
1469/3000 [=============>................] - ETA: 10:28 - loss: 0.7556 - regression_loss: 0.4860 - classification_loss: 0.2696
1470/3000 [=============>................] - ETA: 10:28 - loss: 0.7557 - regression_loss: 0.4861 - classification_loss: 0.2696
1471/3000 [=============>................] - ETA: 10:27 - loss: 0.7558 - regression_loss: 0.4862 - classification_loss: 0.2696
1472/3000 [=============>................] - ETA: 10:27 - loss: 0.7556 - regression_loss: 0.4860 - classification_loss: 0.2696
1473/3000 [=============>................] - ETA: 10:27 - loss: 0.7554 - regression_loss: 0.4859 - classification_loss: 0.2695
1474/3000 [=============>................] - ETA: 10:26 - loss: 0.7553 - regression_loss: 0.4858 - classification_loss: 0.2695
1475/3000 [=============>................] - ETA: 10:26 - loss: 0.7550 - regression_loss: 0.4856 - classification_loss: 0.2694
1476/3000 [=============>................] - ETA: 10:25 - loss: 0.7550 - regression_loss: 0.4855 - classification_loss: 0.2695
1477/3000 [=============>................] - ETA: 10:25 - loss: 0.7549 - regression_loss: 0.4854 - classification_loss: 0.2695
1478/3000 [=============>................] - ETA: 10:25 - loss: 0.7547 - regression_loss: 0.4852 - classification_loss: 0.2695
1479/3000 [=============>................] - ETA: 10:24 - loss: 0.7546 - regression_loss: 0.4852 - classification_loss: 0.2694
1480/3000 [=============>................] - ETA: 10:24 - loss: 0.7545 - regression_loss: 0.4851 - classification_loss: 0.2694
1481/3000 [=============>................] - ETA: 10:23 - loss: 0.7545 - regression_loss: 0.4851 - classification_loss: 0.2694
1482/3000 [=============>................] - ETA: 10:23 - loss: 0.7542 - regression_loss: 0.4850 - classification_loss: 0.2692
1483/3000 [=============>................] - ETA: 10:22 - loss: 0.7542 - regression_loss: 0.4851 - classification_loss: 0.2691
1484/3000 [=============>................] - ETA: 10:22 - loss: 0.7539 - regression_loss: 0.4849 - classification_loss: 0.2690
1485/3000 [=============>................] - ETA: 10:22 - loss: 0.7538 - regression_loss: 0.4849 - classification_loss: 0.2690
1486/3000 [=============>................] - ETA: 10:21 - loss: 0.7538 - regression_loss: 0.4849 - classification_loss: 0.2689
1487/3000 [=============>................] - ETA: 10:21 - loss: 0.7536 - regression_loss: 0.4847 - classification_loss: 0.2689
1488/3000 [=============>................] - ETA: 10:20 - loss: 0.7535 - regression_loss: 0.4846 - classification_loss: 0.2689
1489/3000 [=============>................] - ETA: 10:20 - loss: 0.7535 - regression_loss: 0.4845 - classification_loss: 0.2689
1490/3000 [=============>................] - ETA: 10:20 - loss: 0.7536 - regression_loss: 0.4847 - classification_loss: 0.2689
1491/3000 [=============>................] - ETA: 10:19 - loss: 0.7536 - regression_loss: 0.4847 - classification_loss: 0.2688
1492/3000 [=============>................] - ETA: 10:19 - loss: 0.7535 - regression_loss: 0.4847 - classification_loss: 0.2689
1493/3000 [=============>................] - ETA: 10:18 - loss: 0.7535 - regression_loss: 0.4847 - classification_loss: 0.2688
1494/3000 [=============>................] - ETA: 10:18 - loss: 0.7535 - regression_loss: 0.4847 - classification_loss: 0.2688
1495/3000 [=============>................] - ETA: 10:18 - loss: 0.7538 - regression_loss: 0.4850 - classification_loss: 0.2688
1496/3000 [=============>................] - ETA: 10:17 - loss: 0.7537 - regression_loss: 0.4849 - classification_loss: 0.2688
1497/3000 [=============>................] - ETA: 10:17 - loss: 0.7536 - regression_loss: 0.4848 - classification_loss: 0.2688
1498/3000 [=============>................] - ETA: 10:16 - loss: 0.7536 - regression_loss: 0.4848 - classification_loss: 0.2687
1499/3000 [=============>................] - ETA: 10:16 - loss: 0.7534 - regression_loss: 0.4847 - classification_loss: 0.2687
1500/3000 [==============>...............] - ETA: 10:15 - loss: 0.7532 - regression_loss: 0.4845 - classification_loss: 0.2687
1501/3000 [==============>...............] - ETA: 10:15 - loss: 0.7529 - regression_loss: 0.4844 - classification_loss: 0.2685
1502/3000 [==============>...............] - ETA: 10:15 - loss: 0.7527 - regression_loss: 0.4843 - classification_loss: 0.2685
1503/3000 [==============>...............] - ETA: 10:14 - loss: 0.7526 - regression_loss: 0.4843 - classification_loss: 0.2684
1504/3000 [==============>...............] - ETA: 10:14 - loss: 0.7528 - regression_loss: 0.4844 - classification_loss: 0.2683
1505/3000 [==============>...............] - ETA: 10:13 - loss: 0.7526 - regression_loss: 0.4843 - classification_loss: 0.2682
1506/3000 [==============>...............] - ETA: 10:13 - loss: 0.7526 - regression_loss: 0.4844 - classification_loss: 0.2682
1507/3000 [==============>...............] - ETA: 10:13 - loss: 0.7525 - regression_loss: 0.4844 - classification_loss: 0.2681
1508/3000 [==============>...............] - ETA: 10:12 - loss: 0.7524 - regression_loss: 0.4843 - classification_loss: 0.2681
1509/3000 [==============>...............] - ETA: 10:12 - loss: 0.7525 - regression_loss: 0.4845 - classification_loss: 0.2680
1510/3000 [==============>...............] - ETA: 10:11 - loss: 0.7527 - regression_loss: 0.4846 - classification_loss: 0.2681
1511/3000 [==============>...............] - ETA: 10:11 - loss: 0.7524 - regression_loss: 0.4844 - classification_loss: 0.2681
1512/3000 [==============>...............] - ETA: 10:11 - loss: 0.7523 - regression_loss: 0.4844 - classification_loss: 0.2680
1513/3000 [==============>...............] - ETA: 10:10 - loss: 0.7521 - regression_loss: 0.4842 - classification_loss: 0.2679
1514/3000 [==============>...............] - ETA: 10:10 - loss: 0.7520 - regression_loss: 0.4841 - classification_loss: 0.2679
1515/3000 [==============>...............] - ETA: 10:09 - loss: 0.7519 - regression_loss: 0.4840 - classification_loss: 0.2678
1516/3000 [==============>...............] - ETA: 10:09 - loss: 0.7516 - regression_loss: 0.4839 - classification_loss: 0.2677
1517/3000 [==============>...............] - ETA: 10:08 - loss: 0.7517 - regression_loss: 0.4840 - classification_loss: 0.2677
1518/3000 [==============>...............] - ETA: 10:08 - loss: 0.7515 - regression_loss: 0.4839 - classification_loss: 0.2676
1519/3000 [==============>...............] - ETA: 10:08 - loss: 0.7514 - regression_loss: 0.4839 - classification_loss: 0.2675
1520/3000 [==============>...............] - ETA: 10:07 - loss: 0.7514 - regression_loss: 0.4838 - classification_loss: 0.2676
1521/3000 [==============>...............] - ETA: 10:07 - loss: 0.7512 - regression_loss: 0.4837 - classification_loss: 0.2675
1522/3000 [==============>...............] - ETA: 10:06 - loss: 0.7510 - regression_loss: 0.4835 - classification_loss: 0.2675
1523/3000 [==============>...............] - ETA: 10:06 - loss: 0.7509 - regression_loss: 0.4834 - classification_loss: 0.2675
1524/3000 [==============>...............] - ETA: 10:06 - loss: 0.7508 - regression_loss: 0.4834 - classification_loss: 0.2675
1525/3000 [==============>...............] - ETA: 10:05 - loss: 0.7509 - regression_loss: 0.4835 - classification_loss: 0.2674
1526/3000 [==============>...............] - ETA: 10:05 - loss: 0.7507 - regression_loss: 0.4833 - classification_loss: 0.2674
1527/3000 [==============>...............] - ETA: 10:04 - loss: 0.7508 - regression_loss: 0.4835 - classification_loss: 0.2673
1528/3000 [==============>...............] - ETA: 10:04 - loss: 0.7505 - regression_loss: 0.4834 - classification_loss: 0.2671
1529/3000 [==============>...............] - ETA: 10:03 - loss: 0.7505 - regression_loss: 0.4834 - classification_loss: 0.2671
1530/3000 [==============>...............] - ETA: 10:03 - loss: 0.7504 - regression_loss: 0.4834 - classification_loss: 0.2670
1531/3000 [==============>...............] - ETA: 10:03 - loss: 0.7502 - regression_loss: 0.4833 - classification_loss: 0.2669
1532/3000 [==============>...............] - ETA: 10:02 - loss: 0.7503 - regression_loss: 0.4834 - classification_loss: 0.2669
1533/3000 [==============>...............] - ETA: 10:02 - loss: 0.7503 - regression_loss: 0.4835 - classification_loss: 0.2669
1534/3000 [==============>...............] - ETA: 10:01 - loss: 0.7501 - regression_loss: 0.4833 - classification_loss: 0.2668
1535/3000 [==============>...............] - ETA: 10:01 - loss: 0.7500 - regression_loss: 0.4832 - classification_loss: 0.2668
1536/3000 [==============>...............] - ETA: 10:01 - loss: 0.7499 - regression_loss: 0.4831 - classification_loss: 0.2668
1537/3000 [==============>...............] - ETA: 10:00 - loss: 0.7499 - regression_loss: 0.4831 - classification_loss: 0.2668
1538/3000 [==============>...............] - ETA: 10:00 - loss: 0.7498 - regression_loss: 0.4830 - classification_loss: 0.2668
1539/3000 [==============>...............] - ETA: 9:59 - loss: 0.7497 - regression_loss: 0.4829 - classification_loss: 0.2668 
1540/3000 [==============>...............] - ETA: 9:59 - loss: 0.7499 - regression_loss: 0.4829 - classification_loss: 0.2670
1541/3000 [==============>...............] - ETA: 9:59 - loss: 0.7498 - regression_loss: 0.4827 - classification_loss: 0.2671
1542/3000 [==============>...............] - ETA: 9:58 - loss: 0.7499 - regression_loss: 0.4828 - classification_loss: 0.2670
1543/3000 [==============>...............] - ETA: 9:58 - loss: 0.7497 - regression_loss: 0.4827 - classification_loss: 0.2671
1544/3000 [==============>...............] - ETA: 9:57 - loss: 0.7497 - regression_loss: 0.4827 - classification_loss: 0.2670
1545/3000 [==============>...............] - ETA: 9:57 - loss: 0.7495 - regression_loss: 0.4826 - classification_loss: 0.2669
1546/3000 [==============>...............] - ETA: 9:57 - loss: 0.7495 - regression_loss: 0.4826 - classification_loss: 0.2669
1547/3000 [==============>...............] - ETA: 9:56 - loss: 0.7495 - regression_loss: 0.4827 - classification_loss: 0.2668
1548/3000 [==============>...............] - ETA: 9:56 - loss: 0.7495 - regression_loss: 0.4827 - classification_loss: 0.2668
1549/3000 [==============>...............] - ETA: 9:55 - loss: 0.7492 - regression_loss: 0.4825 - classification_loss: 0.2666
1550/3000 [==============>...............] - ETA: 9:55 - loss: 0.7491 - regression_loss: 0.4825 - classification_loss: 0.2666
1551/3000 [==============>...............] - ETA: 9:54 - loss: 0.7490 - regression_loss: 0.4825 - classification_loss: 0.2665
1552/3000 [==============>...............] - ETA: 9:54 - loss: 0.7490 - regression_loss: 0.4825 - classification_loss: 0.2664
1553/3000 [==============>...............] - ETA: 9:54 - loss: 0.7489 - regression_loss: 0.4825 - classification_loss: 0.2664
1554/3000 [==============>...............] - ETA: 9:53 - loss: 0.7489 - regression_loss: 0.4825 - classification_loss: 0.2663
1555/3000 [==============>...............] - ETA: 9:53 - loss: 0.7486 - regression_loss: 0.4825 - classification_loss: 0.2662
1556/3000 [==============>...............] - ETA: 9:52 - loss: 0.7485 - regression_loss: 0.4823 - classification_loss: 0.2662
1557/3000 [==============>...............] - ETA: 9:52 - loss: 0.7485 - regression_loss: 0.4824 - classification_loss: 0.2661
1558/3000 [==============>...............] - ETA: 9:52 - loss: 0.7484 - regression_loss: 0.4824 - classification_loss: 0.2660
1559/3000 [==============>...............] - ETA: 9:51 - loss: 0.7481 - regression_loss: 0.4822 - classification_loss: 0.2659
1560/3000 [==============>...............] - ETA: 9:51 - loss: 0.7481 - regression_loss: 0.4823 - classification_loss: 0.2658
1561/3000 [==============>...............] - ETA: 9:50 - loss: 0.7479 - regression_loss: 0.4822 - classification_loss: 0.2658
1562/3000 [==============>...............] - ETA: 9:50 - loss: 0.7478 - regression_loss: 0.4820 - classification_loss: 0.2657
1563/3000 [==============>...............] - ETA: 9:49 - loss: 0.7477 - regression_loss: 0.4820 - classification_loss: 0.2657
1564/3000 [==============>...............] - ETA: 9:49 - loss: 0.7476 - regression_loss: 0.4820 - classification_loss: 0.2656
1565/3000 [==============>...............] - ETA: 9:49 - loss: 0.7475 - regression_loss: 0.4819 - classification_loss: 0.2655
1566/3000 [==============>...............] - ETA: 9:48 - loss: 0.7475 - regression_loss: 0.4819 - classification_loss: 0.2655
1567/3000 [==============>...............] - ETA: 9:48 - loss: 0.7472 - regression_loss: 0.4818 - classification_loss: 0.2654
1568/3000 [==============>...............] - ETA: 9:47 - loss: 0.7472 - regression_loss: 0.4818 - classification_loss: 0.2653
1569/3000 [==============>...............] - ETA: 9:47 - loss: 0.7471 - regression_loss: 0.4818 - classification_loss: 0.2653
1570/3000 [==============>...............] - ETA: 9:47 - loss: 0.7469 - regression_loss: 0.4817 - classification_loss: 0.2652
1571/3000 [==============>...............] - ETA: 9:46 - loss: 0.7468 - regression_loss: 0.4815 - classification_loss: 0.2653
1572/3000 [==============>...............] - ETA: 9:46 - loss: 0.7467 - regression_loss: 0.4815 - classification_loss: 0.2652
1573/3000 [==============>...............] - ETA: 9:45 - loss: 0.7465 - regression_loss: 0.4814 - classification_loss: 0.2652
1574/3000 [==============>...............] - ETA: 9:45 - loss: 0.7465 - regression_loss: 0.4813 - classification_loss: 0.2652
1575/3000 [==============>...............] - ETA: 9:45 - loss: 0.7462 - regression_loss: 0.4812 - classification_loss: 0.2651
1576/3000 [==============>...............] - ETA: 9:44 - loss: 0.7460 - regression_loss: 0.4810 - classification_loss: 0.2649
1577/3000 [==============>...............] - ETA: 9:44 - loss: 0.7457 - regression_loss: 0.4809 - classification_loss: 0.2648
1578/3000 [==============>...............] - ETA: 9:43 - loss: 0.7455 - regression_loss: 0.4807 - classification_loss: 0.2648
1579/3000 [==============>...............] - ETA: 9:43 - loss: 0.7456 - regression_loss: 0.4808 - classification_loss: 0.2648
1580/3000 [==============>...............] - ETA: 9:43 - loss: 0.7456 - regression_loss: 0.4809 - classification_loss: 0.2647
1581/3000 [==============>...............] - ETA: 9:42 - loss: 0.7455 - regression_loss: 0.4808 - classification_loss: 0.2647
1582/3000 [==============>...............] - ETA: 9:42 - loss: 0.7453 - regression_loss: 0.4806 - classification_loss: 0.2647
1583/3000 [==============>...............] - ETA: 9:41 - loss: 0.7452 - regression_loss: 0.4806 - classification_loss: 0.2646
1584/3000 [==============>...............] - ETA: 9:41 - loss: 0.7450 - regression_loss: 0.4804 - classification_loss: 0.2646
1585/3000 [==============>...............] - ETA: 9:41 - loss: 0.7449 - regression_loss: 0.4804 - classification_loss: 0.2644
1586/3000 [==============>...............] - ETA: 9:40 - loss: 0.7448 - regression_loss: 0.4804 - classification_loss: 0.2644
1587/3000 [==============>...............] - ETA: 9:40 - loss: 0.7446 - regression_loss: 0.4802 - classification_loss: 0.2644
1588/3000 [==============>...............] - ETA: 9:39 - loss: 0.7446 - regression_loss: 0.4802 - classification_loss: 0.2644
1589/3000 [==============>...............] - ETA: 9:39 - loss: 0.7446 - regression_loss: 0.4802 - classification_loss: 0.2644
1590/3000 [==============>...............] - ETA: 9:38 - loss: 0.7445 - regression_loss: 0.4801 - classification_loss: 0.2644
1591/3000 [==============>...............] - ETA: 9:38 - loss: 0.7443 - regression_loss: 0.4799 - classification_loss: 0.2644
1592/3000 [==============>...............] - ETA: 9:38 - loss: 0.7440 - regression_loss: 0.4797 - classification_loss: 0.2642
1593/3000 [==============>...............] - ETA: 9:37 - loss: 0.7438 - regression_loss: 0.4796 - classification_loss: 0.2641
1594/3000 [==============>...............] - ETA: 9:37 - loss: 0.7436 - regression_loss: 0.4795 - classification_loss: 0.2641
1595/3000 [==============>...............] - ETA: 9:36 - loss: 0.7433 - regression_loss: 0.4793 - classification_loss: 0.2640
1596/3000 [==============>...............] - ETA: 9:36 - loss: 0.7431 - regression_loss: 0.4793 - classification_loss: 0.2639
1597/3000 [==============>...............] - ETA: 9:36 - loss: 0.7431 - regression_loss: 0.4792 - classification_loss: 0.2639
1598/3000 [==============>...............] - ETA: 9:35 - loss: 0.7430 - regression_loss: 0.4792 - classification_loss: 0.2638
1599/3000 [==============>...............] - ETA: 9:35 - loss: 0.7430 - regression_loss: 0.4792 - classification_loss: 0.2638
1600/3000 [===============>..............] - ETA: 9:34 - loss: 0.7428 - regression_loss: 0.4790 - classification_loss: 0.2638
1601/3000 [===============>..............] - ETA: 9:34 - loss: 0.7429 - regression_loss: 0.4791 - classification_loss: 0.2638
1602/3000 [===============>..............] - ETA: 9:34 - loss: 0.7428 - regression_loss: 0.4790 - classification_loss: 0.2638
1603/3000 [===============>..............] - ETA: 9:33 - loss: 0.7427 - regression_loss: 0.4789 - classification_loss: 0.2637
1604/3000 [===============>..............] - ETA: 9:33 - loss: 0.7427 - regression_loss: 0.4791 - classification_loss: 0.2636
1605/3000 [===============>..............] - ETA: 9:32 - loss: 0.7426 - regression_loss: 0.4790 - classification_loss: 0.2636
1606/3000 [===============>..............] - ETA: 9:32 - loss: 0.7427 - regression_loss: 0.4791 - classification_loss: 0.2636
1607/3000 [===============>..............] - ETA: 9:31 - loss: 0.7427 - regression_loss: 0.4792 - classification_loss: 0.2635
1608/3000 [===============>..............] - ETA: 9:31 - loss: 0.7427 - regression_loss: 0.4792 - classification_loss: 0.2635
1609/3000 [===============>..............] - ETA: 9:31 - loss: 0.7424 - regression_loss: 0.4790 - classification_loss: 0.2634
1610/3000 [===============>..............] - ETA: 9:30 - loss: 0.7425 - regression_loss: 0.4791 - classification_loss: 0.2634
1611/3000 [===============>..............] - ETA: 9:30 - loss: 0.7424 - regression_loss: 0.4790 - classification_loss: 0.2633
1612/3000 [===============>..............] - ETA: 9:29 - loss: 0.7422 - regression_loss: 0.4789 - classification_loss: 0.2633
1613/3000 [===============>..............] - ETA: 9:29 - loss: 0.7422 - regression_loss: 0.4788 - classification_loss: 0.2633
1614/3000 [===============>..............] - ETA: 9:29 - loss: 0.7421 - regression_loss: 0.4788 - classification_loss: 0.2633
1615/3000 [===============>..............] - ETA: 9:28 - loss: 0.7422 - regression_loss: 0.4789 - classification_loss: 0.2632
1616/3000 [===============>..............] - ETA: 9:28 - loss: 0.7420 - regression_loss: 0.4788 - classification_loss: 0.2632
1617/3000 [===============>..............] - ETA: 9:27 - loss: 0.7419 - regression_loss: 0.4787 - classification_loss: 0.2631
1618/3000 [===============>..............] - ETA: 9:27 - loss: 0.7417 - regression_loss: 0.4786 - classification_loss: 0.2631
1619/3000 [===============>..............] - ETA: 9:27 - loss: 0.7417 - regression_loss: 0.4786 - classification_loss: 0.2631
1620/3000 [===============>..............] - ETA: 9:26 - loss: 0.7417 - regression_loss: 0.4786 - classification_loss: 0.2631
1621/3000 [===============>..............] - ETA: 9:26 - loss: 0.7416 - regression_loss: 0.4784 - classification_loss: 0.2631
1622/3000 [===============>..............] - ETA: 9:25 - loss: 0.7414 - regression_loss: 0.4783 - classification_loss: 0.2631
1623/3000 [===============>..............] - ETA: 9:25 - loss: 0.7413 - regression_loss: 0.4782 - classification_loss: 0.2631
1624/3000 [===============>..............] - ETA: 9:25 - loss: 0.7412 - regression_loss: 0.4782 - classification_loss: 0.2630
1625/3000 [===============>..............] - ETA: 9:24 - loss: 0.7409 - regression_loss: 0.4780 - classification_loss: 0.2629
1626/3000 [===============>..............] - ETA: 9:24 - loss: 0.7410 - regression_loss: 0.4781 - classification_loss: 0.2629
1627/3000 [===============>..............] - ETA: 9:23 - loss: 0.7408 - regression_loss: 0.4780 - classification_loss: 0.2628
1628/3000 [===============>..............] - ETA: 9:23 - loss: 0.7408 - regression_loss: 0.4779 - classification_loss: 0.2629
1629/3000 [===============>..............] - ETA: 9:23 - loss: 0.7409 - regression_loss: 0.4781 - classification_loss: 0.2629
1630/3000 [===============>..............] - ETA: 9:22 - loss: 0.7409 - regression_loss: 0.4780 - classification_loss: 0.2629
1631/3000 [===============>..............] - ETA: 9:22 - loss: 0.7408 - regression_loss: 0.4780 - classification_loss: 0.2628
1632/3000 [===============>..............] - ETA: 9:21 - loss: 0.7407 - regression_loss: 0.4779 - classification_loss: 0.2628
1633/3000 [===============>..............] - ETA: 9:21 - loss: 0.7406 - regression_loss: 0.4778 - classification_loss: 0.2627
1634/3000 [===============>..............] - ETA: 9:20 - loss: 0.7405 - regression_loss: 0.4778 - classification_loss: 0.2627
1635/3000 [===============>..............] - ETA: 9:20 - loss: 0.7403 - regression_loss: 0.4776 - classification_loss: 0.2627
1636/3000 [===============>..............] - ETA: 9:20 - loss: 0.7403 - regression_loss: 0.4776 - classification_loss: 0.2627
1637/3000 [===============>..............] - ETA: 9:19 - loss: 0.7404 - regression_loss: 0.4778 - classification_loss: 0.2627
1638/3000 [===============>..............] - ETA: 9:19 - loss: 0.7402 - regression_loss: 0.4776 - classification_loss: 0.2626
1639/3000 [===============>..............] - ETA: 9:18 - loss: 0.7401 - regression_loss: 0.4775 - classification_loss: 0.2626
1640/3000 [===============>..............] - ETA: 9:18 - loss: 0.7400 - regression_loss: 0.4774 - classification_loss: 0.2626
1641/3000 [===============>..............] - ETA: 9:18 - loss: 0.7399 - regression_loss: 0.4773 - classification_loss: 0.2626
1642/3000 [===============>..............] - ETA: 9:17 - loss: 0.7398 - regression_loss: 0.4772 - classification_loss: 0.2626
1643/3000 [===============>..............] - ETA: 9:17 - loss: 0.7397 - regression_loss: 0.4771 - classification_loss: 0.2626
1644/3000 [===============>..............] - ETA: 9:16 - loss: 0.7395 - regression_loss: 0.4770 - classification_loss: 0.2625
1645/3000 [===============>..............] - ETA: 9:16 - loss: 0.7394 - regression_loss: 0.4769 - classification_loss: 0.2625
1646/3000 [===============>..............] - ETA: 9:15 - loss: 0.7392 - regression_loss: 0.4767 - classification_loss: 0.2624
1647/3000 [===============>..............] - ETA: 9:15 - loss: 0.7391 - regression_loss: 0.4766 - classification_loss: 0.2625
1648/3000 [===============>..............] - ETA: 9:15 - loss: 0.7390 - regression_loss: 0.4766 - classification_loss: 0.2623
1649/3000 [===============>..............] - ETA: 9:14 - loss: 0.7390 - regression_loss: 0.4767 - classification_loss: 0.2623
1650/3000 [===============>..............] - ETA: 9:14 - loss: 0.7391 - regression_loss: 0.4768 - classification_loss: 0.2623
1651/3000 [===============>..............] - ETA: 9:13 - loss: 0.7389 - regression_loss: 0.4767 - classification_loss: 0.2622
1652/3000 [===============>..............] - ETA: 9:13 - loss: 0.7389 - regression_loss: 0.4767 - classification_loss: 0.2622
1653/3000 [===============>..............] - ETA: 9:13 - loss: 0.7388 - regression_loss: 0.4766 - classification_loss: 0.2622
1654/3000 [===============>..............] - ETA: 9:12 - loss: 0.7388 - regression_loss: 0.4766 - classification_loss: 0.2622
1655/3000 [===============>..............] - ETA: 9:12 - loss: 0.7387 - regression_loss: 0.4765 - classification_loss: 0.2622
1656/3000 [===============>..............] - ETA: 9:11 - loss: 0.7386 - regression_loss: 0.4764 - classification_loss: 0.2621
1657/3000 [===============>..............] - ETA: 9:11 - loss: 0.7383 - regression_loss: 0.4763 - classification_loss: 0.2620
1658/3000 [===============>..............] - ETA: 9:11 - loss: 0.7384 - regression_loss: 0.4764 - classification_loss: 0.2620
1659/3000 [===============>..............] - ETA: 9:10 - loss: 0.7383 - regression_loss: 0.4763 - classification_loss: 0.2620
1660/3000 [===============>..............] - ETA: 9:10 - loss: 0.7382 - regression_loss: 0.4763 - classification_loss: 0.2620
1661/3000 [===============>..............] - ETA: 9:09 - loss: 0.7382 - regression_loss: 0.4763 - classification_loss: 0.2619
1662/3000 [===============>..............] - ETA: 9:09 - loss: 0.7381 - regression_loss: 0.4762 - classification_loss: 0.2619
1663/3000 [===============>..............] - ETA: 9:09 - loss: 0.7378 - regression_loss: 0.4761 - classification_loss: 0.2618
1664/3000 [===============>..............] - ETA: 9:08 - loss: 0.7378 - regression_loss: 0.4761 - classification_loss: 0.2617
1665/3000 [===============>..............] - ETA: 9:08 - loss: 0.7376 - regression_loss: 0.4759 - classification_loss: 0.2617
1666/3000 [===============>..............] - ETA: 9:07 - loss: 0.7374 - regression_loss: 0.4759 - classification_loss: 0.2616
1667/3000 [===============>..............] - ETA: 9:07 - loss: 0.7374 - regression_loss: 0.4758 - classification_loss: 0.2615
1668/3000 [===============>..............] - ETA: 9:07 - loss: 0.7372 - regression_loss: 0.4757 - classification_loss: 0.2615
1669/3000 [===============>..............] - ETA: 9:06 - loss: 0.7370 - regression_loss: 0.4755 - classification_loss: 0.2614
1670/3000 [===============>..............] - ETA: 9:06 - loss: 0.7368 - regression_loss: 0.4754 - classification_loss: 0.2614
1671/3000 [===============>..............] - ETA: 9:05 - loss: 0.7367 - regression_loss: 0.4754 - classification_loss: 0.2613
1672/3000 [===============>..............] - ETA: 9:05 - loss: 0.7368 - regression_loss: 0.4754 - classification_loss: 0.2613
1673/3000 [===============>..............] - ETA: 9:04 - loss: 0.7369 - regression_loss: 0.4755 - classification_loss: 0.2613
1674/3000 [===============>..............] - ETA: 9:04 - loss: 0.7368 - regression_loss: 0.4755 - classification_loss: 0.2613
1675/3000 [===============>..............] - ETA: 9:04 - loss: 0.7366 - regression_loss: 0.4754 - classification_loss: 0.2612
1676/3000 [===============>..............] - ETA: 9:03 - loss: 0.7365 - regression_loss: 0.4753 - classification_loss: 0.2612
1677/3000 [===============>..............] - ETA: 9:03 - loss: 0.7363 - regression_loss: 0.4752 - classification_loss: 0.2611
1678/3000 [===============>..............] - ETA: 9:02 - loss: 0.7362 - regression_loss: 0.4751 - classification_loss: 0.2611
1679/3000 [===============>..............] - ETA: 9:02 - loss: 0.7363 - regression_loss: 0.4752 - classification_loss: 0.2611
1680/3000 [===============>..............] - ETA: 9:02 - loss: 0.7360 - regression_loss: 0.4751 - classification_loss: 0.2610
1681/3000 [===============>..............] - ETA: 9:01 - loss: 0.7360 - regression_loss: 0.4750 - classification_loss: 0.2609
1682/3000 [===============>..............] - ETA: 9:01 - loss: 0.7359 - regression_loss: 0.4749 - classification_loss: 0.2610
1683/3000 [===============>..............] - ETA: 9:00 - loss: 0.7358 - regression_loss: 0.4748 - classification_loss: 0.2610
1684/3000 [===============>..............] - ETA: 9:00 - loss: 0.7357 - regression_loss: 0.4748 - classification_loss: 0.2609
1685/3000 [===============>..............] - ETA: 9:00 - loss: 0.7357 - regression_loss: 0.4748 - classification_loss: 0.2609
1686/3000 [===============>..............] - ETA: 8:59 - loss: 0.7355 - regression_loss: 0.4746 - classification_loss: 0.2608
1687/3000 [===============>..............] - ETA: 8:59 - loss: 0.7352 - regression_loss: 0.4745 - classification_loss: 0.2607
1688/3000 [===============>..............] - ETA: 8:58 - loss: 0.7350 - regression_loss: 0.4744 - classification_loss: 0.2606
1689/3000 [===============>..............] - ETA: 8:58 - loss: 0.7351 - regression_loss: 0.4745 - classification_loss: 0.2606
1690/3000 [===============>..............] - ETA: 8:58 - loss: 0.7350 - regression_loss: 0.4744 - classification_loss: 0.2606
1691/3000 [===============>..............] - ETA: 8:57 - loss: 0.7349 - regression_loss: 0.4743 - classification_loss: 0.2606
1692/3000 [===============>..............] - ETA: 8:57 - loss: 0.7347 - regression_loss: 0.4742 - classification_loss: 0.2605
1693/3000 [===============>..............] - ETA: 8:56 - loss: 0.7344 - regression_loss: 0.4741 - classification_loss: 0.2604
1694/3000 [===============>..............] - ETA: 8:56 - loss: 0.7343 - regression_loss: 0.4740 - classification_loss: 0.2604
1695/3000 [===============>..............] - ETA: 8:56 - loss: 0.7344 - regression_loss: 0.4740 - classification_loss: 0.2603
1696/3000 [===============>..............] - ETA: 8:55 - loss: 0.7342 - regression_loss: 0.4739 - classification_loss: 0.2603
1697/3000 [===============>..............] - ETA: 8:55 - loss: 0.7341 - regression_loss: 0.4738 - classification_loss: 0.2603
1698/3000 [===============>..............] - ETA: 8:54 - loss: 0.7339 - regression_loss: 0.4737 - classification_loss: 0.2602
1699/3000 [===============>..............] - ETA: 8:54 - loss: 0.7340 - regression_loss: 0.4738 - classification_loss: 0.2602
1700/3000 [================>.............] - ETA: 8:53 - loss: 0.7339 - regression_loss: 0.4738 - classification_loss: 0.2601
1701/3000 [================>.............] - ETA: 8:53 - loss: 0.7339 - regression_loss: 0.4738 - classification_loss: 0.2600
1702/3000 [================>.............] - ETA: 8:53 - loss: 0.7339 - regression_loss: 0.4739 - classification_loss: 0.2600
1703/3000 [================>.............] - ETA: 8:52 - loss: 0.7338 - regression_loss: 0.4739 - classification_loss: 0.2599
1704/3000 [================>.............] - ETA: 8:52 - loss: 0.7336 - regression_loss: 0.4738 - classification_loss: 0.2599
1705/3000 [================>.............] - ETA: 8:51 - loss: 0.7334 - regression_loss: 0.4737 - classification_loss: 0.2597
1706/3000 [================>.............] - ETA: 8:51 - loss: 0.7333 - regression_loss: 0.4736 - classification_loss: 0.2597
1707/3000 [================>.............] - ETA: 8:51 - loss: 0.7332 - regression_loss: 0.4735 - classification_loss: 0.2597
1708/3000 [================>.............] - ETA: 8:50 - loss: 0.7330 - regression_loss: 0.4734 - classification_loss: 0.2596
1709/3000 [================>.............] - ETA: 8:50 - loss: 0.7329 - regression_loss: 0.4734 - classification_loss: 0.2595
1710/3000 [================>.............] - ETA: 8:49 - loss: 0.7330 - regression_loss: 0.4735 - classification_loss: 0.2595
1711/3000 [================>.............] - ETA: 8:49 - loss: 0.7327 - regression_loss: 0.4733 - classification_loss: 0.2593
1712/3000 [================>.............] - ETA: 8:49 - loss: 0.7327 - regression_loss: 0.4734 - classification_loss: 0.2593
1713/3000 [================>.............] - ETA: 8:48 - loss: 0.7327 - regression_loss: 0.4735 - classification_loss: 0.2593
1714/3000 [================>.............] - ETA: 8:48 - loss: 0.7325 - regression_loss: 0.4734 - classification_loss: 0.2591
1715/3000 [================>.............] - ETA: 8:47 - loss: 0.7324 - regression_loss: 0.4733 - classification_loss: 0.2591
1716/3000 [================>.............] - ETA: 8:47 - loss: 0.7323 - regression_loss: 0.4732 - classification_loss: 0.2591
1717/3000 [================>.............] - ETA: 8:46 - loss: 0.7322 - regression_loss: 0.4731 - classification_loss: 0.2590
1718/3000 [================>.............] - ETA: 8:46 - loss: 0.7320 - regression_loss: 0.4730 - classification_loss: 0.2591
1719/3000 [================>.............] - ETA: 8:46 - loss: 0.7320 - regression_loss: 0.4729 - classification_loss: 0.2590
1720/3000 [================>.............] - ETA: 8:45 - loss: 0.7319 - regression_loss: 0.4729 - classification_loss: 0.2590
1721/3000 [================>.............] - ETA: 8:45 - loss: 0.7317 - regression_loss: 0.4728 - classification_loss: 0.2589
1722/3000 [================>.............] - ETA: 8:44 - loss: 0.7316 - regression_loss: 0.4727 - classification_loss: 0.2589
1723/3000 [================>.............] - ETA: 8:44 - loss: 0.7314 - regression_loss: 0.4725 - classification_loss: 0.2589
1724/3000 [================>.............] - ETA: 8:44 - loss: 0.7312 - regression_loss: 0.4724 - classification_loss: 0.2589
1725/3000 [================>.............] - ETA: 8:43 - loss: 0.7312 - regression_loss: 0.4724 - classification_loss: 0.2588
1726/3000 [================>.............] - ETA: 8:43 - loss: 0.7312 - regression_loss: 0.4725 - classification_loss: 0.2587
1727/3000 [================>.............] - ETA: 8:42 - loss: 0.7311 - regression_loss: 0.4724 - classification_loss: 0.2587
1728/3000 [================>.............] - ETA: 8:42 - loss: 0.7309 - regression_loss: 0.4723 - classification_loss: 0.2586
1729/3000 [================>.............] - ETA: 8:42 - loss: 0.7309 - regression_loss: 0.4723 - classification_loss: 0.2586
1730/3000 [================>.............] - ETA: 8:41 - loss: 0.7308 - regression_loss: 0.4721 - classification_loss: 0.2586
1731/3000 [================>.............] - ETA: 8:41 - loss: 0.7307 - regression_loss: 0.4720 - classification_loss: 0.2586
1732/3000 [================>.............] - ETA: 8:40 - loss: 0.7306 - regression_loss: 0.4720 - classification_loss: 0.2586
1733/3000 [================>.............] - ETA: 8:40 - loss: 0.7305 - regression_loss: 0.4718 - classification_loss: 0.2587
1734/3000 [================>.............] - ETA: 8:39 - loss: 0.7304 - regression_loss: 0.4717 - classification_loss: 0.2587
1735/3000 [================>.............] - ETA: 8:39 - loss: 0.7303 - regression_loss: 0.4716 - classification_loss: 0.2587
1736/3000 [================>.............] - ETA: 8:39 - loss: 0.7303 - regression_loss: 0.4717 - classification_loss: 0.2586
1737/3000 [================>.............] - ETA: 8:38 - loss: 0.7303 - regression_loss: 0.4717 - classification_loss: 0.2586
1738/3000 [================>.............] - ETA: 8:38 - loss: 0.7302 - regression_loss: 0.4716 - classification_loss: 0.2585
1739/3000 [================>.............] - ETA: 8:37 - loss: 0.7300 - regression_loss: 0.4716 - classification_loss: 0.2584
1740/3000 [================>.............] - ETA: 8:37 - loss: 0.7299 - regression_loss: 0.4716 - classification_loss: 0.2583
1741/3000 [================>.............] - ETA: 8:37 - loss: 0.7298 - regression_loss: 0.4715 - classification_loss: 0.2583
1742/3000 [================>.............] - ETA: 8:36 - loss: 0.7296 - regression_loss: 0.4714 - classification_loss: 0.2582
1743/3000 [================>.............] - ETA: 8:36 - loss: 0.7294 - regression_loss: 0.4713 - classification_loss: 0.2581
1744/3000 [================>.............] - ETA: 8:35 - loss: 0.7292 - regression_loss: 0.4712 - classification_loss: 0.2581
1745/3000 [================>.............] - ETA: 8:35 - loss: 0.7291 - regression_loss: 0.4711 - classification_loss: 0.2580
1746/3000 [================>.............] - ETA: 8:35 - loss: 0.7290 - regression_loss: 0.4711 - classification_loss: 0.2579
1747/3000 [================>.............] - ETA: 8:34 - loss: 0.7289 - regression_loss: 0.4710 - classification_loss: 0.2579
1748/3000 [================>.............] - ETA: 8:34 - loss: 0.7287 - regression_loss: 0.4709 - classification_loss: 0.2579
1749/3000 [================>.............] - ETA: 8:33 - loss: 0.7286 - regression_loss: 0.4708 - classification_loss: 0.2578
1750/3000 [================>.............] - ETA: 8:33 - loss: 0.7286 - regression_loss: 0.4708 - classification_loss: 0.2578
1751/3000 [================>.............] - ETA: 8:33 - loss: 0.7285 - regression_loss: 0.4707 - classification_loss: 0.2578
1752/3000 [================>.............] - ETA: 8:32 - loss: 0.7285 - regression_loss: 0.4707 - classification_loss: 0.2577
1753/3000 [================>.............] - ETA: 8:32 - loss: 0.7283 - regression_loss: 0.4706 - classification_loss: 0.2577
1754/3000 [================>.............] - ETA: 8:31 - loss: 0.7282 - regression_loss: 0.4706 - classification_loss: 0.2576
1755/3000 [================>.............] - ETA: 8:31 - loss: 0.7282 - regression_loss: 0.4706 - classification_loss: 0.2576
1756/3000 [================>.............] - ETA: 8:30 - loss: 0.7281 - regression_loss: 0.4706 - classification_loss: 0.2575
1757/3000 [================>.............] - ETA: 8:30 - loss: 0.7281 - regression_loss: 0.4706 - classification_loss: 0.2575
1758/3000 [================>.............] - ETA: 8:30 - loss: 0.7280 - regression_loss: 0.4705 - classification_loss: 0.2575
1759/3000 [================>.............] - ETA: 8:29 - loss: 0.7279 - regression_loss: 0.4704 - classification_loss: 0.2575
1760/3000 [================>.............] - ETA: 8:29 - loss: 0.7278 - regression_loss: 0.4703 - classification_loss: 0.2575
1761/3000 [================>.............] - ETA: 8:28 - loss: 0.7277 - regression_loss: 0.4703 - classification_loss: 0.2574
1762/3000 [================>.............] - ETA: 8:28 - loss: 0.7276 - regression_loss: 0.4703 - classification_loss: 0.2574
1763/3000 [================>.............] - ETA: 8:28 - loss: 0.7279 - regression_loss: 0.4706 - classification_loss: 0.2573
1764/3000 [================>.............] - ETA: 8:27 - loss: 0.7278 - regression_loss: 0.4706 - classification_loss: 0.2572
1765/3000 [================>.............] - ETA: 8:27 - loss: 0.7277 - regression_loss: 0.4705 - classification_loss: 0.2572
1766/3000 [================>.............] - ETA: 8:26 - loss: 0.7275 - regression_loss: 0.4704 - classification_loss: 0.2571
1767/3000 [================>.............] - ETA: 8:26 - loss: 0.7275 - regression_loss: 0.4704 - classification_loss: 0.2571
1768/3000 [================>.............] - ETA: 8:26 - loss: 0.7274 - regression_loss: 0.4703 - classification_loss: 0.2571
1769/3000 [================>.............] - ETA: 8:25 - loss: 0.7273 - regression_loss: 0.4703 - classification_loss: 0.2570
1770/3000 [================>.............] - ETA: 8:25 - loss: 0.7271 - regression_loss: 0.4702 - classification_loss: 0.2569
1771/3000 [================>.............] - ETA: 8:24 - loss: 0.7271 - regression_loss: 0.4703 - classification_loss: 0.2568
1772/3000 [================>.............] - ETA: 8:24 - loss: 0.7272 - regression_loss: 0.4704 - classification_loss: 0.2568
1773/3000 [================>.............] - ETA: 8:24 - loss: 0.7271 - regression_loss: 0.4703 - classification_loss: 0.2568
1774/3000 [================>.............] - ETA: 8:23 - loss: 0.7271 - regression_loss: 0.4703 - classification_loss: 0.2568
1775/3000 [================>.............] - ETA: 8:23 - loss: 0.7270 - regression_loss: 0.4702 - classification_loss: 0.2567
1776/3000 [================>.............] - ETA: 8:22 - loss: 0.7271 - regression_loss: 0.4704 - classification_loss: 0.2567
1777/3000 [================>.............] - ETA: 8:22 - loss: 0.7270 - regression_loss: 0.4704 - classification_loss: 0.2567
1778/3000 [================>.............] - ETA: 8:22 - loss: 0.7269 - regression_loss: 0.4703 - classification_loss: 0.2566
1779/3000 [================>.............] - ETA: 8:21 - loss: 0.7268 - regression_loss: 0.4702 - classification_loss: 0.2566
1780/3000 [================>.............] - ETA: 8:21 - loss: 0.7266 - regression_loss: 0.4701 - classification_loss: 0.2565
1781/3000 [================>.............] - ETA: 8:20 - loss: 0.7265 - regression_loss: 0.4700 - classification_loss: 0.2565
1782/3000 [================>.............] - ETA: 8:20 - loss: 0.7265 - regression_loss: 0.4700 - classification_loss: 0.2565
1783/3000 [================>.............] - ETA: 8:19 - loss: 0.7264 - regression_loss: 0.4699 - classification_loss: 0.2565
1784/3000 [================>.............] - ETA: 8:19 - loss: 0.7265 - regression_loss: 0.4700 - classification_loss: 0.2565
1785/3000 [================>.............] - ETA: 8:19 - loss: 0.7262 - regression_loss: 0.4699 - classification_loss: 0.2564
1786/3000 [================>.............] - ETA: 8:18 - loss: 0.7261 - regression_loss: 0.4698 - classification_loss: 0.2563
1787/3000 [================>.............] - ETA: 8:18 - loss: 0.7261 - regression_loss: 0.4699 - classification_loss: 0.2562
1788/3000 [================>.............] - ETA: 8:17 - loss: 0.7262 - regression_loss: 0.4700 - classification_loss: 0.2562
1789/3000 [================>.............] - ETA: 8:17 - loss: 0.7261 - regression_loss: 0.4700 - classification_loss: 0.2562
1790/3000 [================>.............] - ETA: 8:17 - loss: 0.7261 - regression_loss: 0.4700 - classification_loss: 0.2561
1791/3000 [================>.............] - ETA: 8:16 - loss: 0.7259 - regression_loss: 0.4699 - classification_loss: 0.2560
1792/3000 [================>.............] - ETA: 8:16 - loss: 0.7257 - regression_loss: 0.4697 - classification_loss: 0.2560
1793/3000 [================>.............] - ETA: 8:15 - loss: 0.7254 - regression_loss: 0.4696 - classification_loss: 0.2558
1794/3000 [================>.............] - ETA: 8:15 - loss: 0.7252 - regression_loss: 0.4694 - classification_loss: 0.2558
1795/3000 [================>.............] - ETA: 8:15 - loss: 0.7251 - regression_loss: 0.4694 - classification_loss: 0.2557
1796/3000 [================>.............] - ETA: 8:14 - loss: 0.7252 - regression_loss: 0.4695 - classification_loss: 0.2557
1797/3000 [================>.............] - ETA: 8:14 - loss: 0.7250 - regression_loss: 0.4694 - classification_loss: 0.2556
1798/3000 [================>.............] - ETA: 8:13 - loss: 0.7248 - regression_loss: 0.4693 - classification_loss: 0.2555
1799/3000 [================>.............] - ETA: 8:13 - loss: 0.7247 - regression_loss: 0.4692 - classification_loss: 0.2554
1800/3000 [=================>............] - ETA: 8:12 - loss: 0.7247 - regression_loss: 0.4692 - classification_loss: 0.2554
1801/3000 [=================>............] - ETA: 8:12 - loss: 0.7245 - regression_loss: 0.4691 - classification_loss: 0.2553
1802/3000 [=================>............] - ETA: 8:12 - loss: 0.7244 - regression_loss: 0.4691 - classification_loss: 0.2553
1803/3000 [=================>............] - ETA: 8:11 - loss: 0.7242 - regression_loss: 0.4690 - classification_loss: 0.2552
1804/3000 [=================>............] - ETA: 8:11 - loss: 0.7240 - regression_loss: 0.4689 - classification_loss: 0.2552
1805/3000 [=================>............] - ETA: 8:10 - loss: 0.7239 - regression_loss: 0.4687 - classification_loss: 0.2551
1806/3000 [=================>............] - ETA: 8:10 - loss: 0.7238 - regression_loss: 0.4687 - classification_loss: 0.2551
1807/3000 [=================>............] - ETA: 8:10 - loss: 0.7236 - regression_loss: 0.4687 - classification_loss: 0.2550
1808/3000 [=================>............] - ETA: 8:09 - loss: 0.7235 - regression_loss: 0.4685 - classification_loss: 0.2550
1809/3000 [=================>............] - ETA: 8:09 - loss: 0.7234 - regression_loss: 0.4684 - classification_loss: 0.2549
1810/3000 [=================>............] - ETA: 8:08 - loss: 0.7232 - regression_loss: 0.4683 - classification_loss: 0.2549
1811/3000 [=================>............] - ETA: 8:08 - loss: 0.7233 - regression_loss: 0.4684 - classification_loss: 0.2549
1812/3000 [=================>............] - ETA: 8:08 - loss: 0.7232 - regression_loss: 0.4683 - classification_loss: 0.2549
1813/3000 [=================>............] - ETA: 8:07 - loss: 0.7231 - regression_loss: 0.4682 - classification_loss: 0.2549
1814/3000 [=================>............] - ETA: 8:07 - loss: 0.7229 - regression_loss: 0.4681 - classification_loss: 0.2548
1815/3000 [=================>............] - ETA: 8:06 - loss: 0.7228 - regression_loss: 0.4680 - classification_loss: 0.2548
1816/3000 [=================>............] - ETA: 8:06 - loss: 0.7228 - regression_loss: 0.4680 - classification_loss: 0.2548
1817/3000 [=================>............] - ETA: 8:06 - loss: 0.7226 - regression_loss: 0.4680 - classification_loss: 0.2547
1818/3000 [=================>............] - ETA: 8:05 - loss: 0.7225 - regression_loss: 0.4678 - classification_loss: 0.2546
1819/3000 [=================>............] - ETA: 8:05 - loss: 0.7223 - regression_loss: 0.4678 - classification_loss: 0.2546
1820/3000 [=================>............] - ETA: 8:04 - loss: 0.7223 - regression_loss: 0.4678 - classification_loss: 0.2545
1821/3000 [=================>............] - ETA: 8:04 - loss: 0.7221 - regression_loss: 0.4677 - classification_loss: 0.2544
1822/3000 [=================>............] - ETA: 8:03 - loss: 0.7221 - regression_loss: 0.4678 - classification_loss: 0.2544
1823/3000 [=================>............] - ETA: 8:03 - loss: 0.7220 - regression_loss: 0.4677 - classification_loss: 0.2543
1824/3000 [=================>............] - ETA: 8:03 - loss: 0.7219 - regression_loss: 0.4676 - classification_loss: 0.2543
1825/3000 [=================>............] - ETA: 8:02 - loss: 0.7217 - regression_loss: 0.4675 - classification_loss: 0.2542
1826/3000 [=================>............] - ETA: 8:02 - loss: 0.7217 - regression_loss: 0.4675 - classification_loss: 0.2542
1827/3000 [=================>............] - ETA: 8:01 - loss: 0.7216 - regression_loss: 0.4675 - classification_loss: 0.2541
1828/3000 [=================>............] - ETA: 8:01 - loss: 0.7216 - regression_loss: 0.4676 - classification_loss: 0.2540
1829/3000 [=================>............] - ETA: 8:01 - loss: 0.7215 - regression_loss: 0.4675 - classification_loss: 0.2540
1830/3000 [=================>............] - ETA: 8:00 - loss: 0.7216 - regression_loss: 0.4676 - classification_loss: 0.2540
1831/3000 [=================>............] - ETA: 8:00 - loss: 0.7214 - regression_loss: 0.4675 - classification_loss: 0.2539
1832/3000 [=================>............] - ETA: 7:59 - loss: 0.7213 - regression_loss: 0.4674 - classification_loss: 0.2539
1833/3000 [=================>............] - ETA: 7:59 - loss: 0.7213 - regression_loss: 0.4674 - classification_loss: 0.2539
1834/3000 [=================>............] - ETA: 7:59 - loss: 0.7212 - regression_loss: 0.4674 - classification_loss: 0.2538
1835/3000 [=================>............] - ETA: 7:58 - loss: 0.7211 - regression_loss: 0.4674 - classification_loss: 0.2538
1836/3000 [=================>............] - ETA: 7:58 - loss: 0.7211 - regression_loss: 0.4674 - classification_loss: 0.2537
1837/3000 [=================>............] - ETA: 7:57 - loss: 0.7211 - regression_loss: 0.4675 - classification_loss: 0.2537
1838/3000 [=================>............] - ETA: 7:57 - loss: 0.7211 - regression_loss: 0.4675 - classification_loss: 0.2536
1839/3000 [=================>............] - ETA: 7:56 - loss: 0.7211 - regression_loss: 0.4676 - classification_loss: 0.2536
1840/3000 [=================>............] - ETA: 7:56 - loss: 0.7209 - regression_loss: 0.4674 - classification_loss: 0.2535
1841/3000 [=================>............] - ETA: 7:56 - loss: 0.7210 - regression_loss: 0.4675 - classification_loss: 0.2535
1842/3000 [=================>............] - ETA: 7:55 - loss: 0.7208 - regression_loss: 0.4674 - classification_loss: 0.2534
1843/3000 [=================>............] - ETA: 7:55 - loss: 0.7210 - regression_loss: 0.4676 - classification_loss: 0.2534
1844/3000 [=================>............] - ETA: 7:54 - loss: 0.7210 - regression_loss: 0.4677 - classification_loss: 0.2534
1845/3000 [=================>............] - ETA: 7:54 - loss: 0.7210 - regression_loss: 0.4677 - classification_loss: 0.2534
1846/3000 [=================>............] - ETA: 7:54 - loss: 0.7209 - regression_loss: 0.4676 - classification_loss: 0.2533
1847/3000 [=================>............] - ETA: 7:53 - loss: 0.7208 - regression_loss: 0.4676 - classification_loss: 0.2532
1848/3000 [=================>............] - ETA: 7:53 - loss: 0.7208 - regression_loss: 0.4677 - classification_loss: 0.2531
1849/3000 [=================>............] - ETA: 7:52 - loss: 0.7206 - regression_loss: 0.4676 - classification_loss: 0.2530
1850/3000 [=================>............] - ETA: 7:52 - loss: 0.7206 - regression_loss: 0.4676 - classification_loss: 0.2530
1851/3000 [=================>............] - ETA: 7:51 - loss: 0.7206 - regression_loss: 0.4676 - classification_loss: 0.2529
1852/3000 [=================>............] - ETA: 7:51 - loss: 0.7207 - regression_loss: 0.4678 - classification_loss: 0.2529
1853/3000 [=================>............] - ETA: 7:51 - loss: 0.7208 - regression_loss: 0.4679 - classification_loss: 0.2528
1854/3000 [=================>............] - ETA: 7:50 - loss: 0.7207 - regression_loss: 0.4679 - classification_loss: 0.2528
1855/3000 [=================>............] - ETA: 7:50 - loss: 0.7206 - regression_loss: 0.4678 - classification_loss: 0.2528
1856/3000 [=================>............] - ETA: 7:49 - loss: 0.7205 - regression_loss: 0.4678 - classification_loss: 0.2527
1857/3000 [=================>............] - ETA: 7:49 - loss: 0.7205 - regression_loss: 0.4678 - classification_loss: 0.2527
1858/3000 [=================>............] - ETA: 7:49 - loss: 0.7205 - regression_loss: 0.4679 - classification_loss: 0.2526
1859/3000 [=================>............] - ETA: 7:48 - loss: 0.7206 - regression_loss: 0.4680 - classification_loss: 0.2526
1860/3000 [=================>............] - ETA: 7:48 - loss: 0.7205 - regression_loss: 0.4680 - classification_loss: 0.2525
1861/3000 [=================>............] - ETA: 7:47 - loss: 0.7204 - regression_loss: 0.4679 - classification_loss: 0.2525
1862/3000 [=================>............] - ETA: 7:47 - loss: 0.7204 - regression_loss: 0.4679 - classification_loss: 0.2525
1863/3000 [=================>............] - ETA: 7:47 - loss: 0.7203 - regression_loss: 0.4679 - classification_loss: 0.2524
1864/3000 [=================>............] - ETA: 7:46 - loss: 0.7204 - regression_loss: 0.4680 - classification_loss: 0.2524
1865/3000 [=================>............] - ETA: 7:46 - loss: 0.7203 - regression_loss: 0.4679 - classification_loss: 0.2524
1866/3000 [=================>............] - ETA: 7:45 - loss: 0.7204 - regression_loss: 0.4681 - classification_loss: 0.2523
1867/3000 [=================>............] - ETA: 7:45 - loss: 0.7202 - regression_loss: 0.4680 - classification_loss: 0.2522
1868/3000 [=================>............] - ETA: 7:45 - loss: 0.7202 - regression_loss: 0.4680 - classification_loss: 0.2522
1869/3000 [=================>............] - ETA: 7:44 - loss: 0.7201 - regression_loss: 0.4680 - classification_loss: 0.2522
1870/3000 [=================>............] - ETA: 7:44 - loss: 0.7200 - regression_loss: 0.4679 - classification_loss: 0.2521
1871/3000 [=================>............] - ETA: 7:43 - loss: 0.7199 - regression_loss: 0.4678 - classification_loss: 0.2520
1872/3000 [=================>............] - ETA: 7:43 - loss: 0.7197 - regression_loss: 0.4677 - classification_loss: 0.2520
1873/3000 [=================>............] - ETA: 7:42 - loss: 0.7197 - regression_loss: 0.4678 - classification_loss: 0.2519
1874/3000 [=================>............] - ETA: 7:42 - loss: 0.7198 - regression_loss: 0.4679 - classification_loss: 0.2519
1875/3000 [=================>............] - ETA: 7:42 - loss: 0.7197 - regression_loss: 0.4679 - classification_loss: 0.2518
1876/3000 [=================>............] - ETA: 7:41 - loss: 0.7195 - regression_loss: 0.4678 - classification_loss: 0.2517
1877/3000 [=================>............] - ETA: 7:41 - loss: 0.7193 - regression_loss: 0.4677 - classification_loss: 0.2516
1878/3000 [=================>............] - ETA: 7:40 - loss: 0.7191 - regression_loss: 0.4676 - classification_loss: 0.2515
1879/3000 [=================>............] - ETA: 7:40 - loss: 0.7190 - regression_loss: 0.4676 - classification_loss: 0.2514
1880/3000 [=================>............] - ETA: 7:40 - loss: 0.7188 - regression_loss: 0.4674 - classification_loss: 0.2514
1881/3000 [=================>............] - ETA: 7:39 - loss: 0.7187 - regression_loss: 0.4674 - classification_loss: 0.2514
1882/3000 [=================>............] - ETA: 7:39 - loss: 0.7185 - regression_loss: 0.4673 - classification_loss: 0.2513
1883/3000 [=================>............] - ETA: 7:38 - loss: 0.7186 - regression_loss: 0.4674 - classification_loss: 0.2512
1884/3000 [=================>............] - ETA: 7:38 - loss: 0.7185 - regression_loss: 0.4673 - classification_loss: 0.2511
1885/3000 [=================>............] - ETA: 7:37 - loss: 0.7183 - regression_loss: 0.4673 - classification_loss: 0.2510
1886/3000 [=================>............] - ETA: 7:37 - loss: 0.7182 - regression_loss: 0.4671 - classification_loss: 0.2510
1887/3000 [=================>............] - ETA: 7:37 - loss: 0.7181 - regression_loss: 0.4671 - classification_loss: 0.2510
1888/3000 [=================>............] - ETA: 7:36 - loss: 0.7181 - regression_loss: 0.4671 - classification_loss: 0.2510
1889/3000 [=================>............] - ETA: 7:36 - loss: 0.7184 - regression_loss: 0.4674 - classification_loss: 0.2509
1890/3000 [=================>............] - ETA: 7:35 - loss: 0.7184 - regression_loss: 0.4674 - classification_loss: 0.2510
1891/3000 [=================>............] - ETA: 7:35 - loss: 0.7183 - regression_loss: 0.4674 - classification_loss: 0.2509
1892/3000 [=================>............] - ETA: 7:35 - loss: 0.7181 - regression_loss: 0.4673 - classification_loss: 0.2508
1893/3000 [=================>............] - ETA: 7:34 - loss: 0.7180 - regression_loss: 0.4672 - classification_loss: 0.2508
1894/3000 [=================>............] - ETA: 7:34 - loss: 0.7179 - regression_loss: 0.4672 - classification_loss: 0.2507
1895/3000 [=================>............] - ETA: 7:33 - loss: 0.7179 - regression_loss: 0.4672 - classification_loss: 0.2507
1896/3000 [=================>............] - ETA: 7:33 - loss: 0.7178 - regression_loss: 0.4671 - classification_loss: 0.2507
1897/3000 [=================>............] - ETA: 7:33 - loss: 0.7179 - regression_loss: 0.4672 - classification_loss: 0.2507
1898/3000 [=================>............] - ETA: 7:32 - loss: 0.7179 - regression_loss: 0.4672 - classification_loss: 0.2507
1899/3000 [=================>............] - ETA: 7:32 - loss: 0.7179 - regression_loss: 0.4672 - classification_loss: 0.2507
1900/3000 [==================>...........] - ETA: 7:31 - loss: 0.7178 - regression_loss: 0.4672 - classification_loss: 0.2506
1901/3000 [==================>...........] - ETA: 7:31 - loss: 0.7176 - regression_loss: 0.4671 - classification_loss: 0.2505
1902/3000 [==================>...........] - ETA: 7:31 - loss: 0.7174 - regression_loss: 0.4670 - classification_loss: 0.2504
1903/3000 [==================>...........] - ETA: 7:30 - loss: 0.7173 - regression_loss: 0.4670 - classification_loss: 0.2504
1904/3000 [==================>...........] - ETA: 7:30 - loss: 0.7173 - regression_loss: 0.4670 - classification_loss: 0.2503
1905/3000 [==================>...........] - ETA: 7:29 - loss: 0.7173 - regression_loss: 0.4670 - classification_loss: 0.2503
1906/3000 [==================>...........] - ETA: 7:29 - loss: 0.7172 - regression_loss: 0.4670 - classification_loss: 0.2502
1907/3000 [==================>...........] - ETA: 7:28 - loss: 0.7171 - regression_loss: 0.4669 - classification_loss: 0.2502
1908/3000 [==================>...........] - ETA: 7:28 - loss: 0.7171 - regression_loss: 0.4670 - classification_loss: 0.2502
1909/3000 [==================>...........] - ETA: 7:28 - loss: 0.7169 - regression_loss: 0.4668 - classification_loss: 0.2501
1910/3000 [==================>...........] - ETA: 7:27 - loss: 0.7169 - regression_loss: 0.4669 - classification_loss: 0.2500
1911/3000 [==================>...........] - ETA: 7:27 - loss: 0.7168 - regression_loss: 0.4667 - classification_loss: 0.2500
1912/3000 [==================>...........] - ETA: 7:26 - loss: 0.7169 - regression_loss: 0.4669 - classification_loss: 0.2500
1913/3000 [==================>...........] - ETA: 7:26 - loss: 0.7168 - regression_loss: 0.4668 - classification_loss: 0.2500
1914/3000 [==================>...........] - ETA: 7:26 - loss: 0.7167 - regression_loss: 0.4667 - classification_loss: 0.2500
1915/3000 [==================>...........] - ETA: 7:25 - loss: 0.7166 - regression_loss: 0.4667 - classification_loss: 0.2499
1916/3000 [==================>...........] - ETA: 7:25 - loss: 0.7164 - regression_loss: 0.4666 - classification_loss: 0.2498
1917/3000 [==================>...........] - ETA: 7:24 - loss: 0.7164 - regression_loss: 0.4666 - classification_loss: 0.2498
1918/3000 [==================>...........] - ETA: 7:24 - loss: 0.7162 - regression_loss: 0.4664 - classification_loss: 0.2498
1919/3000 [==================>...........] - ETA: 7:24 - loss: 0.7163 - regression_loss: 0.4664 - classification_loss: 0.2499
1920/3000 [==================>...........] - ETA: 7:23 - loss: 0.7162 - regression_loss: 0.4663 - classification_loss: 0.2499
1921/3000 [==================>...........] - ETA: 7:23 - loss: 0.7162 - regression_loss: 0.4663 - classification_loss: 0.2499
1922/3000 [==================>...........] - ETA: 7:22 - loss: 0.7161 - regression_loss: 0.4663 - classification_loss: 0.2498
1923/3000 [==================>...........] - ETA: 7:22 - loss: 0.7159 - regression_loss: 0.4662 - classification_loss: 0.2498
1924/3000 [==================>...........] - ETA: 7:21 - loss: 0.7157 - regression_loss: 0.4660 - classification_loss: 0.2497
1925/3000 [==================>...........] - ETA: 7:21 - loss: 0.7155 - regression_loss: 0.4658 - classification_loss: 0.2496
1926/3000 [==================>...........] - ETA: 7:21 - loss: 0.7154 - regression_loss: 0.4658 - classification_loss: 0.2496
1927/3000 [==================>...........] - ETA: 7:20 - loss: 0.7152 - regression_loss: 0.4656 - classification_loss: 0.2496
1928/3000 [==================>...........] - ETA: 7:20 - loss: 0.7153 - regression_loss: 0.4657 - classification_loss: 0.2495
1929/3000 [==================>...........] - ETA: 7:19 - loss: 0.7152 - regression_loss: 0.4656 - classification_loss: 0.2495
1930/3000 [==================>...........] - ETA: 7:19 - loss: 0.7150 - regression_loss: 0.4655 - classification_loss: 0.2495
1931/3000 [==================>...........] - ETA: 7:19 - loss: 0.7149 - regression_loss: 0.4654 - classification_loss: 0.2495
1932/3000 [==================>...........] - ETA: 7:18 - loss: 0.7149 - regression_loss: 0.4655 - classification_loss: 0.2494
1933/3000 [==================>...........] - ETA: 7:18 - loss: 0.7148 - regression_loss: 0.4655 - classification_loss: 0.2494
1934/3000 [==================>...........] - ETA: 7:17 - loss: 0.7147 - regression_loss: 0.4654 - classification_loss: 0.2493
1935/3000 [==================>...........] - ETA: 7:17 - loss: 0.7148 - regression_loss: 0.4654 - classification_loss: 0.2493
1936/3000 [==================>...........] - ETA: 7:17 - loss: 0.7147 - regression_loss: 0.4654 - classification_loss: 0.2493
1937/3000 [==================>...........] - ETA: 7:16 - loss: 0.7146 - regression_loss: 0.4654 - classification_loss: 0.2493
1938/3000 [==================>...........] - ETA: 7:16 - loss: 0.7145 - regression_loss: 0.4653 - classification_loss: 0.2493
1939/3000 [==================>...........] - ETA: 7:15 - loss: 0.7144 - regression_loss: 0.4652 - classification_loss: 0.2492
1940/3000 [==================>...........] - ETA: 7:15 - loss: 0.7146 - regression_loss: 0.4654 - classification_loss: 0.2492
1941/3000 [==================>...........] - ETA: 7:14 - loss: 0.7144 - regression_loss: 0.4653 - classification_loss: 0.2491
1942/3000 [==================>...........] - ETA: 7:14 - loss: 0.7143 - regression_loss: 0.4652 - classification_loss: 0.2491
1943/3000 [==================>...........] - ETA: 7:14 - loss: 0.7144 - regression_loss: 0.4653 - classification_loss: 0.2491
1944/3000 [==================>...........] - ETA: 7:13 - loss: 0.7142 - regression_loss: 0.4652 - classification_loss: 0.2490
1945/3000 [==================>...........] - ETA: 7:13 - loss: 0.7142 - regression_loss: 0.4652 - classification_loss: 0.2490
1946/3000 [==================>...........] - ETA: 7:12 - loss: 0.7141 - regression_loss: 0.4651 - classification_loss: 0.2490
1947/3000 [==================>...........] - ETA: 7:12 - loss: 0.7140 - regression_loss: 0.4651 - classification_loss: 0.2489
1948/3000 [==================>...........] - ETA: 7:12 - loss: 0.7139 - regression_loss: 0.4650 - classification_loss: 0.2489
1949/3000 [==================>...........] - ETA: 7:11 - loss: 0.7137 - regression_loss: 0.4649 - classification_loss: 0.2488
1950/3000 [==================>...........] - ETA: 7:11 - loss: 0.7136 - regression_loss: 0.4649 - classification_loss: 0.2487
1951/3000 [==================>...........] - ETA: 7:10 - loss: 0.7135 - regression_loss: 0.4648 - classification_loss: 0.2487
1952/3000 [==================>...........] - ETA: 7:10 - loss: 0.7134 - regression_loss: 0.4648 - classification_loss: 0.2486
1953/3000 [==================>...........] - ETA: 7:10 - loss: 0.7134 - regression_loss: 0.4648 - classification_loss: 0.2486
1954/3000 [==================>...........] - ETA: 7:09 - loss: 0.7132 - regression_loss: 0.4647 - classification_loss: 0.2485
1955/3000 [==================>...........] - ETA: 7:09 - loss: 0.7132 - regression_loss: 0.4647 - classification_loss: 0.2485
1956/3000 [==================>...........] - ETA: 7:08 - loss: 0.7131 - regression_loss: 0.4646 - classification_loss: 0.2485
1957/3000 [==================>...........] - ETA: 7:08 - loss: 0.7129 - regression_loss: 0.4645 - classification_loss: 0.2485
1958/3000 [==================>...........] - ETA: 7:07 - loss: 0.7129 - regression_loss: 0.4645 - classification_loss: 0.2484
1959/3000 [==================>...........] - ETA: 7:07 - loss: 0.7129 - regression_loss: 0.4645 - classification_loss: 0.2484
1960/3000 [==================>...........] - ETA: 7:07 - loss: 0.7128 - regression_loss: 0.4645 - classification_loss: 0.2483
1961/3000 [==================>...........] - ETA: 7:06 - loss: 0.7128 - regression_loss: 0.4645 - classification_loss: 0.2483
1962/3000 [==================>...........] - ETA: 7:06 - loss: 0.7128 - regression_loss: 0.4646 - classification_loss: 0.2482
1963/3000 [==================>...........] - ETA: 7:05 - loss: 0.7129 - regression_loss: 0.4646 - classification_loss: 0.2482
1964/3000 [==================>...........] - ETA: 7:05 - loss: 0.7127 - regression_loss: 0.4646 - classification_loss: 0.2481
1965/3000 [==================>...........] - ETA: 7:05 - loss: 0.7127 - regression_loss: 0.4646 - classification_loss: 0.2481
1966/3000 [==================>...........] - ETA: 7:04 - loss: 0.7126 - regression_loss: 0.4645 - classification_loss: 0.2481
1967/3000 [==================>...........] - ETA: 7:04 - loss: 0.7125 - regression_loss: 0.4644 - classification_loss: 0.2480
1968/3000 [==================>...........] - ETA: 7:03 - loss: 0.7123 - regression_loss: 0.4643 - classification_loss: 0.2479
1969/3000 [==================>...........] - ETA: 7:03 - loss: 0.7122 - regression_loss: 0.4643 - classification_loss: 0.2479
1970/3000 [==================>...........] - ETA: 7:03 - loss: 0.7122 - regression_loss: 0.4643 - classification_loss: 0.2478
1971/3000 [==================>...........] - ETA: 7:02 - loss: 0.7122 - regression_loss: 0.4644 - classification_loss: 0.2478
1972/3000 [==================>...........] - ETA: 7:02 - loss: 0.7120 - regression_loss: 0.4643 - classification_loss: 0.2477
1973/3000 [==================>...........] - ETA: 7:01 - loss: 0.7119 - regression_loss: 0.4642 - classification_loss: 0.2477
1974/3000 [==================>...........] - ETA: 7:01 - loss: 0.7118 - regression_loss: 0.4642 - classification_loss: 0.2477
1975/3000 [==================>...........] - ETA: 7:00 - loss: 0.7118 - regression_loss: 0.4641 - classification_loss: 0.2476
1976/3000 [==================>...........] - ETA: 7:00 - loss: 0.7119 - regression_loss: 0.4643 - classification_loss: 0.2476
1977/3000 [==================>...........] - ETA: 7:00 - loss: 0.7120 - regression_loss: 0.4644 - classification_loss: 0.2475
1978/3000 [==================>...........] - ETA: 6:59 - loss: 0.7120 - regression_loss: 0.4645 - classification_loss: 0.2475
1979/3000 [==================>...........] - ETA: 6:59 - loss: 0.7120 - regression_loss: 0.4645 - classification_loss: 0.2475
1980/3000 [==================>...........] - ETA: 6:58 - loss: 0.7118 - regression_loss: 0.4645 - classification_loss: 0.2474
1981/3000 [==================>...........] - ETA: 6:58 - loss: 0.7118 - regression_loss: 0.4645 - classification_loss: 0.2474
1982/3000 [==================>...........] - ETA: 6:58 - loss: 0.7118 - regression_loss: 0.4645 - classification_loss: 0.2474
1983/3000 [==================>...........] - ETA: 6:57 - loss: 0.7118 - regression_loss: 0.4644 - classification_loss: 0.2473
1984/3000 [==================>...........] - ETA: 6:57 - loss: 0.7117 - regression_loss: 0.4644 - classification_loss: 0.2473
1985/3000 [==================>...........] - ETA: 6:56 - loss: 0.7117 - regression_loss: 0.4645 - classification_loss: 0.2472
1986/3000 [==================>...........] - ETA: 6:56 - loss: 0.7118 - regression_loss: 0.4646 - classification_loss: 0.2472
1987/3000 [==================>...........] - ETA: 6:56 - loss: 0.7120 - regression_loss: 0.4648 - classification_loss: 0.2472
1988/3000 [==================>...........] - ETA: 6:55 - loss: 0.7119 - regression_loss: 0.4647 - classification_loss: 0.2472
1989/3000 [==================>...........] - ETA: 6:55 - loss: 0.7119 - regression_loss: 0.4647 - classification_loss: 0.2472
1990/3000 [==================>...........] - ETA: 6:54 - loss: 0.7120 - regression_loss: 0.4649 - classification_loss: 0.2472
1991/3000 [==================>...........] - ETA: 6:54 - loss: 0.7120 - regression_loss: 0.4649 - classification_loss: 0.2471
1992/3000 [==================>...........] - ETA: 6:53 - loss: 0.7119 - regression_loss: 0.4649 - classification_loss: 0.2470
1993/3000 [==================>...........] - ETA: 6:53 - loss: 0.7118 - regression_loss: 0.4649 - classification_loss: 0.2470
1994/3000 [==================>...........] - ETA: 6:53 - loss: 0.7118 - regression_loss: 0.4648 - classification_loss: 0.2469
1995/3000 [==================>...........] - ETA: 6:52 - loss: 0.7116 - regression_loss: 0.4648 - classification_loss: 0.2469
1996/3000 [==================>...........] - ETA: 6:52 - loss: 0.7115 - regression_loss: 0.4647 - classification_loss: 0.2468
1997/3000 [==================>...........] - ETA: 6:51 - loss: 0.7115 - regression_loss: 0.4647 - classification_loss: 0.2468
1998/3000 [==================>...........] - ETA: 6:51 - loss: 0.7116 - regression_loss: 0.4648 - classification_loss: 0.2468
1999/3000 [==================>...........] - ETA: 6:51 - loss: 0.7114 - regression_loss: 0.4647 - classification_loss: 0.2467
2000/3000 [===================>..........] - ETA: 6:50 - loss: 0.7114 - regression_loss: 0.4647 - classification_loss: 0.2467
2001/3000 [===================>..........] - ETA: 6:50 - loss: 0.7114 - regression_loss: 0.4647 - classification_loss: 0.2467
2002/3000 [===================>..........] - ETA: 6:49 - loss: 0.7113 - regression_loss: 0.4647 - classification_loss: 0.2466
2003/3000 [===================>..........] - ETA: 6:49 - loss: 0.7112 - regression_loss: 0.4646 - classification_loss: 0.2466
2004/3000 [===================>..........] - ETA: 6:49 - loss: 0.7110 - regression_loss: 0.4645 - classification_loss: 0.2465
2005/3000 [===================>..........] - ETA: 6:48 - loss: 0.7112 - regression_loss: 0.4648 - classification_loss: 0.2464
2006/3000 [===================>..........] - ETA: 6:48 - loss: 0.7112 - regression_loss: 0.4649 - classification_loss: 0.2463
2007/3000 [===================>..........] - ETA: 6:47 - loss: 0.7112 - regression_loss: 0.4649 - classification_loss: 0.2463
2008/3000 [===================>..........] - ETA: 6:47 - loss: 0.7112 - regression_loss: 0.4650 - classification_loss: 0.2462
2009/3000 [===================>..........] - ETA: 6:47 - loss: 0.7111 - regression_loss: 0.4649 - classification_loss: 0.2462
2010/3000 [===================>..........] - ETA: 6:46 - loss: 0.7111 - regression_loss: 0.4648 - classification_loss: 0.2462
2011/3000 [===================>..........] - ETA: 6:46 - loss: 0.7111 - regression_loss: 0.4649 - classification_loss: 0.2462
2012/3000 [===================>..........] - ETA: 6:45 - loss: 0.7111 - regression_loss: 0.4649 - classification_loss: 0.2461
2013/3000 [===================>..........] - ETA: 6:45 - loss: 0.7111 - regression_loss: 0.4649 - classification_loss: 0.2462
2014/3000 [===================>..........] - ETA: 6:44 - loss: 0.7110 - regression_loss: 0.4648 - classification_loss: 0.2462
2015/3000 [===================>..........] - ETA: 6:44 - loss: 0.7109 - regression_loss: 0.4648 - classification_loss: 0.2461
2016/3000 [===================>..........] - ETA: 6:44 - loss: 0.7108 - regression_loss: 0.4647 - classification_loss: 0.2462
2017/3000 [===================>..........] - ETA: 6:43 - loss: 0.7109 - regression_loss: 0.4648 - classification_loss: 0.2461
2018/3000 [===================>..........] - ETA: 6:43 - loss: 0.7109 - regression_loss: 0.4648 - classification_loss: 0.2461
2019/3000 [===================>..........] - ETA: 6:42 - loss: 0.7108 - regression_loss: 0.4647 - classification_loss: 0.2461
2020/3000 [===================>..........] - ETA: 6:42 - loss: 0.7107 - regression_loss: 0.4646 - classification_loss: 0.2461
2021/3000 [===================>..........] - ETA: 6:42 - loss: 0.7106 - regression_loss: 0.4646 - classification_loss: 0.2460
2022/3000 [===================>..........] - ETA: 6:41 - loss: 0.7106 - regression_loss: 0.4646 - classification_loss: 0.2460
2023/3000 [===================>..........] - ETA: 6:41 - loss: 0.7104 - regression_loss: 0.4645 - classification_loss: 0.2459
2024/3000 [===================>..........] - ETA: 6:40 - loss: 0.7102 - regression_loss: 0.4644 - classification_loss: 0.2458
2025/3000 [===================>..........] - ETA: 6:40 - loss: 0.7102 - regression_loss: 0.4643 - classification_loss: 0.2459
2026/3000 [===================>..........] - ETA: 6:40 - loss: 0.7104 - regression_loss: 0.4645 - classification_loss: 0.2458
2027/3000 [===================>..........] - ETA: 6:39 - loss: 0.7104 - regression_loss: 0.4646 - classification_loss: 0.2458
2028/3000 [===================>..........] - ETA: 6:39 - loss: 0.7102 - regression_loss: 0.4644 - classification_loss: 0.2458
2029/3000 [===================>..........] - ETA: 6:38 - loss: 0.7101 - regression_loss: 0.4643 - classification_loss: 0.2457
2030/3000 [===================>..........] - ETA: 6:38 - loss: 0.7100 - regression_loss: 0.4643 - classification_loss: 0.2457
2031/3000 [===================>..........] - ETA: 6:38 - loss: 0.7098 - regression_loss: 0.4641 - classification_loss: 0.2456
2032/3000 [===================>..........] - ETA: 6:37 - loss: 0.7097 - regression_loss: 0.4641 - classification_loss: 0.2456
2033/3000 [===================>..........] - ETA: 6:37 - loss: 0.7097 - regression_loss: 0.4642 - classification_loss: 0.2456
2034/3000 [===================>..........] - ETA: 6:36 - loss: 0.7097 - regression_loss: 0.4642 - classification_loss: 0.2456
2035/3000 [===================>..........] - ETA: 6:36 - loss: 0.7096 - regression_loss: 0.4641 - classification_loss: 0.2455
2036/3000 [===================>..........] - ETA: 6:35 - loss: 0.7095 - regression_loss: 0.4641 - classification_loss: 0.2454
2037/3000 [===================>..........] - ETA: 6:35 - loss: 0.7095 - regression_loss: 0.4640 - classification_loss: 0.2454
2038/3000 [===================>..........] - ETA: 6:35 - loss: 0.7094 - regression_loss: 0.4640 - classification_loss: 0.2454
2039/3000 [===================>..........] - ETA: 6:34 - loss: 0.7093 - regression_loss: 0.4639 - classification_loss: 0.2454
2040/3000 [===================>..........] - ETA: 6:34 - loss: 0.7093 - regression_loss: 0.4640 - classification_loss: 0.2453
2041/3000 [===================>..........] - ETA: 6:33 - loss: 0.7091 - regression_loss: 0.4638 - classification_loss: 0.2453
2042/3000 [===================>..........] - ETA: 6:33 - loss: 0.7090 - regression_loss: 0.4638 - classification_loss: 0.2452
2043/3000 [===================>..........] - ETA: 6:33 - loss: 0.7089 - regression_loss: 0.4637 - classification_loss: 0.2452
2044/3000 [===================>..........] - ETA: 6:32 - loss: 0.7089 - regression_loss: 0.4637 - classification_loss: 0.2451
2045/3000 [===================>..........] - ETA: 6:32 - loss: 0.7087 - regression_loss: 0.4636 - classification_loss: 0.2450
2046/3000 [===================>..........] - ETA: 6:31 - loss: 0.7085 - regression_loss: 0.4635 - classification_loss: 0.2450
2047/3000 [===================>..........] - ETA: 6:31 - loss: 0.7084 - regression_loss: 0.4634 - classification_loss: 0.2450
2048/3000 [===================>..........] - ETA: 6:31 - loss: 0.7082 - regression_loss: 0.4633 - classification_loss: 0.2449
2049/3000 [===================>..........] - ETA: 6:30 - loss: 0.7082 - regression_loss: 0.4633 - classification_loss: 0.2449
2050/3000 [===================>..........] - ETA: 6:30 - loss: 0.7081 - regression_loss: 0.4632 - classification_loss: 0.2448
2051/3000 [===================>..........] - ETA: 6:29 - loss: 0.7080 - regression_loss: 0.4631 - classification_loss: 0.2448
2052/3000 [===================>..........] - ETA: 6:29 - loss: 0.7079 - regression_loss: 0.4631 - classification_loss: 0.2448
2053/3000 [===================>..........] - ETA: 6:28 - loss: 0.7078 - regression_loss: 0.4630 - classification_loss: 0.2448
2054/3000 [===================>..........] - ETA: 6:28 - loss: 0.7076 - regression_loss: 0.4630 - classification_loss: 0.2447
2055/3000 [===================>..........] - ETA: 6:28 - loss: 0.7075 - regression_loss: 0.4629 - classification_loss: 0.2446
2056/3000 [===================>..........] - ETA: 6:27 - loss: 0.7075 - regression_loss: 0.4629 - classification_loss: 0.2446
2057/3000 [===================>..........] - ETA: 6:27 - loss: 0.7073 - regression_loss: 0.4628 - classification_loss: 0.2445
2058/3000 [===================>..........] - ETA: 6:26 - loss: 0.7071 - regression_loss: 0.4627 - classification_loss: 0.2444
2059/3000 [===================>..........] - ETA: 6:26 - loss: 0.7070 - regression_loss: 0.4627 - classification_loss: 0.2444
2060/3000 [===================>..........] - ETA: 6:26 - loss: 0.7068 - regression_loss: 0.4625 - classification_loss: 0.2443
2061/3000 [===================>..........] - ETA: 6:25 - loss: 0.7068 - regression_loss: 0.4626 - classification_loss: 0.2442
2062/3000 [===================>..........] - ETA: 6:25 - loss: 0.7066 - regression_loss: 0.4624 - classification_loss: 0.2441
2063/3000 [===================>..........] - ETA: 6:24 - loss: 0.7064 - regression_loss: 0.4623 - classification_loss: 0.2441
2064/3000 [===================>..........] - ETA: 6:24 - loss: 0.7064 - regression_loss: 0.4623 - classification_loss: 0.2441
2065/3000 [===================>..........] - ETA: 6:23 - loss: 0.7063 - regression_loss: 0.4623 - classification_loss: 0.2440
2066/3000 [===================>..........] - ETA: 6:23 - loss: 0.7062 - regression_loss: 0.4622 - classification_loss: 0.2440
2067/3000 [===================>..........] - ETA: 6:23 - loss: 0.7062 - regression_loss: 0.4623 - classification_loss: 0.2439
2068/3000 [===================>..........] - ETA: 6:22 - loss: 0.7062 - regression_loss: 0.4623 - classification_loss: 0.2439
2069/3000 [===================>..........] - ETA: 6:22 - loss: 0.7061 - regression_loss: 0.4622 - classification_loss: 0.2438
2070/3000 [===================>..........] - ETA: 6:21 - loss: 0.7061 - regression_loss: 0.4623 - classification_loss: 0.2439
2071/3000 [===================>..........] - ETA: 6:21 - loss: 0.7060 - regression_loss: 0.4622 - classification_loss: 0.2438
2072/3000 [===================>..........] - ETA: 6:21 - loss: 0.7059 - regression_loss: 0.4621 - classification_loss: 0.2438
2073/3000 [===================>..........] - ETA: 6:20 - loss: 0.7057 - regression_loss: 0.4620 - classification_loss: 0.2437
2074/3000 [===================>..........] - ETA: 6:20 - loss: 0.7055 - regression_loss: 0.4619 - classification_loss: 0.2436
2075/3000 [===================>..........] - ETA: 6:19 - loss: 0.7053 - regression_loss: 0.4618 - classification_loss: 0.2435
2076/3000 [===================>..........] - ETA: 6:19 - loss: 0.7052 - regression_loss: 0.4617 - classification_loss: 0.2435
2077/3000 [===================>..........] - ETA: 6:19 - loss: 0.7051 - regression_loss: 0.4617 - classification_loss: 0.2435
2078/3000 [===================>..........] - ETA: 6:18 - loss: 0.7050 - regression_loss: 0.4616 - classification_loss: 0.2435
2079/3000 [===================>..........] - ETA: 6:18 - loss: 0.7050 - regression_loss: 0.4615 - classification_loss: 0.2434
2080/3000 [===================>..........] - ETA: 6:17 - loss: 0.7048 - regression_loss: 0.4614 - classification_loss: 0.2434
2081/3000 [===================>..........] - ETA: 6:17 - loss: 0.7048 - regression_loss: 0.4614 - classification_loss: 0.2434
2082/3000 [===================>..........] - ETA: 6:17 - loss: 0.7047 - regression_loss: 0.4614 - classification_loss: 0.2433
2083/3000 [===================>..........] - ETA: 6:16 - loss: 0.7047 - regression_loss: 0.4614 - classification_loss: 0.2433
2084/3000 [===================>..........] - ETA: 6:16 - loss: 0.7046 - regression_loss: 0.4614 - classification_loss: 0.2432
2085/3000 [===================>..........] - ETA: 6:15 - loss: 0.7045 - regression_loss: 0.4614 - classification_loss: 0.2431
2086/3000 [===================>..........] - ETA: 6:15 - loss: 0.7044 - regression_loss: 0.4612 - classification_loss: 0.2431
2087/3000 [===================>..........] - ETA: 6:14 - loss: 0.7042 - regression_loss: 0.4611 - classification_loss: 0.2430
2088/3000 [===================>..........] - ETA: 6:14 - loss: 0.7041 - regression_loss: 0.4611 - classification_loss: 0.2430
2089/3000 [===================>..........] - ETA: 6:14 - loss: 0.7040 - regression_loss: 0.4610 - classification_loss: 0.2430
2090/3000 [===================>..........] - ETA: 6:13 - loss: 0.7038 - regression_loss: 0.4609 - classification_loss: 0.2429
2091/3000 [===================>..........] - ETA: 6:13 - loss: 0.7037 - regression_loss: 0.4608 - classification_loss: 0.2429
2092/3000 [===================>..........] - ETA: 6:12 - loss: 0.7036 - regression_loss: 0.4607 - classification_loss: 0.2429
2093/3000 [===================>..........] - ETA: 6:12 - loss: 0.7035 - regression_loss: 0.4607 - classification_loss: 0.2428
2094/3000 [===================>..........] - ETA: 6:12 - loss: 0.7035 - regression_loss: 0.4608 - classification_loss: 0.2427
2095/3000 [===================>..........] - ETA: 6:11 - loss: 0.7036 - regression_loss: 0.4609 - classification_loss: 0.2427
2096/3000 [===================>..........] - ETA: 6:11 - loss: 0.7034 - regression_loss: 0.4608 - classification_loss: 0.2426
2097/3000 [===================>..........] - ETA: 6:10 - loss: 0.7032 - regression_loss: 0.4607 - classification_loss: 0.2426
2098/3000 [===================>..........] - ETA: 6:10 - loss: 0.7031 - regression_loss: 0.4606 - classification_loss: 0.2425
2099/3000 [===================>..........] - ETA: 6:10 - loss: 0.7029 - regression_loss: 0.4605 - classification_loss: 0.2424
2100/3000 [====================>.........] - ETA: 6:09 - loss: 0.7029 - regression_loss: 0.4605 - classification_loss: 0.2424
2101/3000 [====================>.........] - ETA: 6:09 - loss: 0.7028 - regression_loss: 0.4605 - classification_loss: 0.2423
2102/3000 [====================>.........] - ETA: 6:08 - loss: 0.7026 - regression_loss: 0.4603 - classification_loss: 0.2423
2103/3000 [====================>.........] - ETA: 6:08 - loss: 0.7026 - regression_loss: 0.4603 - classification_loss: 0.2423
2104/3000 [====================>.........] - ETA: 6:07 - loss: 0.7026 - regression_loss: 0.4603 - classification_loss: 0.2422
2105/3000 [====================>.........] - ETA: 6:07 - loss: 0.7024 - regression_loss: 0.4603 - classification_loss: 0.2422
2106/3000 [====================>.........] - ETA: 6:07 - loss: 0.7023 - regression_loss: 0.4602 - classification_loss: 0.2421
2107/3000 [====================>.........] - ETA: 6:06 - loss: 0.7022 - regression_loss: 0.4602 - classification_loss: 0.2421
2108/3000 [====================>.........] - ETA: 6:06 - loss: 0.7022 - regression_loss: 0.4601 - classification_loss: 0.2420
2109/3000 [====================>.........] - ETA: 6:05 - loss: 0.7020 - regression_loss: 0.4601 - classification_loss: 0.2420
2110/3000 [====================>.........] - ETA: 6:05 - loss: 0.7019 - regression_loss: 0.4600 - classification_loss: 0.2419
2111/3000 [====================>.........] - ETA: 6:05 - loss: 0.7019 - regression_loss: 0.4601 - classification_loss: 0.2418
2112/3000 [====================>.........] - ETA: 6:04 - loss: 0.7019 - regression_loss: 0.4601 - classification_loss: 0.2418
2113/3000 [====================>.........] - ETA: 6:04 - loss: 0.7018 - regression_loss: 0.4601 - classification_loss: 0.2417
2114/3000 [====================>.........] - ETA: 6:03 - loss: 0.7016 - regression_loss: 0.4600 - classification_loss: 0.2416
2115/3000 [====================>.........] - ETA: 6:03 - loss: 0.7015 - regression_loss: 0.4599 - classification_loss: 0.2416
2116/3000 [====================>.........] - ETA: 6:03 - loss: 0.7015 - regression_loss: 0.4600 - classification_loss: 0.2415
2117/3000 [====================>.........] - ETA: 6:02 - loss: 0.7015 - regression_loss: 0.4601 - classification_loss: 0.2415
2118/3000 [====================>.........] - ETA: 6:02 - loss: 0.7014 - regression_loss: 0.4600 - classification_loss: 0.2414
2119/3000 [====================>.........] - ETA: 6:01 - loss: 0.7013 - regression_loss: 0.4600 - classification_loss: 0.2413
2120/3000 [====================>.........] - ETA: 6:01 - loss: 0.7012 - regression_loss: 0.4599 - classification_loss: 0.2413
2121/3000 [====================>.........] - ETA: 6:01 - loss: 0.7011 - regression_loss: 0.4599 - classification_loss: 0.2413
2122/3000 [====================>.........] - ETA: 6:00 - loss: 0.7010 - regression_loss: 0.4598 - classification_loss: 0.2412
2123/3000 [====================>.........] - ETA: 6:00 - loss: 0.7010 - regression_loss: 0.4598 - classification_loss: 0.2411
2124/3000 [====================>.........] - ETA: 5:59 - loss: 0.7010 - regression_loss: 0.4599 - classification_loss: 0.2411
2125/3000 [====================>.........] - ETA: 5:59 - loss: 0.7009 - regression_loss: 0.4599 - classification_loss: 0.2410
2126/3000 [====================>.........] - ETA: 5:58 - loss: 0.7010 - regression_loss: 0.4600 - classification_loss: 0.2410
2127/3000 [====================>.........] - ETA: 5:58 - loss: 0.7009 - regression_loss: 0.4600 - classification_loss: 0.2409
2128/3000 [====================>.........] - ETA: 5:58 - loss: 0.7008 - regression_loss: 0.4599 - classification_loss: 0.2409
2129/3000 [====================>.........] - ETA: 5:57 - loss: 0.7008 - regression_loss: 0.4599 - classification_loss: 0.2408
2130/3000 [====================>.........] - ETA: 5:57 - loss: 0.7006 - regression_loss: 0.4598 - classification_loss: 0.2408
2131/3000 [====================>.........] - ETA: 5:56 - loss: 0.7005 - regression_loss: 0.4598 - classification_loss: 0.2407
2132/3000 [====================>.........] - ETA: 5:56 - loss: 0.7007 - regression_loss: 0.4600 - classification_loss: 0.2407
2133/3000 [====================>.........] - ETA: 5:56 - loss: 0.7006 - regression_loss: 0.4599 - classification_loss: 0.2407
2134/3000 [====================>.........] - ETA: 5:55 - loss: 0.7007 - regression_loss: 0.4600 - classification_loss: 0.2407
2135/3000 [====================>.........] - ETA: 5:55 - loss: 0.7005 - regression_loss: 0.4599 - classification_loss: 0.2406
2136/3000 [====================>.........] - ETA: 5:54 - loss: 0.7004 - regression_loss: 0.4598 - classification_loss: 0.2406
2137/3000 [====================>.........] - ETA: 5:54 - loss: 0.7003 - regression_loss: 0.4598 - classification_loss: 0.2405
2138/3000 [====================>.........] - ETA: 5:54 - loss: 0.7002 - regression_loss: 0.4597 - classification_loss: 0.2405
2139/3000 [====================>.........] - ETA: 5:53 - loss: 0.7001 - regression_loss: 0.4597 - classification_loss: 0.2405
2140/3000 [====================>.........] - ETA: 5:53 - loss: 0.7001 - regression_loss: 0.4596 - classification_loss: 0.2404
2141/3000 [====================>.........] - ETA: 5:52 - loss: 0.7001 - regression_loss: 0.4596 - classification_loss: 0.2404
2142/3000 [====================>.........] - ETA: 5:52 - loss: 0.6999 - regression_loss: 0.4596 - classification_loss: 0.2404
2143/3000 [====================>.........] - ETA: 5:51 - loss: 0.6998 - regression_loss: 0.4595 - classification_loss: 0.2403
2144/3000 [====================>.........] - ETA: 5:51 - loss: 0.6998 - regression_loss: 0.4595 - classification_loss: 0.2403
2145/3000 [====================>.........] - ETA: 5:51 - loss: 0.6996 - regression_loss: 0.4593 - classification_loss: 0.2403
2146/3000 [====================>.........] - ETA: 5:50 - loss: 0.6995 - regression_loss: 0.4592 - classification_loss: 0.2403
2147/3000 [====================>.........] - ETA: 5:50 - loss: 0.6994 - regression_loss: 0.4592 - classification_loss: 0.2402
2148/3000 [====================>.........] - ETA: 5:49 - loss: 0.6993 - regression_loss: 0.4592 - classification_loss: 0.2401
2149/3000 [====================>.........] - ETA: 5:49 - loss: 0.6991 - regression_loss: 0.4590 - classification_loss: 0.2401
2150/3000 [====================>.........] - ETA: 5:49 - loss: 0.6992 - regression_loss: 0.4591 - classification_loss: 0.2401
2151/3000 [====================>.........] - ETA: 5:48 - loss: 0.6990 - regression_loss: 0.4590 - classification_loss: 0.2400
2152/3000 [====================>.........] - ETA: 5:48 - loss: 0.6990 - regression_loss: 0.4590 - classification_loss: 0.2400
2153/3000 [====================>.........] - ETA: 5:47 - loss: 0.6988 - regression_loss: 0.4589 - classification_loss: 0.2399
2154/3000 [====================>.........] - ETA: 5:47 - loss: 0.6988 - regression_loss: 0.4589 - classification_loss: 0.2399
2155/3000 [====================>.........] - ETA: 5:47 - loss: 0.6987 - regression_loss: 0.4588 - classification_loss: 0.2399
2156/3000 [====================>.........] - ETA: 5:46 - loss: 0.6987 - regression_loss: 0.4589 - classification_loss: 0.2398
2157/3000 [====================>.........] - ETA: 5:46 - loss: 0.6986 - regression_loss: 0.4588 - classification_loss: 0.2397
2158/3000 [====================>.........] - ETA: 5:45 - loss: 0.6985 - regression_loss: 0.4587 - classification_loss: 0.2397
2159/3000 [====================>.........] - ETA: 5:45 - loss: 0.6984 - regression_loss: 0.4587 - classification_loss: 0.2397
2160/3000 [====================>.........] - ETA: 5:44 - loss: 0.6983 - regression_loss: 0.4586 - classification_loss: 0.2396
2161/3000 [====================>.........] - ETA: 5:44 - loss: 0.6982 - regression_loss: 0.4586 - classification_loss: 0.2396
2162/3000 [====================>.........] - ETA: 5:44 - loss: 0.6982 - regression_loss: 0.4586 - classification_loss: 0.2396
2163/3000 [====================>.........] - ETA: 5:43 - loss: 0.6982 - regression_loss: 0.4587 - classification_loss: 0.2396
2164/3000 [====================>.........] - ETA: 5:43 - loss: 0.6981 - regression_loss: 0.4586 - classification_loss: 0.2395
2165/3000 [====================>.........] - ETA: 5:42 - loss: 0.6980 - regression_loss: 0.4585 - classification_loss: 0.2394
2166/3000 [====================>.........] - ETA: 5:42 - loss: 0.6979 - regression_loss: 0.4586 - classification_loss: 0.2394
2167/3000 [====================>.........] - ETA: 5:42 - loss: 0.6978 - regression_loss: 0.4585 - classification_loss: 0.2393
2168/3000 [====================>.........] - ETA: 5:41 - loss: 0.6978 - regression_loss: 0.4585 - classification_loss: 0.2393
2169/3000 [====================>.........] - ETA: 5:41 - loss: 0.6978 - regression_loss: 0.4585 - classification_loss: 0.2393
2170/3000 [====================>.........] - ETA: 5:40 - loss: 0.6977 - regression_loss: 0.4585 - classification_loss: 0.2393
2171/3000 [====================>.........] - ETA: 5:40 - loss: 0.6977 - regression_loss: 0.4585 - classification_loss: 0.2392
2172/3000 [====================>.........] - ETA: 5:40 - loss: 0.6976 - regression_loss: 0.4584 - classification_loss: 0.2392
2173/3000 [====================>.........] - ETA: 5:39 - loss: 0.6977 - regression_loss: 0.4585 - classification_loss: 0.2391
2174/3000 [====================>.........] - ETA: 5:39 - loss: 0.6976 - regression_loss: 0.4585 - classification_loss: 0.2391
2175/3000 [====================>.........] - ETA: 5:38 - loss: 0.6974 - regression_loss: 0.4585 - classification_loss: 0.2390
2176/3000 [====================>.........] - ETA: 5:38 - loss: 0.6973 - regression_loss: 0.4584 - classification_loss: 0.2389
2177/3000 [====================>.........] - ETA: 5:38 - loss: 0.6973 - regression_loss: 0.4584 - classification_loss: 0.2389
2178/3000 [====================>.........] - ETA: 5:37 - loss: 0.6972 - regression_loss: 0.4583 - classification_loss: 0.2389
2179/3000 [====================>.........] - ETA: 5:37 - loss: 0.6972 - regression_loss: 0.4583 - classification_loss: 0.2389
2180/3000 [====================>.........] - ETA: 5:36 - loss: 0.6973 - regression_loss: 0.4585 - classification_loss: 0.2388
2181/3000 [====================>.........] - ETA: 5:36 - loss: 0.6974 - regression_loss: 0.4586 - classification_loss: 0.2388
2182/3000 [====================>.........] - ETA: 5:35 - loss: 0.6973 - regression_loss: 0.4585 - classification_loss: 0.2388
2183/3000 [====================>.........] - ETA: 5:35 - loss: 0.6972 - regression_loss: 0.4585 - classification_loss: 0.2387
2184/3000 [====================>.........] - ETA: 5:35 - loss: 0.6971 - regression_loss: 0.4584 - classification_loss: 0.2387
2185/3000 [====================>.........] - ETA: 5:34 - loss: 0.6971 - regression_loss: 0.4584 - classification_loss: 0.2386
2186/3000 [====================>.........] - ETA: 5:34 - loss: 0.6970 - regression_loss: 0.4584 - classification_loss: 0.2386
2187/3000 [====================>.........] - ETA: 5:33 - loss: 0.6968 - regression_loss: 0.4584 - classification_loss: 0.2385
2188/3000 [====================>.........] - ETA: 5:33 - loss: 0.6968 - regression_loss: 0.4583 - classification_loss: 0.2384
2189/3000 [====================>.........] - ETA: 5:33 - loss: 0.6967 - regression_loss: 0.4583 - classification_loss: 0.2384
2190/3000 [====================>.........] - ETA: 5:32 - loss: 0.6966 - regression_loss: 0.4583 - classification_loss: 0.2383
2191/3000 [====================>.........] - ETA: 5:32 - loss: 0.6965 - regression_loss: 0.4583 - classification_loss: 0.2383
2192/3000 [====================>.........] - ETA: 5:31 - loss: 0.6965 - regression_loss: 0.4583 - classification_loss: 0.2382
2193/3000 [====================>.........] - ETA: 5:31 - loss: 0.6964 - regression_loss: 0.4583 - classification_loss: 0.2382
2194/3000 [====================>.........] - ETA: 5:31 - loss: 0.6964 - regression_loss: 0.4583 - classification_loss: 0.2381
2195/3000 [====================>.........] - ETA: 5:30 - loss: 0.6964 - regression_loss: 0.4583 - classification_loss: 0.2380
2196/3000 [====================>.........] - ETA: 5:30 - loss: 0.6963 - regression_loss: 0.4583 - classification_loss: 0.2380
2197/3000 [====================>.........] - ETA: 5:29 - loss: 0.6961 - regression_loss: 0.4582 - classification_loss: 0.2379
2198/3000 [====================>.........] - ETA: 5:29 - loss: 0.6960 - regression_loss: 0.4582 - classification_loss: 0.2378
2199/3000 [====================>.........] - ETA: 5:28 - loss: 0.6960 - regression_loss: 0.4582 - classification_loss: 0.2378
2200/3000 [=====================>........] - ETA: 5:28 - loss: 0.6961 - regression_loss: 0.4582 - classification_loss: 0.2379
2201/3000 [=====================>........] - ETA: 5:28 - loss: 0.6960 - regression_loss: 0.4581 - classification_loss: 0.2378
2202/3000 [=====================>........] - ETA: 5:27 - loss: 0.6960 - regression_loss: 0.4582 - classification_loss: 0.2378
2203/3000 [=====================>........] - ETA: 5:27 - loss: 0.6960 - regression_loss: 0.4582 - classification_loss: 0.2377
2204/3000 [=====================>........] - ETA: 5:26 - loss: 0.6959 - regression_loss: 0.4582 - classification_loss: 0.2377
2205/3000 [=====================>........] - ETA: 5:26 - loss: 0.6958 - regression_loss: 0.4581 - classification_loss: 0.2376
2206/3000 [=====================>........] - ETA: 5:26 - loss: 0.6957 - regression_loss: 0.4581 - classification_loss: 0.2376
2207/3000 [=====================>........] - ETA: 5:25 - loss: 0.6956 - regression_loss: 0.4581 - classification_loss: 0.2375
2208/3000 [=====================>........] - ETA: 5:25 - loss: 0.6957 - regression_loss: 0.4582 - classification_loss: 0.2375
2209/3000 [=====================>........] - ETA: 5:24 - loss: 0.6957 - regression_loss: 0.4582 - classification_loss: 0.2374
2210/3000 [=====================>........] - ETA: 5:24 - loss: 0.6957 - regression_loss: 0.4583 - classification_loss: 0.2374
2211/3000 [=====================>........] - ETA: 5:24 - loss: 0.6957 - regression_loss: 0.4583 - classification_loss: 0.2374
2212/3000 [=====================>........] - ETA: 5:23 - loss: 0.6955 - regression_loss: 0.4582 - classification_loss: 0.2374
2213/3000 [=====================>........] - ETA: 5:23 - loss: 0.6955 - regression_loss: 0.4582 - classification_loss: 0.2373
2214/3000 [=====================>........] - ETA: 5:22 - loss: 0.6954 - regression_loss: 0.4581 - classification_loss: 0.2373
2215/3000 [=====================>........] - ETA: 5:22 - loss: 0.6954 - regression_loss: 0.4582 - classification_loss: 0.2372
2216/3000 [=====================>........] - ETA: 5:21 - loss: 0.6952 - regression_loss: 0.4581 - classification_loss: 0.2371
2217/3000 [=====================>........] - ETA: 5:21 - loss: 0.6951 - regression_loss: 0.4580 - classification_loss: 0.2370
2218/3000 [=====================>........] - ETA: 5:21 - loss: 0.6950 - regression_loss: 0.4580 - classification_loss: 0.2370
2219/3000 [=====================>........] - ETA: 5:20 - loss: 0.6950 - regression_loss: 0.4581 - classification_loss: 0.2369
2220/3000 [=====================>........] - ETA: 5:20 - loss: 0.6949 - regression_loss: 0.4580 - classification_loss: 0.2369
2221/3000 [=====================>........] - ETA: 5:19 - loss: 0.6948 - regression_loss: 0.4580 - classification_loss: 0.2368
2222/3000 [=====================>........] - ETA: 5:19 - loss: 0.6948 - regression_loss: 0.4580 - classification_loss: 0.2368
2223/3000 [=====================>........] - ETA: 5:19 - loss: 0.6947 - regression_loss: 0.4581 - classification_loss: 0.2367
2224/3000 [=====================>........] - ETA: 5:18 - loss: 0.6948 - regression_loss: 0.4581 - classification_loss: 0.2366
2225/3000 [=====================>........] - ETA: 5:18 - loss: 0.6946 - regression_loss: 0.4581 - classification_loss: 0.2366
2226/3000 [=====================>........] - ETA: 5:17 - loss: 0.6946 - regression_loss: 0.4581 - classification_loss: 0.2365
2227/3000 [=====================>........] - ETA: 5:17 - loss: 0.6946 - regression_loss: 0.4581 - classification_loss: 0.2365
2228/3000 [=====================>........] - ETA: 5:17 - loss: 0.6946 - regression_loss: 0.4582 - classification_loss: 0.2364
2229/3000 [=====================>........] - ETA: 5:16 - loss: 0.6945 - regression_loss: 0.4581 - classification_loss: 0.2364
2230/3000 [=====================>........] - ETA: 5:16 - loss: 0.6944 - regression_loss: 0.4581 - classification_loss: 0.2363
2231/3000 [=====================>........] - ETA: 5:15 - loss: 0.6943 - regression_loss: 0.4580 - classification_loss: 0.2363
2232/3000 [=====================>........] - ETA: 5:15 - loss: 0.6943 - regression_loss: 0.4581 - classification_loss: 0.2362
2233/3000 [=====================>........] - ETA: 5:14 - loss: 0.6942 - regression_loss: 0.4581 - classification_loss: 0.2361
2234/3000 [=====================>........] - ETA: 5:14 - loss: 0.6941 - regression_loss: 0.4580 - classification_loss: 0.2361
2235/3000 [=====================>........] - ETA: 5:14 - loss: 0.6939 - regression_loss: 0.4579 - classification_loss: 0.2360
2236/3000 [=====================>........] - ETA: 5:13 - loss: 0.6940 - regression_loss: 0.4580 - classification_loss: 0.2360
2237/3000 [=====================>........] - ETA: 5:13 - loss: 0.6938 - regression_loss: 0.4579 - classification_loss: 0.2359
2238/3000 [=====================>........] - ETA: 5:12 - loss: 0.6937 - regression_loss: 0.4578 - classification_loss: 0.2359
2239/3000 [=====================>........] - ETA: 5:12 - loss: 0.6936 - regression_loss: 0.4578 - classification_loss: 0.2358
2240/3000 [=====================>........] - ETA: 5:12 - loss: 0.6935 - regression_loss: 0.4577 - classification_loss: 0.2358
2241/3000 [=====================>........] - ETA: 5:11 - loss: 0.6934 - regression_loss: 0.4576 - classification_loss: 0.2358
2242/3000 [=====================>........] - ETA: 5:11 - loss: 0.6933 - regression_loss: 0.4576 - classification_loss: 0.2357
2243/3000 [=====================>........] - ETA: 5:10 - loss: 0.6932 - regression_loss: 0.4575 - classification_loss: 0.2357
2244/3000 [=====================>........] - ETA: 5:10 - loss: 0.6931 - regression_loss: 0.4575 - classification_loss: 0.2356
2245/3000 [=====================>........] - ETA: 5:10 - loss: 0.6929 - regression_loss: 0.4574 - classification_loss: 0.2355
2246/3000 [=====================>........] - ETA: 5:09 - loss: 0.6928 - regression_loss: 0.4574 - classification_loss: 0.2354
2247/3000 [=====================>........] - ETA: 5:09 - loss: 0.6927 - regression_loss: 0.4573 - classification_loss: 0.2354
2248/3000 [=====================>........] - ETA: 5:08 - loss: 0.6927 - regression_loss: 0.4574 - classification_loss: 0.2354
2249/3000 [=====================>........] - ETA: 5:08 - loss: 0.6927 - regression_loss: 0.4574 - classification_loss: 0.2353
2250/3000 [=====================>........] - ETA: 5:08 - loss: 0.6925 - regression_loss: 0.4573 - classification_loss: 0.2352
2251/3000 [=====================>........] - ETA: 5:07 - loss: 0.6925 - regression_loss: 0.4574 - classification_loss: 0.2352
2252/3000 [=====================>........] - ETA: 5:07 - loss: 0.6923 - regression_loss: 0.4573 - classification_loss: 0.2351
2253/3000 [=====================>........] - ETA: 5:06 - loss: 0.6922 - regression_loss: 0.4572 - classification_loss: 0.2350
2254/3000 [=====================>........] - ETA: 5:06 - loss: 0.6921 - regression_loss: 0.4571 - classification_loss: 0.2349
2255/3000 [=====================>........] - ETA: 5:05 - loss: 0.6921 - regression_loss: 0.4572 - classification_loss: 0.2349
2256/3000 [=====================>........] - ETA: 5:05 - loss: 0.6920 - regression_loss: 0.4572 - classification_loss: 0.2348
2257/3000 [=====================>........] - ETA: 5:05 - loss: 0.6920 - regression_loss: 0.4572 - classification_loss: 0.2348
2258/3000 [=====================>........] - ETA: 5:04 - loss: 0.6919 - regression_loss: 0.4572 - classification_loss: 0.2347
2259/3000 [=====================>........] - ETA: 5:04 - loss: 0.6917 - regression_loss: 0.4571 - classification_loss: 0.2346
2260/3000 [=====================>........] - ETA: 5:03 - loss: 0.6916 - regression_loss: 0.4570 - classification_loss: 0.2346
2261/3000 [=====================>........] - ETA: 5:03 - loss: 0.6914 - regression_loss: 0.4570 - classification_loss: 0.2345
2262/3000 [=====================>........] - ETA: 5:03 - loss: 0.6914 - regression_loss: 0.4569 - classification_loss: 0.2344
2263/3000 [=====================>........] - ETA: 5:02 - loss: 0.6913 - regression_loss: 0.4569 - classification_loss: 0.2344
2264/3000 [=====================>........] - ETA: 5:02 - loss: 0.6912 - regression_loss: 0.4568 - classification_loss: 0.2344
2265/3000 [=====================>........] - ETA: 5:01 - loss: 0.6912 - regression_loss: 0.4568 - classification_loss: 0.2344
2266/3000 [=====================>........] - ETA: 5:01 - loss: 0.6910 - regression_loss: 0.4567 - classification_loss: 0.2343
2267/3000 [=====================>........] - ETA: 5:01 - loss: 0.6909 - regression_loss: 0.4567 - classification_loss: 0.2342
2268/3000 [=====================>........] - ETA: 5:00 - loss: 0.6907 - regression_loss: 0.4566 - classification_loss: 0.2342
2269/3000 [=====================>........] - ETA: 5:00 - loss: 0.6906 - regression_loss: 0.4565 - classification_loss: 0.2341
2270/3000 [=====================>........] - ETA: 4:59 - loss: 0.6904 - regression_loss: 0.4563 - classification_loss: 0.2340
2271/3000 [=====================>........] - ETA: 4:59 - loss: 0.6904 - regression_loss: 0.4564 - classification_loss: 0.2340
2272/3000 [=====================>........] - ETA: 4:58 - loss: 0.6903 - regression_loss: 0.4563 - classification_loss: 0.2340
2273/3000 [=====================>........] - ETA: 4:58 - loss: 0.6902 - regression_loss: 0.4562 - classification_loss: 0.2339
2274/3000 [=====================>........] - ETA: 4:58 - loss: 0.6901 - regression_loss: 0.4562 - classification_loss: 0.2339
2275/3000 [=====================>........] - ETA: 4:57 - loss: 0.6900 - regression_loss: 0.4562 - classification_loss: 0.2338
2276/3000 [=====================>........] - ETA: 4:57 - loss: 0.6898 - regression_loss: 0.4561 - classification_loss: 0.2337
2277/3000 [=====================>........] - ETA: 4:56 - loss: 0.6899 - regression_loss: 0.4562 - classification_loss: 0.2337
2278/3000 [=====================>........] - ETA: 4:56 - loss: 0.6900 - regression_loss: 0.4563 - classification_loss: 0.2336
2279/3000 [=====================>........] - ETA: 4:56 - loss: 0.6899 - regression_loss: 0.4563 - classification_loss: 0.2336
2280/3000 [=====================>........] - ETA: 4:55 - loss: 0.6899 - regression_loss: 0.4563 - classification_loss: 0.2336
2281/3000 [=====================>........] - ETA: 4:55 - loss: 0.6898 - regression_loss: 0.4563 - classification_loss: 0.2335
2282/3000 [=====================>........] - ETA: 4:54 - loss: 0.6897 - regression_loss: 0.4562 - classification_loss: 0.2335
2283/3000 [=====================>........] - ETA: 4:54 - loss: 0.6897 - regression_loss: 0.4563 - classification_loss: 0.2334
2284/3000 [=====================>........] - ETA: 4:54 - loss: 0.6896 - regression_loss: 0.4563 - classification_loss: 0.2334
2285/3000 [=====================>........] - ETA: 4:53 - loss: 0.6895 - regression_loss: 0.4562 - classification_loss: 0.2333
2286/3000 [=====================>........] - ETA: 4:53 - loss: 0.6895 - regression_loss: 0.4562 - classification_loss: 0.2333
2287/3000 [=====================>........] - ETA: 4:52 - loss: 0.6895 - regression_loss: 0.4563 - classification_loss: 0.2332
2288/3000 [=====================>........] - ETA: 4:52 - loss: 0.6894 - regression_loss: 0.4562 - classification_loss: 0.2332
2289/3000 [=====================>........] - ETA: 4:52 - loss: 0.6893 - regression_loss: 0.4562 - classification_loss: 0.2331
2290/3000 [=====================>........] - ETA: 4:51 - loss: 0.6892 - regression_loss: 0.4562 - classification_loss: 0.2330
2291/3000 [=====================>........] - ETA: 4:51 - loss: 0.6893 - regression_loss: 0.4563 - classification_loss: 0.2330
2292/3000 [=====================>........] - ETA: 4:50 - loss: 0.6892 - regression_loss: 0.4563 - classification_loss: 0.2329
2293/3000 [=====================>........] - ETA: 4:50 - loss: 0.6894 - regression_loss: 0.4565 - classification_loss: 0.2329
2294/3000 [=====================>........] - ETA: 4:49 - loss: 0.6893 - regression_loss: 0.4565 - classification_loss: 0.2328
2295/3000 [=====================>........] - ETA: 4:49 - loss: 0.6893 - regression_loss: 0.4565 - classification_loss: 0.2329
2296/3000 [=====================>........] - ETA: 4:49 - loss: 0.6893 - regression_loss: 0.4565 - classification_loss: 0.2328
2297/3000 [=====================>........] - ETA: 4:48 - loss: 0.6891 - regression_loss: 0.4564 - classification_loss: 0.2327
2298/3000 [=====================>........] - ETA: 4:48 - loss: 0.6891 - regression_loss: 0.4563 - classification_loss: 0.2327
2299/3000 [=====================>........] - ETA: 4:47 - loss: 0.6890 - regression_loss: 0.4563 - classification_loss: 0.2327
2300/3000 [======================>.......] - ETA: 4:47 - loss: 0.6889 - regression_loss: 0.4562 - classification_loss: 0.2327
2301/3000 [======================>.......] - ETA: 4:47 - loss: 0.6888 - regression_loss: 0.4561 - classification_loss: 0.2327
2302/3000 [======================>.......] - ETA: 4:46 - loss: 0.6889 - regression_loss: 0.4563 - classification_loss: 0.2326
2303/3000 [======================>.......] - ETA: 4:46 - loss: 0.6890 - regression_loss: 0.4565 - classification_loss: 0.2325
2304/3000 [======================>.......] - ETA: 4:45 - loss: 0.6890 - regression_loss: 0.4565 - classification_loss: 0.2325
2305/3000 [======================>.......] - ETA: 4:45 - loss: 0.6889 - regression_loss: 0.4564 - classification_loss: 0.2325
2306/3000 [======================>.......] - ETA: 4:45 - loss: 0.6889 - regression_loss: 0.4564 - classification_loss: 0.2325
2307/3000 [======================>.......] - ETA: 4:44 - loss: 0.6888 - regression_loss: 0.4563 - classification_loss: 0.2324
2308/3000 [======================>.......] - ETA: 4:44 - loss: 0.6887 - regression_loss: 0.4563 - classification_loss: 0.2324
2309/3000 [======================>.......] - ETA: 4:43 - loss: 0.6886 - regression_loss: 0.4562 - classification_loss: 0.2324
2310/3000 [======================>.......] - ETA: 4:43 - loss: 0.6885 - regression_loss: 0.4562 - classification_loss: 0.2324
2311/3000 [======================>.......] - ETA: 4:43 - loss: 0.6884 - regression_loss: 0.4561 - classification_loss: 0.2323
2312/3000 [======================>.......] - ETA: 4:42 - loss: 0.6883 - regression_loss: 0.4560 - classification_loss: 0.2322
2313/3000 [======================>.......] - ETA: 4:42 - loss: 0.6883 - regression_loss: 0.4561 - classification_loss: 0.2322
2314/3000 [======================>.......] - ETA: 4:41 - loss: 0.6882 - regression_loss: 0.4560 - classification_loss: 0.2322
2315/3000 [======================>.......] - ETA: 4:41 - loss: 0.6882 - regression_loss: 0.4560 - classification_loss: 0.2322
2316/3000 [======================>.......] - ETA: 4:40 - loss: 0.6882 - regression_loss: 0.4560 - classification_loss: 0.2322
2317/3000 [======================>.......] - ETA: 4:40 - loss: 0.6881 - regression_loss: 0.4560 - classification_loss: 0.2321
2318/3000 [======================>.......] - ETA: 4:40 - loss: 0.6880 - regression_loss: 0.4560 - classification_loss: 0.2320
2319/3000 [======================>.......] - ETA: 4:39 - loss: 0.6880 - regression_loss: 0.4560 - classification_loss: 0.2320
2320/3000 [======================>.......] - ETA: 4:39 - loss: 0.6879 - regression_loss: 0.4559 - classification_loss: 0.2320
2321/3000 [======================>.......] - ETA: 4:38 - loss: 0.6879 - regression_loss: 0.4560 - classification_loss: 0.2320
2322/3000 [======================>.......] - ETA: 4:38 - loss: 0.6879 - regression_loss: 0.4559 - classification_loss: 0.2319
2323/3000 [======================>.......] - ETA: 4:38 - loss: 0.6878 - regression_loss: 0.4559 - classification_loss: 0.2319
2324/3000 [======================>.......] - ETA: 4:37 - loss: 0.6877 - regression_loss: 0.4558 - classification_loss: 0.2318
2325/3000 [======================>.......] - ETA: 4:37 - loss: 0.6877 - regression_loss: 0.4559 - classification_loss: 0.2318
2326/3000 [======================>.......] - ETA: 4:36 - loss: 0.6876 - regression_loss: 0.4558 - classification_loss: 0.2318
2327/3000 [======================>.......] - ETA: 4:36 - loss: 0.6875 - regression_loss: 0.4558 - classification_loss: 0.2317
2328/3000 [======================>.......] - ETA: 4:36 - loss: 0.6873 - regression_loss: 0.4557 - classification_loss: 0.2316
2329/3000 [======================>.......] - ETA: 4:35 - loss: 0.6873 - regression_loss: 0.4557 - classification_loss: 0.2316
2330/3000 [======================>.......] - ETA: 4:35 - loss: 0.6872 - regression_loss: 0.4557 - classification_loss: 0.2315
2331/3000 [======================>.......] - ETA: 4:34 - loss: 0.6871 - regression_loss: 0.4557 - classification_loss: 0.2314
2332/3000 [======================>.......] - ETA: 4:34 - loss: 0.6871 - regression_loss: 0.4557 - classification_loss: 0.2314
2333/3000 [======================>.......] - ETA: 4:33 - loss: 0.6871 - regression_loss: 0.4557 - classification_loss: 0.2314
2334/3000 [======================>.......] - ETA: 4:33 - loss: 0.6870 - regression_loss: 0.4556 - classification_loss: 0.2314
2335/3000 [======================>.......] - ETA: 4:33 - loss: 0.6872 - regression_loss: 0.4558 - classification_loss: 0.2313
2336/3000 [======================>.......] - ETA: 4:32 - loss: 0.6871 - regression_loss: 0.4558 - classification_loss: 0.2313
2337/3000 [======================>.......] - ETA: 4:32 - loss: 0.6870 - regression_loss: 0.4558 - classification_loss: 0.2312
2338/3000 [======================>.......] - ETA: 4:31 - loss: 0.6870 - regression_loss: 0.4558 - classification_loss: 0.2312
2339/3000 [======================>.......] - ETA: 4:31 - loss: 0.6870 - regression_loss: 0.4559 - classification_loss: 0.2311
2340/3000 [======================>.......] - ETA: 4:31 - loss: 0.6869 - regression_loss: 0.4558 - classification_loss: 0.2311
2341/3000 [======================>.......] - ETA: 4:30 - loss: 0.6869 - regression_loss: 0.4558 - classification_loss: 0.2311
2342/3000 [======================>.......] - ETA: 4:30 - loss: 0.6868 - regression_loss: 0.4558 - classification_loss: 0.2310
2343/3000 [======================>.......] - ETA: 4:29 - loss: 0.6867 - regression_loss: 0.4558 - classification_loss: 0.2309
2344/3000 [======================>.......] - ETA: 4:29 - loss: 0.6866 - regression_loss: 0.4557 - classification_loss: 0.2309
2345/3000 [======================>.......] - ETA: 4:29 - loss: 0.6867 - regression_loss: 0.4558 - classification_loss: 0.2309
2346/3000 [======================>.......] - ETA: 4:28 - loss: 0.6867 - regression_loss: 0.4559 - classification_loss: 0.2309
2347/3000 [======================>.......] - ETA: 4:28 - loss: 0.6868 - regression_loss: 0.4559 - classification_loss: 0.2309
2348/3000 [======================>.......] - ETA: 4:27 - loss: 0.6867 - regression_loss: 0.4559 - classification_loss: 0.2308
2349/3000 [======================>.......] - ETA: 4:27 - loss: 0.6868 - regression_loss: 0.4560 - classification_loss: 0.2308
2350/3000 [======================>.......] - ETA: 4:27 - loss: 0.6866 - regression_loss: 0.4559 - classification_loss: 0.2307
2351/3000 [======================>.......] - ETA: 4:26 - loss: 0.6866 - regression_loss: 0.4560 - classification_loss: 0.2307
2352/3000 [======================>.......] - ETA: 4:26 - loss: 0.6865 - regression_loss: 0.4559 - classification_loss: 0.2306
2353/3000 [======================>.......] - ETA: 4:25 - loss: 0.6864 - regression_loss: 0.4559 - classification_loss: 0.2305
2354/3000 [======================>.......] - ETA: 4:25 - loss: 0.6863 - regression_loss: 0.4558 - classification_loss: 0.2305
2355/3000 [======================>.......] - ETA: 4:24 - loss: 0.6861 - regression_loss: 0.4557 - classification_loss: 0.2304
2356/3000 [======================>.......] - ETA: 4:24 - loss: 0.6860 - regression_loss: 0.4557 - classification_loss: 0.2303
2357/3000 [======================>.......] - ETA: 4:24 - loss: 0.6859 - regression_loss: 0.4556 - classification_loss: 0.2303
2358/3000 [======================>.......] - ETA: 4:23 - loss: 0.6858 - regression_loss: 0.4555 - classification_loss: 0.2303
2359/3000 [======================>.......] - ETA: 4:23 - loss: 0.6857 - regression_loss: 0.4554 - classification_loss: 0.2303
2360/3000 [======================>.......] - ETA: 4:22 - loss: 0.6856 - regression_loss: 0.4553 - classification_loss: 0.2303
2361/3000 [======================>.......] - ETA: 4:22 - loss: 0.6855 - regression_loss: 0.4553 - classification_loss: 0.2302
2362/3000 [======================>.......] - ETA: 4:22 - loss: 0.6853 - regression_loss: 0.4552 - classification_loss: 0.2302
2363/3000 [======================>.......] - ETA: 4:21 - loss: 0.6852 - regression_loss: 0.4551 - classification_loss: 0.2301
2364/3000 [======================>.......] - ETA: 4:21 - loss: 0.6854 - regression_loss: 0.4552 - classification_loss: 0.2301
2365/3000 [======================>.......] - ETA: 4:20 - loss: 0.6852 - regression_loss: 0.4551 - classification_loss: 0.2301
2366/3000 [======================>.......] - ETA: 4:20 - loss: 0.6852 - regression_loss: 0.4551 - classification_loss: 0.2301
2367/3000 [======================>.......] - ETA: 4:20 - loss: 0.6852 - regression_loss: 0.4552 - classification_loss: 0.2300
2368/3000 [======================>.......] - ETA: 4:19 - loss: 0.6851 - regression_loss: 0.4552 - classification_loss: 0.2300
2369/3000 [======================>.......] - ETA: 4:19 - loss: 0.6851 - regression_loss: 0.4552 - classification_loss: 0.2299
2370/3000 [======================>.......] - ETA: 4:18 - loss: 0.6850 - regression_loss: 0.4551 - classification_loss: 0.2299
2371/3000 [======================>.......] - ETA: 4:18 - loss: 0.6849 - regression_loss: 0.4551 - classification_loss: 0.2298
2372/3000 [======================>.......] - ETA: 4:17 - loss: 0.6848 - regression_loss: 0.4551 - classification_loss: 0.2297
2373/3000 [======================>.......] - ETA: 4:17 - loss: 0.6849 - regression_loss: 0.4552 - classification_loss: 0.2297
2374/3000 [======================>.......] - ETA: 4:17 - loss: 0.6848 - regression_loss: 0.4551 - classification_loss: 0.2297
2375/3000 [======================>.......] - ETA: 4:16 - loss: 0.6849 - regression_loss: 0.4553 - classification_loss: 0.2297
2376/3000 [======================>.......] - ETA: 4:16 - loss: 0.6849 - regression_loss: 0.4552 - classification_loss: 0.2297
2377/3000 [======================>.......] - ETA: 4:15 - loss: 0.6849 - regression_loss: 0.4553 - classification_loss: 0.2296
2378/3000 [======================>.......] - ETA: 4:15 - loss: 0.6848 - regression_loss: 0.4552 - classification_loss: 0.2296
2379/3000 [======================>.......] - ETA: 4:15 - loss: 0.6848 - regression_loss: 0.4553 - classification_loss: 0.2296
2380/3000 [======================>.......] - ETA: 4:14 - loss: 0.6847 - regression_loss: 0.4552 - classification_loss: 0.2295
2381/3000 [======================>.......] - ETA: 4:14 - loss: 0.6847 - regression_loss: 0.4553 - classification_loss: 0.2295
2382/3000 [======================>.......] - ETA: 4:13 - loss: 0.6847 - regression_loss: 0.4552 - classification_loss: 0.2294
2383/3000 [======================>.......] - ETA: 4:13 - loss: 0.6846 - regression_loss: 0.4552 - classification_loss: 0.2294
2384/3000 [======================>.......] - ETA: 4:13 - loss: 0.6845 - regression_loss: 0.4552 - classification_loss: 0.2293
2385/3000 [======================>.......] - ETA: 4:12 - loss: 0.6844 - regression_loss: 0.4551 - classification_loss: 0.2292
2386/3000 [======================>.......] - ETA: 4:12 - loss: 0.6844 - regression_loss: 0.4551 - classification_loss: 0.2293
2387/3000 [======================>.......] - ETA: 4:11 - loss: 0.6842 - regression_loss: 0.4550 - classification_loss: 0.2292
2388/3000 [======================>.......] - ETA: 4:11 - loss: 0.6841 - regression_loss: 0.4549 - classification_loss: 0.2292
2389/3000 [======================>.......] - ETA: 4:10 - loss: 0.6840 - regression_loss: 0.4548 - classification_loss: 0.2291
2390/3000 [======================>.......] - ETA: 4:10 - loss: 0.6839 - regression_loss: 0.4548 - classification_loss: 0.2291
2391/3000 [======================>.......] - ETA: 4:10 - loss: 0.6838 - regression_loss: 0.4547 - classification_loss: 0.2291
2392/3000 [======================>.......] - ETA: 4:09 - loss: 0.6836 - regression_loss: 0.4546 - classification_loss: 0.2290
2393/3000 [======================>.......] - ETA: 4:09 - loss: 0.6836 - regression_loss: 0.4546 - classification_loss: 0.2290
2394/3000 [======================>.......] - ETA: 4:08 - loss: 0.6836 - regression_loss: 0.4547 - classification_loss: 0.2290
2395/3000 [======================>.......] - ETA: 4:08 - loss: 0.6835 - regression_loss: 0.4546 - classification_loss: 0.2289
2396/3000 [======================>.......] - ETA: 4:08 - loss: 0.6834 - regression_loss: 0.4545 - classification_loss: 0.2289
2397/3000 [======================>.......] - ETA: 4:07 - loss: 0.6833 - regression_loss: 0.4544 - classification_loss: 0.2288
2398/3000 [======================>.......] - ETA: 4:07 - loss: 0.6832 - regression_loss: 0.4544 - classification_loss: 0.2288
2399/3000 [======================>.......] - ETA: 4:06 - loss: 0.6831 - regression_loss: 0.4543 - classification_loss: 0.2288
2400/3000 [=======================>......] - ETA: 4:06 - loss: 0.6831 - regression_loss: 0.4544 - classification_loss: 0.2288
2401/3000 [=======================>......] - ETA: 4:06 - loss: 0.6830 - regression_loss: 0.4543 - classification_loss: 0.2287
2402/3000 [=======================>......] - ETA: 4:05 - loss: 0.6830 - regression_loss: 0.4543 - classification_loss: 0.2286
2403/3000 [=======================>......] - ETA: 4:05 - loss: 0.6829 - regression_loss: 0.4544 - classification_loss: 0.2286
2404/3000 [=======================>......] - ETA: 4:04 - loss: 0.6829 - regression_loss: 0.4543 - classification_loss: 0.2286
2405/3000 [=======================>......] - ETA: 4:04 - loss: 0.6830 - regression_loss: 0.4544 - classification_loss: 0.2286
2406/3000 [=======================>......] - ETA: 4:03 - loss: 0.6829 - regression_loss: 0.4544 - classification_loss: 0.2285
2407/3000 [=======================>......] - ETA: 4:03 - loss: 0.6828 - regression_loss: 0.4543 - classification_loss: 0.2285
2408/3000 [=======================>......] - ETA: 4:03 - loss: 0.6827 - regression_loss: 0.4543 - classification_loss: 0.2284
2409/3000 [=======================>......] - ETA: 4:02 - loss: 0.6829 - regression_loss: 0.4546 - classification_loss: 0.2283
2410/3000 [=======================>......] - ETA: 4:02 - loss: 0.6828 - regression_loss: 0.4545 - classification_loss: 0.2283
2411/3000 [=======================>......] - ETA: 4:01 - loss: 0.6828 - regression_loss: 0.4545 - classification_loss: 0.2283
2412/3000 [=======================>......] - ETA: 4:01 - loss: 0.6828 - regression_loss: 0.4545 - classification_loss: 0.2283
2413/3000 [=======================>......] - ETA: 4:01 - loss: 0.6827 - regression_loss: 0.4544 - classification_loss: 0.2283
2414/3000 [=======================>......] - ETA: 4:00 - loss: 0.6827 - regression_loss: 0.4545 - classification_loss: 0.2282
2415/3000 [=======================>......] - ETA: 4:00 - loss: 0.6827 - regression_loss: 0.4545 - classification_loss: 0.2282
2416/3000 [=======================>......] - ETA: 3:59 - loss: 0.6826 - regression_loss: 0.4545 - classification_loss: 0.2281
2417/3000 [=======================>......] - ETA: 3:59 - loss: 0.6825 - regression_loss: 0.4544 - classification_loss: 0.2281
2418/3000 [=======================>......] - ETA: 3:59 - loss: 0.6825 - regression_loss: 0.4545 - classification_loss: 0.2281
2419/3000 [=======================>......] - ETA: 3:58 - loss: 0.6825 - regression_loss: 0.4544 - classification_loss: 0.2281
2420/3000 [=======================>......] - ETA: 3:58 - loss: 0.6823 - regression_loss: 0.4543 - classification_loss: 0.2280
2421/3000 [=======================>......] - ETA: 3:57 - loss: 0.6822 - regression_loss: 0.4542 - classification_loss: 0.2280
2422/3000 [=======================>......] - ETA: 3:57 - loss: 0.6821 - regression_loss: 0.4542 - classification_loss: 0.2279
2423/3000 [=======================>......] - ETA: 3:56 - loss: 0.6821 - regression_loss: 0.4542 - classification_loss: 0.2279
2424/3000 [=======================>......] - ETA: 3:56 - loss: 0.6821 - regression_loss: 0.4542 - classification_loss: 0.2278
2425/3000 [=======================>......] - ETA: 3:56 - loss: 0.6820 - regression_loss: 0.4541 - classification_loss: 0.2279
2426/3000 [=======================>......] - ETA: 3:55 - loss: 0.6821 - regression_loss: 0.4542 - classification_loss: 0.2279
2427/3000 [=======================>......] - ETA: 3:55 - loss: 0.6820 - regression_loss: 0.4542 - classification_loss: 0.2278
2428/3000 [=======================>......] - ETA: 3:54 - loss: 0.6819 - regression_loss: 0.4541 - classification_loss: 0.2279
2429/3000 [=======================>......] - ETA: 3:54 - loss: 0.6818 - regression_loss: 0.4540 - classification_loss: 0.2278
2430/3000 [=======================>......] - ETA: 3:54 - loss: 0.6817 - regression_loss: 0.4540 - classification_loss: 0.2277
2431/3000 [=======================>......] - ETA: 3:53 - loss: 0.6815 - regression_loss: 0.4538 - classification_loss: 0.2277
2432/3000 [=======================>......] - ETA: 3:53 - loss: 0.6815 - regression_loss: 0.4538 - classification_loss: 0.2277
2433/3000 [=======================>......] - ETA: 3:52 - loss: 0.6813 - regression_loss: 0.4537 - classification_loss: 0.2276
2434/3000 [=======================>......] - ETA: 3:52 - loss: 0.6812 - regression_loss: 0.4537 - classification_loss: 0.2276
2435/3000 [=======================>......] - ETA: 3:52 - loss: 0.6812 - regression_loss: 0.4536 - classification_loss: 0.2275
2436/3000 [=======================>......] - ETA: 3:51 - loss: 0.6811 - regression_loss: 0.4536 - classification_loss: 0.2275
2437/3000 [=======================>......] - ETA: 3:51 - loss: 0.6809 - regression_loss: 0.4535 - classification_loss: 0.2274
2438/3000 [=======================>......] - ETA: 3:50 - loss: 0.6808 - regression_loss: 0.4534 - classification_loss: 0.2274
2439/3000 [=======================>......] - ETA: 3:50 - loss: 0.6808 - regression_loss: 0.4534 - classification_loss: 0.2274
2440/3000 [=======================>......] - ETA: 3:50 - loss: 0.6807 - regression_loss: 0.4534 - classification_loss: 0.2273
2441/3000 [=======================>......] - ETA: 3:49 - loss: 0.6806 - regression_loss: 0.4533 - classification_loss: 0.2272
2442/3000 [=======================>......] - ETA: 3:49 - loss: 0.6805 - regression_loss: 0.4533 - classification_loss: 0.2272
2443/3000 [=======================>......] - ETA: 3:48 - loss: 0.6805 - regression_loss: 0.4532 - classification_loss: 0.2272
2444/3000 [=======================>......] - ETA: 3:48 - loss: 0.6804 - regression_loss: 0.4531 - classification_loss: 0.2272
2445/3000 [=======================>......] - ETA: 3:47 - loss: 0.6804 - regression_loss: 0.4532 - classification_loss: 0.2272
2446/3000 [=======================>......] - ETA: 3:47 - loss: 0.6803 - regression_loss: 0.4531 - classification_loss: 0.2271
2447/3000 [=======================>......] - ETA: 3:47 - loss: 0.6802 - regression_loss: 0.4531 - classification_loss: 0.2271
2448/3000 [=======================>......] - ETA: 3:46 - loss: 0.6801 - regression_loss: 0.4530 - classification_loss: 0.2270
2449/3000 [=======================>......] - ETA: 3:46 - loss: 0.6800 - regression_loss: 0.4530 - classification_loss: 0.2270
2450/3000 [=======================>......] - ETA: 3:45 - loss: 0.6800 - regression_loss: 0.4530 - classification_loss: 0.2270
2451/3000 [=======================>......] - ETA: 3:45 - loss: 0.6798 - regression_loss: 0.4529 - classification_loss: 0.2269
2452/3000 [=======================>......] - ETA: 3:45 - loss: 0.6797 - regression_loss: 0.4528 - classification_loss: 0.2269
2453/3000 [=======================>......] - ETA: 3:44 - loss: 0.6796 - regression_loss: 0.4528 - classification_loss: 0.2268
2454/3000 [=======================>......] - ETA: 3:44 - loss: 0.6795 - regression_loss: 0.4527 - classification_loss: 0.2268
2455/3000 [=======================>......] - ETA: 3:43 - loss: 0.6794 - regression_loss: 0.4527 - classification_loss: 0.2267
2456/3000 [=======================>......] - ETA: 3:43 - loss: 0.6795 - regression_loss: 0.4526 - classification_loss: 0.2268
2457/3000 [=======================>......] - ETA: 3:43 - loss: 0.6794 - regression_loss: 0.4526 - classification_loss: 0.2268
2458/3000 [=======================>......] - ETA: 3:42 - loss: 0.6793 - regression_loss: 0.4526 - classification_loss: 0.2267
2459/3000 [=======================>......] - ETA: 3:42 - loss: 0.6792 - regression_loss: 0.4525 - classification_loss: 0.2267
2460/3000 [=======================>......] - ETA: 3:41 - loss: 0.6790 - regression_loss: 0.4524 - classification_loss: 0.2267
2461/3000 [=======================>......] - ETA: 3:41 - loss: 0.6790 - regression_loss: 0.4524 - classification_loss: 0.2266
2462/3000 [=======================>......] - ETA: 3:41 - loss: 0.6789 - regression_loss: 0.4523 - classification_loss: 0.2266
2463/3000 [=======================>......] - ETA: 3:40 - loss: 0.6788 - regression_loss: 0.4522 - classification_loss: 0.2266
2464/3000 [=======================>......] - ETA: 3:40 - loss: 0.6787 - regression_loss: 0.4522 - classification_loss: 0.2265
2465/3000 [=======================>......] - ETA: 3:39 - loss: 0.6785 - regression_loss: 0.4520 - classification_loss: 0.2265
2466/3000 [=======================>......] - ETA: 3:39 - loss: 0.6785 - regression_loss: 0.4521 - classification_loss: 0.2264
2467/3000 [=======================>......] - ETA: 3:38 - loss: 0.6784 - regression_loss: 0.4520 - classification_loss: 0.2264
2468/3000 [=======================>......] - ETA: 3:38 - loss: 0.6784 - regression_loss: 0.4521 - classification_loss: 0.2263
2469/3000 [=======================>......] - ETA: 3:38 - loss: 0.6784 - regression_loss: 0.4521 - classification_loss: 0.2263
2470/3000 [=======================>......] - ETA: 3:37 - loss: 0.6784 - regression_loss: 0.4521 - classification_loss: 0.2263
2471/3000 [=======================>......] - ETA: 3:37 - loss: 0.6783 - regression_loss: 0.4521 - classification_loss: 0.2262
2472/3000 [=======================>......] - ETA: 3:36 - loss: 0.6783 - regression_loss: 0.4521 - classification_loss: 0.2262
2473/3000 [=======================>......] - ETA: 3:36 - loss: 0.6781 - regression_loss: 0.4520 - classification_loss: 0.2261
2474/3000 [=======================>......] - ETA: 3:36 - loss: 0.6781 - regression_loss: 0.4520 - classification_loss: 0.2261
2475/3000 [=======================>......] - ETA: 3:35 - loss: 0.6779 - regression_loss: 0.4519 - classification_loss: 0.2260
2476/3000 [=======================>......] - ETA: 3:35 - loss: 0.6779 - regression_loss: 0.4520 - classification_loss: 0.2260
2477/3000 [=======================>......] - ETA: 3:34 - loss: 0.6778 - regression_loss: 0.4519 - classification_loss: 0.2259
2478/3000 [=======================>......] - ETA: 3:34 - loss: 0.6777 - regression_loss: 0.4518 - classification_loss: 0.2259
2479/3000 [=======================>......] - ETA: 3:34 - loss: 0.6777 - regression_loss: 0.4519 - classification_loss: 0.2258
2480/3000 [=======================>......] - ETA: 3:33 - loss: 0.6776 - regression_loss: 0.4518 - classification_loss: 0.2258
2481/3000 [=======================>......] - ETA: 3:33 - loss: 0.6775 - regression_loss: 0.4518 - classification_loss: 0.2257
2482/3000 [=======================>......] - ETA: 3:32 - loss: 0.6774 - regression_loss: 0.4518 - classification_loss: 0.2257
2483/3000 [=======================>......] - ETA: 3:32 - loss: 0.6775 - regression_loss: 0.4519 - classification_loss: 0.2256
2484/3000 [=======================>......] - ETA: 3:31 - loss: 0.6774 - regression_loss: 0.4518 - classification_loss: 0.2256
2485/3000 [=======================>......] - ETA: 3:31 - loss: 0.6774 - regression_loss: 0.4518 - classification_loss: 0.2256
2486/3000 [=======================>......] - ETA: 3:31 - loss: 0.6774 - regression_loss: 0.4519 - classification_loss: 0.2255
2487/3000 [=======================>......] - ETA: 3:30 - loss: 0.6772 - regression_loss: 0.4518 - classification_loss: 0.2255
2488/3000 [=======================>......] - ETA: 3:30 - loss: 0.6771 - regression_loss: 0.4517 - classification_loss: 0.2254
2489/3000 [=======================>......] - ETA: 3:29 - loss: 0.6771 - regression_loss: 0.4517 - classification_loss: 0.2254
2490/3000 [=======================>......] - ETA: 3:29 - loss: 0.6771 - regression_loss: 0.4518 - classification_loss: 0.2253
2491/3000 [=======================>......] - ETA: 3:29 - loss: 0.6771 - regression_loss: 0.4519 - classification_loss: 0.2253
2492/3000 [=======================>......] - ETA: 3:28 - loss: 0.6771 - regression_loss: 0.4518 - classification_loss: 0.2252
2493/3000 [=======================>......] - ETA: 3:28 - loss: 0.6770 - regression_loss: 0.4518 - classification_loss: 0.2252
2494/3000 [=======================>......] - ETA: 3:27 - loss: 0.6768 - regression_loss: 0.4517 - classification_loss: 0.2252
2495/3000 [=======================>......] - ETA: 3:27 - loss: 0.6768 - regression_loss: 0.4517 - classification_loss: 0.2251
2496/3000 [=======================>......] - ETA: 3:27 - loss: 0.6767 - regression_loss: 0.4516 - classification_loss: 0.2251
2497/3000 [=======================>......] - ETA: 3:26 - loss: 0.6767 - regression_loss: 0.4516 - classification_loss: 0.2250
2498/3000 [=======================>......] - ETA: 3:26 - loss: 0.6767 - regression_loss: 0.4517 - classification_loss: 0.2250
2499/3000 [=======================>......] - ETA: 3:25 - loss: 0.6766 - regression_loss: 0.4516 - classification_loss: 0.2249
2500/3000 [========================>.....] - ETA: 3:25 - loss: 0.6764 - regression_loss: 0.4516 - classification_loss: 0.2249
2501/3000 [========================>.....] - ETA: 3:24 - loss: 0.6764 - regression_loss: 0.4515 - classification_loss: 0.2248
2502/3000 [========================>.....] - ETA: 3:24 - loss: 0.6763 - regression_loss: 0.4515 - classification_loss: 0.2248
2503/3000 [========================>.....] - ETA: 3:24 - loss: 0.6763 - regression_loss: 0.4516 - classification_loss: 0.2247
2504/3000 [========================>.....] - ETA: 3:23 - loss: 0.6763 - regression_loss: 0.4516 - classification_loss: 0.2247
2505/3000 [========================>.....] - ETA: 3:23 - loss: 0.6763 - regression_loss: 0.4516 - classification_loss: 0.2247
2506/3000 [========================>.....] - ETA: 3:22 - loss: 0.6762 - regression_loss: 0.4516 - classification_loss: 0.2246
2507/3000 [========================>.....] - ETA: 3:22 - loss: 0.6761 - regression_loss: 0.4515 - classification_loss: 0.2246
2508/3000 [========================>.....] - ETA: 3:22 - loss: 0.6761 - regression_loss: 0.4515 - classification_loss: 0.2246
2509/3000 [========================>.....] - ETA: 3:21 - loss: 0.6760 - regression_loss: 0.4515 - classification_loss: 0.2245
2510/3000 [========================>.....] - ETA: 3:21 - loss: 0.6759 - regression_loss: 0.4514 - classification_loss: 0.2244
2511/3000 [========================>.....] - ETA: 3:20 - loss: 0.6758 - regression_loss: 0.4514 - classification_loss: 0.2244
2512/3000 [========================>.....] - ETA: 3:20 - loss: 0.6757 - regression_loss: 0.4513 - classification_loss: 0.2243
2513/3000 [========================>.....] - ETA: 3:20 - loss: 0.6755 - regression_loss: 0.4513 - classification_loss: 0.2243
2514/3000 [========================>.....] - ETA: 3:19 - loss: 0.6755 - regression_loss: 0.4513 - classification_loss: 0.2242
2515/3000 [========================>.....] - ETA: 3:19 - loss: 0.6755 - regression_loss: 0.4512 - classification_loss: 0.2242
2516/3000 [========================>.....] - ETA: 3:18 - loss: 0.6753 - regression_loss: 0.4512 - classification_loss: 0.2242
2517/3000 [========================>.....] - ETA: 3:18 - loss: 0.6753 - regression_loss: 0.4512 - classification_loss: 0.2241
2518/3000 [========================>.....] - ETA: 3:17 - loss: 0.6752 - regression_loss: 0.4512 - classification_loss: 0.2241
2519/3000 [========================>.....] - ETA: 3:17 - loss: 0.6752 - regression_loss: 0.4512 - classification_loss: 0.2240
2520/3000 [========================>.....] - ETA: 3:17 - loss: 0.6750 - regression_loss: 0.4511 - classification_loss: 0.2239
2521/3000 [========================>.....] - ETA: 3:16 - loss: 0.6749 - regression_loss: 0.4510 - classification_loss: 0.2239
2522/3000 [========================>.....] - ETA: 3:16 - loss: 0.6748 - regression_loss: 0.4510 - classification_loss: 0.2238
2523/3000 [========================>.....] - ETA: 3:15 - loss: 0.6748 - regression_loss: 0.4510 - classification_loss: 0.2238
2524/3000 [========================>.....] - ETA: 3:15 - loss: 0.6747 - regression_loss: 0.4510 - classification_loss: 0.2237
2525/3000 [========================>.....] - ETA: 3:15 - loss: 0.6745 - regression_loss: 0.4509 - classification_loss: 0.2237
2526/3000 [========================>.....] - ETA: 3:14 - loss: 0.6745 - regression_loss: 0.4509 - classification_loss: 0.2236
2527/3000 [========================>.....] - ETA: 3:14 - loss: 0.6744 - regression_loss: 0.4509 - classification_loss: 0.2236
2528/3000 [========================>.....] - ETA: 3:13 - loss: 0.6744 - regression_loss: 0.4508 - classification_loss: 0.2236
2529/3000 [========================>.....] - ETA: 3:13 - loss: 0.6743 - regression_loss: 0.4508 - classification_loss: 0.2235
2530/3000 [========================>.....] - ETA: 3:13 - loss: 0.6742 - regression_loss: 0.4507 - classification_loss: 0.2235
2531/3000 [========================>.....] - ETA: 3:12 - loss: 0.6740 - regression_loss: 0.4506 - classification_loss: 0.2234
2532/3000 [========================>.....] - ETA: 3:12 - loss: 0.6740 - regression_loss: 0.4506 - classification_loss: 0.2234
2533/3000 [========================>.....] - ETA: 3:11 - loss: 0.6739 - regression_loss: 0.4505 - classification_loss: 0.2234
2534/3000 [========================>.....] - ETA: 3:11 - loss: 0.6738 - regression_loss: 0.4505 - classification_loss: 0.2233
2535/3000 [========================>.....] - ETA: 3:11 - loss: 0.6738 - regression_loss: 0.4505 - classification_loss: 0.2233
2536/3000 [========================>.....] - ETA: 3:10 - loss: 0.6737 - regression_loss: 0.4504 - classification_loss: 0.2232
2537/3000 [========================>.....] - ETA: 3:10 - loss: 0.6735 - regression_loss: 0.4503 - classification_loss: 0.2231
2538/3000 [========================>.....] - ETA: 3:09 - loss: 0.6734 - regression_loss: 0.4503 - classification_loss: 0.2231
2539/3000 [========================>.....] - ETA: 3:09 - loss: 0.6734 - regression_loss: 0.4503 - classification_loss: 0.2231
2540/3000 [========================>.....] - ETA: 3:08 - loss: 0.6732 - regression_loss: 0.4503 - classification_loss: 0.2230
2541/3000 [========================>.....] - ETA: 3:08 - loss: 0.6733 - regression_loss: 0.4503 - classification_loss: 0.2230
2542/3000 [========================>.....] - ETA: 3:08 - loss: 0.6732 - regression_loss: 0.4503 - classification_loss: 0.2230
2543/3000 [========================>.....] - ETA: 3:07 - loss: 0.6732 - regression_loss: 0.4503 - classification_loss: 0.2229
2544/3000 [========================>.....] - ETA: 3:07 - loss: 0.6731 - regression_loss: 0.4503 - classification_loss: 0.2229
2545/3000 [========================>.....] - ETA: 3:06 - loss: 0.6730 - regression_loss: 0.4502 - classification_loss: 0.2228
2546/3000 [========================>.....] - ETA: 3:06 - loss: 0.6729 - regression_loss: 0.4502 - classification_loss: 0.2227
2547/3000 [========================>.....] - ETA: 3:06 - loss: 0.6728 - regression_loss: 0.4501 - classification_loss: 0.2227
2548/3000 [========================>.....] - ETA: 3:05 - loss: 0.6728 - regression_loss: 0.4501 - classification_loss: 0.2227
2549/3000 [========================>.....] - ETA: 3:05 - loss: 0.6728 - regression_loss: 0.4501 - classification_loss: 0.2226
2550/3000 [========================>.....] - ETA: 3:04 - loss: 0.6727 - regression_loss: 0.4501 - classification_loss: 0.2226
2551/3000 [========================>.....] - ETA: 3:04 - loss: 0.6725 - regression_loss: 0.4500 - classification_loss: 0.2225
2552/3000 [========================>.....] - ETA: 3:04 - loss: 0.6725 - regression_loss: 0.4500 - classification_loss: 0.2225
2553/3000 [========================>.....] - ETA: 3:03 - loss: 0.6723 - regression_loss: 0.4499 - classification_loss: 0.2224
2554/3000 [========================>.....] - ETA: 3:03 - loss: 0.6722 - regression_loss: 0.4498 - classification_loss: 0.2223
2555/3000 [========================>.....] - ETA: 3:02 - loss: 0.6722 - regression_loss: 0.4499 - classification_loss: 0.2223
2556/3000 [========================>.....] - ETA: 3:02 - loss: 0.6721 - regression_loss: 0.4498 - classification_loss: 0.2223
2557/3000 [========================>.....] - ETA: 3:01 - loss: 0.6719 - regression_loss: 0.4497 - classification_loss: 0.2222
2558/3000 [========================>.....] - ETA: 3:01 - loss: 0.6718 - regression_loss: 0.4496 - classification_loss: 0.2221
2559/3000 [========================>.....] - ETA: 3:01 - loss: 0.6716 - regression_loss: 0.4495 - classification_loss: 0.2221
2560/3000 [========================>.....] - ETA: 3:00 - loss: 0.6715 - regression_loss: 0.4495 - classification_loss: 0.2221
2561/3000 [========================>.....] - ETA: 3:00 - loss: 0.6715 - regression_loss: 0.4495 - classification_loss: 0.2220
2562/3000 [========================>.....] - ETA: 2:59 - loss: 0.6716 - regression_loss: 0.4496 - classification_loss: 0.2220
2563/3000 [========================>.....] - ETA: 2:59 - loss: 0.6715 - regression_loss: 0.4496 - classification_loss: 0.2219
2564/3000 [========================>.....] - ETA: 2:59 - loss: 0.6713 - regression_loss: 0.4495 - classification_loss: 0.2218
2565/3000 [========================>.....] - ETA: 2:58 - loss: 0.6713 - regression_loss: 0.4494 - classification_loss: 0.2218
2566/3000 [========================>.....] - ETA: 2:58 - loss: 0.6712 - regression_loss: 0.4494 - classification_loss: 0.2218
2567/3000 [========================>.....] - ETA: 2:57 - loss: 0.6712 - regression_loss: 0.4495 - classification_loss: 0.2217
2568/3000 [========================>.....] - ETA: 2:57 - loss: 0.6711 - regression_loss: 0.4494 - classification_loss: 0.2217
2569/3000 [========================>.....] - ETA: 2:57 - loss: 0.6711 - regression_loss: 0.4494 - classification_loss: 0.2217
2570/3000 [========================>.....] - ETA: 2:56 - loss: 0.6710 - regression_loss: 0.4493 - classification_loss: 0.2217
2571/3000 [========================>.....] - ETA: 2:56 - loss: 0.6709 - regression_loss: 0.4492 - classification_loss: 0.2216
2572/3000 [========================>.....] - ETA: 2:55 - loss: 0.6707 - regression_loss: 0.4492 - classification_loss: 0.2215
2573/3000 [========================>.....] - ETA: 2:55 - loss: 0.6707 - regression_loss: 0.4492 - classification_loss: 0.2215
2574/3000 [========================>.....] - ETA: 2:54 - loss: 0.6707 - regression_loss: 0.4492 - classification_loss: 0.2215
2575/3000 [========================>.....] - ETA: 2:54 - loss: 0.6707 - regression_loss: 0.4492 - classification_loss: 0.2215
2576/3000 [========================>.....] - ETA: 2:54 - loss: 0.6706 - regression_loss: 0.4492 - classification_loss: 0.2214
2577/3000 [========================>.....] - ETA: 2:53 - loss: 0.6706 - regression_loss: 0.4492 - classification_loss: 0.2214
2578/3000 [========================>.....] - ETA: 2:53 - loss: 0.6705 - regression_loss: 0.4491 - classification_loss: 0.2214
2579/3000 [========================>.....] - ETA: 2:52 - loss: 0.6704 - regression_loss: 0.4491 - classification_loss: 0.2213
2580/3000 [========================>.....] - ETA: 2:52 - loss: 0.6703 - regression_loss: 0.4491 - classification_loss: 0.2213
2581/3000 [========================>.....] - ETA: 2:52 - loss: 0.6702 - regression_loss: 0.4490 - classification_loss: 0.2212
2582/3000 [========================>.....] - ETA: 2:51 - loss: 0.6701 - regression_loss: 0.4490 - classification_loss: 0.2212
2583/3000 [========================>.....] - ETA: 2:51 - loss: 0.6700 - regression_loss: 0.4489 - classification_loss: 0.2211
2584/3000 [========================>.....] - ETA: 2:50 - loss: 0.6698 - regression_loss: 0.4488 - classification_loss: 0.2210
2585/3000 [========================>.....] - ETA: 2:50 - loss: 0.6698 - regression_loss: 0.4488 - classification_loss: 0.2210
2586/3000 [========================>.....] - ETA: 2:50 - loss: 0.6697 - regression_loss: 0.4488 - classification_loss: 0.2209
2587/3000 [========================>.....] - ETA: 2:49 - loss: 0.6695 - regression_loss: 0.4486 - classification_loss: 0.2209
2588/3000 [========================>.....] - ETA: 2:49 - loss: 0.6694 - regression_loss: 0.4486 - classification_loss: 0.2208
2589/3000 [========================>.....] - ETA: 2:48 - loss: 0.6694 - regression_loss: 0.4486 - classification_loss: 0.2208
2590/3000 [========================>.....] - ETA: 2:48 - loss: 0.6692 - regression_loss: 0.4485 - classification_loss: 0.2207
2591/3000 [========================>.....] - ETA: 2:47 - loss: 0.6692 - regression_loss: 0.4485 - classification_loss: 0.2207
2592/3000 [========================>.....] - ETA: 2:47 - loss: 0.6692 - regression_loss: 0.4485 - classification_loss: 0.2207
2593/3000 [========================>.....] - ETA: 2:47 - loss: 0.6690 - regression_loss: 0.4484 - classification_loss: 0.2206
2594/3000 [========================>.....] - ETA: 2:46 - loss: 0.6688 - regression_loss: 0.4483 - classification_loss: 0.2205
2595/3000 [========================>.....] - ETA: 2:46 - loss: 0.6687 - regression_loss: 0.4482 - classification_loss: 0.2205
2596/3000 [========================>.....] - ETA: 2:45 - loss: 0.6687 - regression_loss: 0.4482 - classification_loss: 0.2205
2597/3000 [========================>.....] - ETA: 2:45 - loss: 0.6687 - regression_loss: 0.4482 - classification_loss: 0.2204
2598/3000 [========================>.....] - ETA: 2:45 - loss: 0.6687 - regression_loss: 0.4483 - classification_loss: 0.2204
2599/3000 [========================>.....] - ETA: 2:44 - loss: 0.6686 - regression_loss: 0.4483 - classification_loss: 0.2203
2600/3000 [=========================>....] - ETA: 2:44 - loss: 0.6685 - regression_loss: 0.4482 - classification_loss: 0.2203
2601/3000 [=========================>....] - ETA: 2:43 - loss: 0.6684 - regression_loss: 0.4482 - classification_loss: 0.2203
2602/3000 [=========================>....] - ETA: 2:43 - loss: 0.6684 - regression_loss: 0.4482 - classification_loss: 0.2202
2603/3000 [=========================>....] - ETA: 2:43 - loss: 0.6683 - regression_loss: 0.4482 - classification_loss: 0.2202
2604/3000 [=========================>....] - ETA: 2:42 - loss: 0.6683 - regression_loss: 0.4482 - classification_loss: 0.2201
2605/3000 [=========================>....] - ETA: 2:42 - loss: 0.6682 - regression_loss: 0.4481 - classification_loss: 0.2201
2606/3000 [=========================>....] - ETA: 2:41 - loss: 0.6681 - regression_loss: 0.4481 - classification_loss: 0.2200
2607/3000 [=========================>....] - ETA: 2:41 - loss: 0.6681 - regression_loss: 0.4481 - classification_loss: 0.2200
2608/3000 [=========================>....] - ETA: 2:41 - loss: 0.6680 - regression_loss: 0.4481 - classification_loss: 0.2199
2609/3000 [=========================>....] - ETA: 2:40 - loss: 0.6680 - regression_loss: 0.4481 - classification_loss: 0.2199
2610/3000 [=========================>....] - ETA: 2:40 - loss: 0.6678 - regression_loss: 0.4479 - classification_loss: 0.2199
2611/3000 [=========================>....] - ETA: 2:39 - loss: 0.6678 - regression_loss: 0.4479 - classification_loss: 0.2198
2612/3000 [=========================>....] - ETA: 2:39 - loss: 0.6676 - regression_loss: 0.4478 - classification_loss: 0.2198
2613/3000 [=========================>....] - ETA: 2:38 - loss: 0.6676 - regression_loss: 0.4479 - classification_loss: 0.2197
2614/3000 [=========================>....] - ETA: 2:38 - loss: 0.6675 - regression_loss: 0.4478 - classification_loss: 0.2197
2615/3000 [=========================>....] - ETA: 2:38 - loss: 0.6673 - regression_loss: 0.4477 - classification_loss: 0.2196
2616/3000 [=========================>....] - ETA: 2:37 - loss: 0.6673 - regression_loss: 0.4476 - classification_loss: 0.2196
2617/3000 [=========================>....] - ETA: 2:37 - loss: 0.6672 - regression_loss: 0.4476 - classification_loss: 0.2196
2618/3000 [=========================>....] - ETA: 2:36 - loss: 0.6671 - regression_loss: 0.4475 - classification_loss: 0.2196
2619/3000 [=========================>....] - ETA: 2:36 - loss: 0.6670 - regression_loss: 0.4475 - classification_loss: 0.2195
2620/3000 [=========================>....] - ETA: 2:36 - loss: 0.6669 - regression_loss: 0.4474 - classification_loss: 0.2195
2621/3000 [=========================>....] - ETA: 2:35 - loss: 0.6667 - regression_loss: 0.4473 - classification_loss: 0.2194
2622/3000 [=========================>....] - ETA: 2:35 - loss: 0.6666 - regression_loss: 0.4473 - classification_loss: 0.2193
2623/3000 [=========================>....] - ETA: 2:34 - loss: 0.6666 - regression_loss: 0.4473 - classification_loss: 0.2193
2624/3000 [=========================>....] - ETA: 2:34 - loss: 0.6666 - regression_loss: 0.4474 - classification_loss: 0.2192
2625/3000 [=========================>....] - ETA: 2:34 - loss: 0.6665 - regression_loss: 0.4473 - classification_loss: 0.2192
2626/3000 [=========================>....] - ETA: 2:33 - loss: 0.6664 - regression_loss: 0.4473 - classification_loss: 0.2192
2627/3000 [=========================>....] - ETA: 2:33 - loss: 0.6663 - regression_loss: 0.4472 - classification_loss: 0.2191
2628/3000 [=========================>....] - ETA: 2:32 - loss: 0.6662 - regression_loss: 0.4471 - classification_loss: 0.2191
2629/3000 [=========================>....] - ETA: 2:32 - loss: 0.6661 - regression_loss: 0.4471 - classification_loss: 0.2190
2630/3000 [=========================>....] - ETA: 2:31 - loss: 0.6660 - regression_loss: 0.4471 - classification_loss: 0.2190
2631/3000 [=========================>....] - ETA: 2:31 - loss: 0.6659 - regression_loss: 0.4470 - classification_loss: 0.2189
2632/3000 [=========================>....] - ETA: 2:31 - loss: 0.6659 - regression_loss: 0.4471 - classification_loss: 0.2189
2633/3000 [=========================>....] - ETA: 2:30 - loss: 0.6659 - regression_loss: 0.4471 - classification_loss: 0.2188
2634/3000 [=========================>....] - ETA: 2:30 - loss: 0.6658 - regression_loss: 0.4471 - classification_loss: 0.2188
2635/3000 [=========================>....] - ETA: 2:29 - loss: 0.6657 - regression_loss: 0.4471 - classification_loss: 0.2187
2636/3000 [=========================>....] - ETA: 2:29 - loss: 0.6657 - regression_loss: 0.4471 - classification_loss: 0.2186
2637/3000 [=========================>....] - ETA: 2:29 - loss: 0.6656 - regression_loss: 0.4470 - classification_loss: 0.2186
2638/3000 [=========================>....] - ETA: 2:28 - loss: 0.6655 - regression_loss: 0.4470 - classification_loss: 0.2186
2639/3000 [=========================>....] - ETA: 2:28 - loss: 0.6654 - regression_loss: 0.4469 - classification_loss: 0.2185
2640/3000 [=========================>....] - ETA: 2:27 - loss: 0.6652 - regression_loss: 0.4468 - classification_loss: 0.2184
2641/3000 [=========================>....] - ETA: 2:27 - loss: 0.6652 - regression_loss: 0.4468 - classification_loss: 0.2184
2642/3000 [=========================>....] - ETA: 2:27 - loss: 0.6652 - regression_loss: 0.4468 - classification_loss: 0.2183
2643/3000 [=========================>....] - ETA: 2:26 - loss: 0.6650 - regression_loss: 0.4467 - classification_loss: 0.2183
2644/3000 [=========================>....] - ETA: 2:26 - loss: 0.6650 - regression_loss: 0.4468 - classification_loss: 0.2182
2645/3000 [=========================>....] - ETA: 2:25 - loss: 0.6649 - regression_loss: 0.4468 - classification_loss: 0.2182
2646/3000 [=========================>....] - ETA: 2:25 - loss: 0.6648 - regression_loss: 0.4467 - classification_loss: 0.2181
2647/3000 [=========================>....] - ETA: 2:25 - loss: 0.6648 - regression_loss: 0.4467 - classification_loss: 0.2181
2648/3000 [=========================>....] - ETA: 2:24 - loss: 0.6647 - regression_loss: 0.4467 - classification_loss: 0.2180
2649/3000 [=========================>....] - ETA: 2:24 - loss: 0.6646 - regression_loss: 0.4467 - classification_loss: 0.2180
2650/3000 [=========================>....] - ETA: 2:23 - loss: 0.6646 - regression_loss: 0.4467 - classification_loss: 0.2179
2651/3000 [=========================>....] - ETA: 2:23 - loss: 0.6645 - regression_loss: 0.4466 - classification_loss: 0.2178
2652/3000 [=========================>....] - ETA: 2:22 - loss: 0.6644 - regression_loss: 0.4466 - classification_loss: 0.2178
2653/3000 [=========================>....] - ETA: 2:22 - loss: 0.6643 - regression_loss: 0.4466 - classification_loss: 0.2177
2654/3000 [=========================>....] - ETA: 2:22 - loss: 0.6643 - regression_loss: 0.4466 - classification_loss: 0.2177
2655/3000 [=========================>....] - ETA: 2:21 - loss: 0.6642 - regression_loss: 0.4466 - classification_loss: 0.2177
2656/3000 [=========================>....] - ETA: 2:21 - loss: 0.6643 - regression_loss: 0.4467 - classification_loss: 0.2176
2657/3000 [=========================>....] - ETA: 2:20 - loss: 0.6642 - regression_loss: 0.4466 - classification_loss: 0.2176
2658/3000 [=========================>....] - ETA: 2:20 - loss: 0.6640 - regression_loss: 0.4465 - classification_loss: 0.2175
2659/3000 [=========================>....] - ETA: 2:20 - loss: 0.6639 - regression_loss: 0.4464 - classification_loss: 0.2175
2660/3000 [=========================>....] - ETA: 2:19 - loss: 0.6639 - regression_loss: 0.4464 - classification_loss: 0.2175
2661/3000 [=========================>....] - ETA: 2:19 - loss: 0.6638 - regression_loss: 0.4464 - classification_loss: 0.2174
2662/3000 [=========================>....] - ETA: 2:18 - loss: 0.6637 - regression_loss: 0.4464 - classification_loss: 0.2173
2663/3000 [=========================>....] - ETA: 2:18 - loss: 0.6637 - regression_loss: 0.4463 - classification_loss: 0.2173
2664/3000 [=========================>....] - ETA: 2:18 - loss: 0.6635 - regression_loss: 0.4463 - classification_loss: 0.2173
2665/3000 [=========================>....] - ETA: 2:17 - loss: 0.6635 - regression_loss: 0.4462 - classification_loss: 0.2173
2666/3000 [=========================>....] - ETA: 2:17 - loss: 0.6634 - regression_loss: 0.4461 - classification_loss: 0.2172
2667/3000 [=========================>....] - ETA: 2:16 - loss: 0.6632 - regression_loss: 0.4461 - classification_loss: 0.2172
2668/3000 [=========================>....] - ETA: 2:16 - loss: 0.6631 - regression_loss: 0.4460 - classification_loss: 0.2171
2669/3000 [=========================>....] - ETA: 2:15 - loss: 0.6630 - regression_loss: 0.4459 - classification_loss: 0.2170
2670/3000 [=========================>....] - ETA: 2:15 - loss: 0.6629 - regression_loss: 0.4459 - classification_loss: 0.2170
2671/3000 [=========================>....] - ETA: 2:15 - loss: 0.6628 - regression_loss: 0.4459 - classification_loss: 0.2169
2672/3000 [=========================>....] - ETA: 2:14 - loss: 0.6627 - regression_loss: 0.4458 - classification_loss: 0.2169
2673/3000 [=========================>....] - ETA: 2:14 - loss: 0.6626 - regression_loss: 0.4457 - classification_loss: 0.2169
2674/3000 [=========================>....] - ETA: 2:13 - loss: 0.6625 - regression_loss: 0.4457 - classification_loss: 0.2168
2675/3000 [=========================>....] - ETA: 2:13 - loss: 0.6624 - regression_loss: 0.4456 - classification_loss: 0.2168
2676/3000 [=========================>....] - ETA: 2:13 - loss: 0.6622 - regression_loss: 0.4455 - classification_loss: 0.2167
2677/3000 [=========================>....] - ETA: 2:12 - loss: 0.6622 - regression_loss: 0.4455 - classification_loss: 0.2167
2678/3000 [=========================>....] - ETA: 2:12 - loss: 0.6621 - regression_loss: 0.4454 - classification_loss: 0.2166
2679/3000 [=========================>....] - ETA: 2:11 - loss: 0.6620 - regression_loss: 0.4454 - classification_loss: 0.2166
2680/3000 [=========================>....] - ETA: 2:11 - loss: 0.6620 - regression_loss: 0.4454 - classification_loss: 0.2165
2681/3000 [=========================>....] - ETA: 2:11 - loss: 0.6620 - regression_loss: 0.4455 - classification_loss: 0.2165
2682/3000 [=========================>....] - ETA: 2:10 - loss: 0.6618 - regression_loss: 0.4454 - classification_loss: 0.2164
2683/3000 [=========================>....] - ETA: 2:10 - loss: 0.6617 - regression_loss: 0.4453 - classification_loss: 0.2164
2684/3000 [=========================>....] - ETA: 2:09 - loss: 0.6616 - regression_loss: 0.4452 - classification_loss: 0.2164
2685/3000 [=========================>....] - ETA: 2:09 - loss: 0.6616 - regression_loss: 0.4453 - classification_loss: 0.2163
2686/3000 [=========================>....] - ETA: 2:08 - loss: 0.6616 - regression_loss: 0.4453 - classification_loss: 0.2163
2687/3000 [=========================>....] - ETA: 2:08 - loss: 0.6614 - regression_loss: 0.4452 - classification_loss: 0.2162
2688/3000 [=========================>....] - ETA: 2:08 - loss: 0.6614 - regression_loss: 0.4452 - classification_loss: 0.2162
2689/3000 [=========================>....] - ETA: 2:07 - loss: 0.6613 - regression_loss: 0.4452 - classification_loss: 0.2161
2690/3000 [=========================>....] - ETA: 2:07 - loss: 0.6613 - regression_loss: 0.4452 - classification_loss: 0.2161
2691/3000 [=========================>....] - ETA: 2:06 - loss: 0.6612 - regression_loss: 0.4452 - classification_loss: 0.2161
2692/3000 [=========================>....] - ETA: 2:06 - loss: 0.6611 - regression_loss: 0.4451 - classification_loss: 0.2160
2693/3000 [=========================>....] - ETA: 2:06 - loss: 0.6610 - regression_loss: 0.4451 - classification_loss: 0.2159
2694/3000 [=========================>....] - ETA: 2:05 - loss: 0.6610 - regression_loss: 0.4450 - classification_loss: 0.2159
2695/3000 [=========================>....] - ETA: 2:05 - loss: 0.6609 - regression_loss: 0.4450 - classification_loss: 0.2159
2696/3000 [=========================>....] - ETA: 2:04 - loss: 0.6608 - regression_loss: 0.4449 - classification_loss: 0.2158
2697/3000 [=========================>....] - ETA: 2:04 - loss: 0.6607 - regression_loss: 0.4449 - classification_loss: 0.2158
2698/3000 [=========================>....] - ETA: 2:04 - loss: 0.6606 - regression_loss: 0.4448 - classification_loss: 0.2158
2699/3000 [=========================>....] - ETA: 2:03 - loss: 0.6606 - regression_loss: 0.4449 - classification_loss: 0.2157
2700/3000 [==========================>...] - ETA: 2:03 - loss: 0.6606 - regression_loss: 0.4449 - classification_loss: 0.2157
2701/3000 [==========================>...] - ETA: 2:02 - loss: 0.6605 - regression_loss: 0.4449 - classification_loss: 0.2156
2702/3000 [==========================>...] - ETA: 2:02 - loss: 0.6604 - regression_loss: 0.4448 - classification_loss: 0.2156
2703/3000 [==========================>...] - ETA: 2:02 - loss: 0.6603 - regression_loss: 0.4448 - classification_loss: 0.2155
2704/3000 [==========================>...] - ETA: 2:01 - loss: 0.6603 - regression_loss: 0.4448 - classification_loss: 0.2155
2705/3000 [==========================>...] - ETA: 2:01 - loss: 0.6603 - regression_loss: 0.4448 - classification_loss: 0.2155
2706/3000 [==========================>...] - ETA: 2:00 - loss: 0.6602 - regression_loss: 0.4447 - classification_loss: 0.2154
2707/3000 [==========================>...] - ETA: 2:00 - loss: 0.6601 - regression_loss: 0.4447 - classification_loss: 0.2154
2708/3000 [==========================>...] - ETA: 1:59 - loss: 0.6600 - regression_loss: 0.4446 - classification_loss: 0.2153
2709/3000 [==========================>...] - ETA: 1:59 - loss: 0.6598 - regression_loss: 0.4446 - classification_loss: 0.2153
2710/3000 [==========================>...] - ETA: 1:59 - loss: 0.6598 - regression_loss: 0.4446 - classification_loss: 0.2152
2711/3000 [==========================>...] - ETA: 1:58 - loss: 0.6597 - regression_loss: 0.4446 - classification_loss: 0.2152
2712/3000 [==========================>...] - ETA: 1:58 - loss: 0.6596 - regression_loss: 0.4445 - classification_loss: 0.2151
2713/3000 [==========================>...] - ETA: 1:57 - loss: 0.6596 - regression_loss: 0.4445 - classification_loss: 0.2151
2714/3000 [==========================>...] - ETA: 1:57 - loss: 0.6595 - regression_loss: 0.4445 - classification_loss: 0.2150
2715/3000 [==========================>...] - ETA: 1:57 - loss: 0.6594 - regression_loss: 0.4444 - classification_loss: 0.2150
2716/3000 [==========================>...] - ETA: 1:56 - loss: 0.6593 - regression_loss: 0.4444 - classification_loss: 0.2149
2717/3000 [==========================>...] - ETA: 1:56 - loss: 0.6592 - regression_loss: 0.4443 - classification_loss: 0.2149
2718/3000 [==========================>...] - ETA: 1:55 - loss: 0.6592 - regression_loss: 0.4444 - classification_loss: 0.2148
2719/3000 [==========================>...] - ETA: 1:55 - loss: 0.6593 - regression_loss: 0.4445 - classification_loss: 0.2148
2720/3000 [==========================>...] - ETA: 1:55 - loss: 0.6592 - regression_loss: 0.4444 - classification_loss: 0.2148
2721/3000 [==========================>...] - ETA: 1:54 - loss: 0.6591 - regression_loss: 0.4444 - classification_loss: 0.2147
2722/3000 [==========================>...] - ETA: 1:54 - loss: 0.6591 - regression_loss: 0.4444 - classification_loss: 0.2147
2723/3000 [==========================>...] - ETA: 1:53 - loss: 0.6590 - regression_loss: 0.4444 - classification_loss: 0.2146
2724/3000 [==========================>...] - ETA: 1:53 - loss: 0.6589 - regression_loss: 0.4443 - classification_loss: 0.2146
2725/3000 [==========================>...] - ETA: 1:52 - loss: 0.6589 - regression_loss: 0.4444 - classification_loss: 0.2146
2726/3000 [==========================>...] - ETA: 1:52 - loss: 0.6588 - regression_loss: 0.4443 - classification_loss: 0.2145
2727/3000 [==========================>...] - ETA: 1:52 - loss: 0.6587 - regression_loss: 0.4443 - classification_loss: 0.2145
2728/3000 [==========================>...] - ETA: 1:51 - loss: 0.6586 - regression_loss: 0.4442 - classification_loss: 0.2144
2729/3000 [==========================>...] - ETA: 1:51 - loss: 0.6586 - regression_loss: 0.4442 - classification_loss: 0.2144
2730/3000 [==========================>...] - ETA: 1:50 - loss: 0.6586 - regression_loss: 0.4442 - classification_loss: 0.2144
2731/3000 [==========================>...] - ETA: 1:50 - loss: 0.6586 - regression_loss: 0.4443 - classification_loss: 0.2143
2732/3000 [==========================>...] - ETA: 1:50 - loss: 0.6585 - regression_loss: 0.4442 - classification_loss: 0.2143
2733/3000 [==========================>...] - ETA: 1:49 - loss: 0.6585 - regression_loss: 0.4442 - classification_loss: 0.2143
2734/3000 [==========================>...] - ETA: 1:49 - loss: 0.6585 - regression_loss: 0.4442 - classification_loss: 0.2143
2735/3000 [==========================>...] - ETA: 1:48 - loss: 0.6584 - regression_loss: 0.4441 - classification_loss: 0.2142
2736/3000 [==========================>...] - ETA: 1:48 - loss: 0.6583 - regression_loss: 0.4441 - classification_loss: 0.2142
2737/3000 [==========================>...] - ETA: 1:48 - loss: 0.6584 - regression_loss: 0.4442 - classification_loss: 0.2142
2738/3000 [==========================>...] - ETA: 1:47 - loss: 0.6583 - regression_loss: 0.4442 - classification_loss: 0.2141
2739/3000 [==========================>...] - ETA: 1:47 - loss: 0.6582 - regression_loss: 0.4441 - classification_loss: 0.2141
2740/3000 [==========================>...] - ETA: 1:46 - loss: 0.6581 - regression_loss: 0.4440 - classification_loss: 0.2140
2741/3000 [==========================>...] - ETA: 1:46 - loss: 0.6580 - regression_loss: 0.4440 - classification_loss: 0.2140
2742/3000 [==========================>...] - ETA: 1:45 - loss: 0.6580 - regression_loss: 0.4440 - classification_loss: 0.2139
2743/3000 [==========================>...] - ETA: 1:45 - loss: 0.6579 - regression_loss: 0.4440 - classification_loss: 0.2139
2744/3000 [==========================>...] - ETA: 1:45 - loss: 0.6578 - regression_loss: 0.4439 - classification_loss: 0.2138
2745/3000 [==========================>...] - ETA: 1:44 - loss: 0.6578 - regression_loss: 0.4440 - classification_loss: 0.2138
2746/3000 [==========================>...] - ETA: 1:44 - loss: 0.6577 - regression_loss: 0.4440 - classification_loss: 0.2137
2747/3000 [==========================>...] - ETA: 1:43 - loss: 0.6577 - regression_loss: 0.4440 - classification_loss: 0.2137
2748/3000 [==========================>...] - ETA: 1:43 - loss: 0.6576 - regression_loss: 0.4439 - classification_loss: 0.2137
2749/3000 [==========================>...] - ETA: 1:43 - loss: 0.6575 - regression_loss: 0.4439 - classification_loss: 0.2136
2750/3000 [==========================>...] - ETA: 1:42 - loss: 0.6574 - regression_loss: 0.4439 - classification_loss: 0.2136
2751/3000 [==========================>...] - ETA: 1:42 - loss: 0.6573 - regression_loss: 0.4438 - classification_loss: 0.2135
2752/3000 [==========================>...] - ETA: 1:41 - loss: 0.6572 - regression_loss: 0.4437 - classification_loss: 0.2134
2753/3000 [==========================>...] - ETA: 1:41 - loss: 0.6571 - regression_loss: 0.4436 - classification_loss: 0.2134
2754/3000 [==========================>...] - ETA: 1:41 - loss: 0.6570 - regression_loss: 0.4437 - classification_loss: 0.2134
2755/3000 [==========================>...] - ETA: 1:40 - loss: 0.6570 - regression_loss: 0.4436 - classification_loss: 0.2133
2756/3000 [==========================>...] - ETA: 1:40 - loss: 0.6569 - regression_loss: 0.4436 - classification_loss: 0.2133
2757/3000 [==========================>...] - ETA: 1:39 - loss: 0.6568 - regression_loss: 0.4436 - classification_loss: 0.2132
2758/3000 [==========================>...] - ETA: 1:39 - loss: 0.6567 - regression_loss: 0.4435 - classification_loss: 0.2132
2759/3000 [==========================>...] - ETA: 1:39 - loss: 0.6566 - regression_loss: 0.4434 - classification_loss: 0.2131
2760/3000 [==========================>...] - ETA: 1:38 - loss: 0.6564 - regression_loss: 0.4433 - classification_loss: 0.2131
2761/3000 [==========================>...] - ETA: 1:38 - loss: 0.6563 - regression_loss: 0.4432 - classification_loss: 0.2131
2762/3000 [==========================>...] - ETA: 1:37 - loss: 0.6564 - regression_loss: 0.4433 - classification_loss: 0.2130
2763/3000 [==========================>...] - ETA: 1:37 - loss: 0.6563 - regression_loss: 0.4433 - classification_loss: 0.2130
2764/3000 [==========================>...] - ETA: 1:36 - loss: 0.6562 - regression_loss: 0.4432 - classification_loss: 0.2129
2765/3000 [==========================>...] - ETA: 1:36 - loss: 0.6562 - regression_loss: 0.4433 - classification_loss: 0.2129
2766/3000 [==========================>...] - ETA: 1:36 - loss: 0.6561 - regression_loss: 0.4432 - classification_loss: 0.2129
2767/3000 [==========================>...] - ETA: 1:35 - loss: 0.6560 - regression_loss: 0.4432 - classification_loss: 0.2129
2768/3000 [==========================>...] - ETA: 1:35 - loss: 0.6560 - regression_loss: 0.4432 - classification_loss: 0.2128
2769/3000 [==========================>...] - ETA: 1:34 - loss: 0.6558 - regression_loss: 0.4431 - classification_loss: 0.2128
2770/3000 [==========================>...] - ETA: 1:34 - loss: 0.6557 - regression_loss: 0.4430 - classification_loss: 0.2127
2771/3000 [==========================>...] - ETA: 1:34 - loss: 0.6556 - regression_loss: 0.4429 - classification_loss: 0.2127
2772/3000 [==========================>...] - ETA: 1:33 - loss: 0.6555 - regression_loss: 0.4428 - classification_loss: 0.2126
2773/3000 [==========================>...] - ETA: 1:33 - loss: 0.6553 - regression_loss: 0.4428 - classification_loss: 0.2126
2774/3000 [==========================>...] - ETA: 1:32 - loss: 0.6552 - regression_loss: 0.4427 - classification_loss: 0.2125
2775/3000 [==========================>...] - ETA: 1:32 - loss: 0.6551 - regression_loss: 0.4426 - classification_loss: 0.2125
2776/3000 [==========================>...] - ETA: 1:32 - loss: 0.6551 - regression_loss: 0.4426 - classification_loss: 0.2125
2777/3000 [==========================>...] - ETA: 1:31 - loss: 0.6550 - regression_loss: 0.4426 - classification_loss: 0.2124
2778/3000 [==========================>...] - ETA: 1:31 - loss: 0.6550 - regression_loss: 0.4426 - classification_loss: 0.2124
2779/3000 [==========================>...] - ETA: 1:30 - loss: 0.6551 - regression_loss: 0.4427 - classification_loss: 0.2124
2780/3000 [==========================>...] - ETA: 1:30 - loss: 0.6550 - regression_loss: 0.4426 - classification_loss: 0.2124
2781/3000 [==========================>...] - ETA: 1:29 - loss: 0.6550 - regression_loss: 0.4426 - classification_loss: 0.2123
2782/3000 [==========================>...] - ETA: 1:29 - loss: 0.6549 - regression_loss: 0.4426 - classification_loss: 0.2123
2783/3000 [==========================>...] - ETA: 1:29 - loss: 0.6548 - regression_loss: 0.4426 - classification_loss: 0.2122
2784/3000 [==========================>...] - ETA: 1:28 - loss: 0.6547 - regression_loss: 0.4425 - classification_loss: 0.2122
2785/3000 [==========================>...] - ETA: 1:28 - loss: 0.6547 - regression_loss: 0.4425 - classification_loss: 0.2121
2786/3000 [==========================>...] - ETA: 1:27 - loss: 0.6546 - regression_loss: 0.4425 - classification_loss: 0.2122
2787/3000 [==========================>...] - ETA: 1:27 - loss: 0.6545 - regression_loss: 0.4424 - classification_loss: 0.2121
2788/3000 [==========================>...] - ETA: 1:27 - loss: 0.6544 - regression_loss: 0.4423 - classification_loss: 0.2121
2789/3000 [==========================>...] - ETA: 1:26 - loss: 0.6543 - regression_loss: 0.4423 - classification_loss: 0.2120
2790/3000 [==========================>...] - ETA: 1:26 - loss: 0.6542 - regression_loss: 0.4422 - classification_loss: 0.2120
2791/3000 [==========================>...] - ETA: 1:25 - loss: 0.6541 - regression_loss: 0.4422 - classification_loss: 0.2119
2792/3000 [==========================>...] - ETA: 1:25 - loss: 0.6540 - regression_loss: 0.4421 - classification_loss: 0.2119
2793/3000 [==========================>...] - ETA: 1:25 - loss: 0.6539 - regression_loss: 0.4421 - classification_loss: 0.2119
2794/3000 [==========================>...] - ETA: 1:24 - loss: 0.6538 - regression_loss: 0.4420 - classification_loss: 0.2118
2795/3000 [==========================>...] - ETA: 1:24 - loss: 0.6537 - regression_loss: 0.4419 - classification_loss: 0.2118
2796/3000 [==========================>...] - ETA: 1:23 - loss: 0.6535 - regression_loss: 0.4418 - classification_loss: 0.2117
2797/3000 [==========================>...] - ETA: 1:23 - loss: 0.6534 - regression_loss: 0.4417 - classification_loss: 0.2117
2798/3000 [==========================>...] - ETA: 1:22 - loss: 0.6534 - regression_loss: 0.4418 - classification_loss: 0.2116
2799/3000 [==========================>...] - ETA: 1:22 - loss: 0.6534 - regression_loss: 0.4418 - classification_loss: 0.2116
2800/3000 [===========================>..] - ETA: 1:22 - loss: 0.6533 - regression_loss: 0.4418 - classification_loss: 0.2115
2801/3000 [===========================>..] - ETA: 1:21 - loss: 0.6534 - regression_loss: 0.4419 - classification_loss: 0.2115
2802/3000 [===========================>..] - ETA: 1:21 - loss: 0.6533 - regression_loss: 0.4418 - classification_loss: 0.2115
2803/3000 [===========================>..] - ETA: 1:20 - loss: 0.6532 - regression_loss: 0.4418 - classification_loss: 0.2115
2804/3000 [===========================>..] - ETA: 1:20 - loss: 0.6531 - regression_loss: 0.4417 - classification_loss: 0.2114
2805/3000 [===========================>..] - ETA: 1:20 - loss: 0.6531 - regression_loss: 0.4417 - classification_loss: 0.2114
2806/3000 [===========================>..] - ETA: 1:19 - loss: 0.6530 - regression_loss: 0.4416 - classification_loss: 0.2113
2807/3000 [===========================>..] - ETA: 1:19 - loss: 0.6528 - regression_loss: 0.4416 - classification_loss: 0.2113
2808/3000 [===========================>..] - ETA: 1:18 - loss: 0.6527 - regression_loss: 0.4415 - classification_loss: 0.2112
2809/3000 [===========================>..] - ETA: 1:18 - loss: 0.6526 - regression_loss: 0.4414 - classification_loss: 0.2112
2810/3000 [===========================>..] - ETA: 1:18 - loss: 0.6524 - regression_loss: 0.4413 - classification_loss: 0.2111
2811/3000 [===========================>..] - ETA: 1:17 - loss: 0.6523 - regression_loss: 0.4413 - classification_loss: 0.2111
2812/3000 [===========================>..] - ETA: 1:17 - loss: 0.6523 - regression_loss: 0.4412 - classification_loss: 0.2111
2813/3000 [===========================>..] - ETA: 1:16 - loss: 0.6522 - regression_loss: 0.4412 - classification_loss: 0.2110
2814/3000 [===========================>..] - ETA: 1:16 - loss: 0.6521 - regression_loss: 0.4411 - classification_loss: 0.2110
2815/3000 [===========================>..] - ETA: 1:16 - loss: 0.6520 - regression_loss: 0.4410 - classification_loss: 0.2110
2816/3000 [===========================>..] - ETA: 1:15 - loss: 0.6519 - regression_loss: 0.4410 - classification_loss: 0.2109
2817/3000 [===========================>..] - ETA: 1:15 - loss: 0.6519 - regression_loss: 0.4410 - classification_loss: 0.2109
2818/3000 [===========================>..] - ETA: 1:14 - loss: 0.6519 - regression_loss: 0.4410 - classification_loss: 0.2109
2819/3000 [===========================>..] - ETA: 1:14 - loss: 0.6519 - regression_loss: 0.4410 - classification_loss: 0.2109
2820/3000 [===========================>..] - ETA: 1:13 - loss: 0.6519 - regression_loss: 0.4410 - classification_loss: 0.2109
2821/3000 [===========================>..] - ETA: 1:13 - loss: 0.6518 - regression_loss: 0.4409 - classification_loss: 0.2108
2822/3000 [===========================>..] - ETA: 1:13 - loss: 0.6517 - regression_loss: 0.4408 - classification_loss: 0.2108
2823/3000 [===========================>..] - ETA: 1:12 - loss: 0.6515 - regression_loss: 0.4408 - classification_loss: 0.2108
2824/3000 [===========================>..] - ETA: 1:12 - loss: 0.6514 - regression_loss: 0.4407 - classification_loss: 0.2107
2825/3000 [===========================>..] - ETA: 1:11 - loss: 0.6514 - regression_loss: 0.4407 - classification_loss: 0.2107
2826/3000 [===========================>..] - ETA: 1:11 - loss: 0.6513 - regression_loss: 0.4407 - classification_loss: 0.2107
2827/3000 [===========================>..] - ETA: 1:11 - loss: 0.6513 - regression_loss: 0.4406 - classification_loss: 0.2106
2828/3000 [===========================>..] - ETA: 1:10 - loss: 0.6513 - regression_loss: 0.4407 - classification_loss: 0.2106
2829/3000 [===========================>..] - ETA: 1:10 - loss: 0.6512 - regression_loss: 0.4406 - classification_loss: 0.2106
2830/3000 [===========================>..] - ETA: 1:09 - loss: 0.6511 - regression_loss: 0.4406 - classification_loss: 0.2105
2831/3000 [===========================>..] - ETA: 1:09 - loss: 0.6509 - regression_loss: 0.4405 - classification_loss: 0.2105
2832/3000 [===========================>..] - ETA: 1:09 - loss: 0.6509 - regression_loss: 0.4405 - classification_loss: 0.2104
2833/3000 [===========================>..] - ETA: 1:08 - loss: 0.6508 - regression_loss: 0.4404 - classification_loss: 0.2104
2834/3000 [===========================>..] - ETA: 1:08 - loss: 0.6507 - regression_loss: 0.4404 - classification_loss: 0.2103
2835/3000 [===========================>..] - ETA: 1:07 - loss: 0.6506 - regression_loss: 0.4403 - classification_loss: 0.2103
2836/3000 [===========================>..] - ETA: 1:07 - loss: 0.6505 - regression_loss: 0.4403 - classification_loss: 0.2103
2837/3000 [===========================>..] - ETA: 1:06 - loss: 0.6504 - regression_loss: 0.4402 - classification_loss: 0.2102
2838/3000 [===========================>..] - ETA: 1:06 - loss: 0.6503 - regression_loss: 0.4401 - classification_loss: 0.2101
2839/3000 [===========================>..] - ETA: 1:06 - loss: 0.6502 - regression_loss: 0.4401 - classification_loss: 0.2101
2840/3000 [===========================>..] - ETA: 1:05 - loss: 0.6501 - regression_loss: 0.4400 - classification_loss: 0.2101
2841/3000 [===========================>..] - ETA: 1:05 - loss: 0.6500 - regression_loss: 0.4399 - classification_loss: 0.2100
2842/3000 [===========================>..] - ETA: 1:04 - loss: 0.6499 - regression_loss: 0.4399 - classification_loss: 0.2100
2843/3000 [===========================>..] - ETA: 1:04 - loss: 0.6498 - regression_loss: 0.4398 - classification_loss: 0.2100
2844/3000 [===========================>..] - ETA: 1:04 - loss: 0.6498 - regression_loss: 0.4398 - classification_loss: 0.2100
2845/3000 [===========================>..] - ETA: 1:03 - loss: 0.6498 - regression_loss: 0.4399 - classification_loss: 0.2099
2846/3000 [===========================>..] - ETA: 1:03 - loss: 0.6496 - regression_loss: 0.4398 - classification_loss: 0.2099
2847/3000 [===========================>..] - ETA: 1:02 - loss: 0.6496 - regression_loss: 0.4398 - classification_loss: 0.2098
2848/3000 [===========================>..] - ETA: 1:02 - loss: 0.6496 - regression_loss: 0.4398 - classification_loss: 0.2098
2849/3000 [===========================>..] - ETA: 1:02 - loss: 0.6495 - regression_loss: 0.4398 - classification_loss: 0.2097
2850/3000 [===========================>..] - ETA: 1:01 - loss: 0.6494 - regression_loss: 0.4397 - classification_loss: 0.2097
2851/3000 [===========================>..] - ETA: 1:01 - loss: 0.6493 - regression_loss: 0.4397 - classification_loss: 0.2096
2852/3000 [===========================>..] - ETA: 1:00 - loss: 0.6494 - regression_loss: 0.4398 - classification_loss: 0.2096
2853/3000 [===========================>..] - ETA: 1:00 - loss: 0.6493 - regression_loss: 0.4397 - classification_loss: 0.2096
2854/3000 [===========================>..] - ETA: 59s - loss: 0.6492 - regression_loss: 0.4397 - classification_loss: 0.2095 
2855/3000 [===========================>..] - ETA: 59s - loss: 0.6491 - regression_loss: 0.4397 - classification_loss: 0.2095
2856/3000 [===========================>..] - ETA: 59s - loss: 0.6491 - regression_loss: 0.4396 - classification_loss: 0.2094
2857/3000 [===========================>..] - ETA: 58s - loss: 0.6489 - regression_loss: 0.4395 - classification_loss: 0.2094
2858/3000 [===========================>..] - ETA: 58s - loss: 0.6488 - regression_loss: 0.4395 - classification_loss: 0.2093
2859/3000 [===========================>..] - ETA: 57s - loss: 0.6488 - regression_loss: 0.4395 - classification_loss: 0.2093
2860/3000 [===========================>..] - ETA: 57s - loss: 0.6487 - regression_loss: 0.4395 - classification_loss: 0.2093
2861/3000 [===========================>..] - ETA: 57s - loss: 0.6486 - regression_loss: 0.4394 - classification_loss: 0.2092
2862/3000 [===========================>..] - ETA: 56s - loss: 0.6485 - regression_loss: 0.4393 - classification_loss: 0.2091
2863/3000 [===========================>..] - ETA: 56s - loss: 0.6484 - regression_loss: 0.4394 - classification_loss: 0.2091
2864/3000 [===========================>..] - ETA: 55s - loss: 0.6483 - regression_loss: 0.4393 - classification_loss: 0.2090
2865/3000 [===========================>..] - ETA: 55s - loss: 0.6482 - regression_loss: 0.4392 - classification_loss: 0.2090
2866/3000 [===========================>..] - ETA: 55s - loss: 0.6481 - regression_loss: 0.4391 - classification_loss: 0.2089
2867/3000 [===========================>..] - ETA: 54s - loss: 0.6480 - regression_loss: 0.4391 - classification_loss: 0.2089
2868/3000 [===========================>..] - ETA: 54s - loss: 0.6480 - regression_loss: 0.4391 - classification_loss: 0.2088
2869/3000 [===========================>..] - ETA: 53s - loss: 0.6480 - regression_loss: 0.4392 - classification_loss: 0.2088
2870/3000 [===========================>..] - ETA: 53s - loss: 0.6480 - regression_loss: 0.4392 - classification_loss: 0.2088
2871/3000 [===========================>..] - ETA: 53s - loss: 0.6478 - regression_loss: 0.4391 - classification_loss: 0.2087
2872/3000 [===========================>..] - ETA: 52s - loss: 0.6478 - regression_loss: 0.4391 - classification_loss: 0.2087
2873/3000 [===========================>..] - ETA: 52s - loss: 0.6478 - regression_loss: 0.4392 - classification_loss: 0.2086
2874/3000 [===========================>..] - ETA: 51s - loss: 0.6477 - regression_loss: 0.4391 - classification_loss: 0.2086
2875/3000 [===========================>..] - ETA: 51s - loss: 0.6476 - regression_loss: 0.4391 - classification_loss: 0.2085
2876/3000 [===========================>..] - ETA: 50s - loss: 0.6475 - regression_loss: 0.4390 - classification_loss: 0.2085
2877/3000 [===========================>..] - ETA: 50s - loss: 0.6474 - regression_loss: 0.4390 - classification_loss: 0.2084
2878/3000 [===========================>..] - ETA: 50s - loss: 0.6473 - regression_loss: 0.4389 - classification_loss: 0.2084
2879/3000 [===========================>..] - ETA: 49s - loss: 0.6472 - regression_loss: 0.4389 - classification_loss: 0.2083
2880/3000 [===========================>..] - ETA: 49s - loss: 0.6471 - regression_loss: 0.4388 - classification_loss: 0.2083
2881/3000 [===========================>..] - ETA: 48s - loss: 0.6471 - regression_loss: 0.4388 - classification_loss: 0.2082
2882/3000 [===========================>..] - ETA: 48s - loss: 0.6469 - regression_loss: 0.4387 - classification_loss: 0.2082
2883/3000 [===========================>..] - ETA: 48s - loss: 0.6469 - regression_loss: 0.4387 - classification_loss: 0.2082
2884/3000 [===========================>..] - ETA: 47s - loss: 0.6467 - regression_loss: 0.4386 - classification_loss: 0.2081
2885/3000 [===========================>..] - ETA: 47s - loss: 0.6467 - regression_loss: 0.4386 - classification_loss: 0.2081
2886/3000 [===========================>..] - ETA: 46s - loss: 0.6466 - regression_loss: 0.4385 - classification_loss: 0.2080
2887/3000 [===========================>..] - ETA: 46s - loss: 0.6466 - regression_loss: 0.4386 - classification_loss: 0.2080
2888/3000 [===========================>..] - ETA: 46s - loss: 0.6465 - regression_loss: 0.4385 - classification_loss: 0.2080
2889/3000 [===========================>..] - ETA: 45s - loss: 0.6463 - regression_loss: 0.4384 - classification_loss: 0.2079
2890/3000 [===========================>..] - ETA: 45s - loss: 0.6462 - regression_loss: 0.4384 - classification_loss: 0.2078
2891/3000 [===========================>..] - ETA: 44s - loss: 0.6462 - regression_loss: 0.4384 - classification_loss: 0.2078
2892/3000 [===========================>..] - ETA: 44s - loss: 0.6461 - regression_loss: 0.4383 - classification_loss: 0.2078
2893/3000 [===========================>..] - ETA: 43s - loss: 0.6460 - regression_loss: 0.4383 - classification_loss: 0.2077
2894/3000 [===========================>..] - ETA: 43s - loss: 0.6460 - regression_loss: 0.4383 - classification_loss: 0.2077
2895/3000 [===========================>..] - ETA: 43s - loss: 0.6459 - regression_loss: 0.4383 - classification_loss: 0.2076
2896/3000 [===========================>..] - ETA: 42s - loss: 0.6459 - regression_loss: 0.4383 - classification_loss: 0.2076
2897/3000 [===========================>..] - ETA: 42s - loss: 0.6458 - regression_loss: 0.4382 - classification_loss: 0.2075
2898/3000 [===========================>..] - ETA: 41s - loss: 0.6457 - regression_loss: 0.4382 - classification_loss: 0.2075
2899/3000 [===========================>..] - ETA: 41s - loss: 0.6457 - regression_loss: 0.4382 - classification_loss: 0.2075
2900/3000 [============================>.] - ETA: 41s - loss: 0.6456 - regression_loss: 0.4381 - classification_loss: 0.2074
2901/3000 [============================>.] - ETA: 40s - loss: 0.6455 - regression_loss: 0.4381 - classification_loss: 0.2074
2902/3000 [============================>.] - ETA: 40s - loss: 0.6453 - regression_loss: 0.4380 - classification_loss: 0.2073
2903/3000 [============================>.] - ETA: 39s - loss: 0.6452 - regression_loss: 0.4379 - classification_loss: 0.2073
2904/3000 [============================>.] - ETA: 39s - loss: 0.6452 - regression_loss: 0.4379 - classification_loss: 0.2073
2905/3000 [============================>.] - ETA: 39s - loss: 0.6451 - regression_loss: 0.4379 - classification_loss: 0.2072
2906/3000 [============================>.] - ETA: 38s - loss: 0.6450 - regression_loss: 0.4379 - classification_loss: 0.2071
2907/3000 [============================>.] - ETA: 38s - loss: 0.6450 - regression_loss: 0.4378 - classification_loss: 0.2071
2908/3000 [============================>.] - ETA: 37s - loss: 0.6448 - regression_loss: 0.4377 - classification_loss: 0.2071
2909/3000 [============================>.] - ETA: 37s - loss: 0.6448 - regression_loss: 0.4378 - classification_loss: 0.2070
2910/3000 [============================>.] - ETA: 36s - loss: 0.6447 - regression_loss: 0.4378 - classification_loss: 0.2070
2911/3000 [============================>.] - ETA: 36s - loss: 0.6447 - regression_loss: 0.4377 - classification_loss: 0.2069
2912/3000 [============================>.] - ETA: 36s - loss: 0.6446 - regression_loss: 0.4377 - classification_loss: 0.2069
2913/3000 [============================>.] - ETA: 35s - loss: 0.6444 - regression_loss: 0.4376 - classification_loss: 0.2068
2914/3000 [============================>.] - ETA: 35s - loss: 0.6444 - regression_loss: 0.4376 - classification_loss: 0.2068
2915/3000 [============================>.] - ETA: 34s - loss: 0.6444 - regression_loss: 0.4377 - classification_loss: 0.2067
2916/3000 [============================>.] - ETA: 34s - loss: 0.6443 - regression_loss: 0.4376 - classification_loss: 0.2067
2917/3000 [============================>.] - ETA: 34s - loss: 0.6442 - regression_loss: 0.4375 - classification_loss: 0.2066
2918/3000 [============================>.] - ETA: 33s - loss: 0.6441 - regression_loss: 0.4375 - classification_loss: 0.2066
2919/3000 [============================>.] - ETA: 33s - loss: 0.6440 - regression_loss: 0.4374 - classification_loss: 0.2066
2920/3000 [============================>.] - ETA: 32s - loss: 0.6440 - regression_loss: 0.4374 - classification_loss: 0.2066
2921/3000 [============================>.] - ETA: 32s - loss: 0.6438 - regression_loss: 0.4373 - classification_loss: 0.2065
2922/3000 [============================>.] - ETA: 32s - loss: 0.6437 - regression_loss: 0.4372 - classification_loss: 0.2065
2923/3000 [============================>.] - ETA: 31s - loss: 0.6435 - regression_loss: 0.4371 - classification_loss: 0.2064
2924/3000 [============================>.] - ETA: 31s - loss: 0.6434 - regression_loss: 0.4371 - classification_loss: 0.2064
2925/3000 [============================>.] - ETA: 30s - loss: 0.6434 - regression_loss: 0.4371 - classification_loss: 0.2063
2926/3000 [============================>.] - ETA: 30s - loss: 0.6433 - regression_loss: 0.4370 - classification_loss: 0.2063
2927/3000 [============================>.] - ETA: 29s - loss: 0.6433 - regression_loss: 0.4370 - classification_loss: 0.2062
2928/3000 [============================>.] - ETA: 29s - loss: 0.6432 - regression_loss: 0.4370 - classification_loss: 0.2062
2929/3000 [============================>.] - ETA: 29s - loss: 0.6432 - regression_loss: 0.4370 - classification_loss: 0.2062
2930/3000 [============================>.] - ETA: 28s - loss: 0.6431 - regression_loss: 0.4370 - classification_loss: 0.2061
2931/3000 [============================>.] - ETA: 28s - loss: 0.6431 - regression_loss: 0.4370 - classification_loss: 0.2061
2932/3000 [============================>.] - ETA: 27s - loss: 0.6430 - regression_loss: 0.4370 - classification_loss: 0.2060
2933/3000 [============================>.] - ETA: 27s - loss: 0.6429 - regression_loss: 0.4370 - classification_loss: 0.2059
2934/3000 [============================>.] - ETA: 27s - loss: 0.6429 - regression_loss: 0.4370 - classification_loss: 0.2059
2935/3000 [============================>.] - ETA: 26s - loss: 0.6429 - regression_loss: 0.4370 - classification_loss: 0.2059
2936/3000 [============================>.] - ETA: 26s - loss: 0.6428 - regression_loss: 0.4370 - classification_loss: 0.2058
2937/3000 [============================>.] - ETA: 25s - loss: 0.6428 - regression_loss: 0.4370 - classification_loss: 0.2058
2938/3000 [============================>.] - ETA: 25s - loss: 0.6426 - regression_loss: 0.4369 - classification_loss: 0.2057
2939/3000 [============================>.] - ETA: 25s - loss: 0.6425 - regression_loss: 0.4368 - classification_loss: 0.2057
2940/3000 [============================>.] - ETA: 24s - loss: 0.6425 - regression_loss: 0.4369 - classification_loss: 0.2056
2941/3000 [============================>.] - ETA: 24s - loss: 0.6426 - regression_loss: 0.4369 - classification_loss: 0.2056
2942/3000 [============================>.] - ETA: 23s - loss: 0.6426 - regression_loss: 0.4370 - classification_loss: 0.2056
2943/3000 [============================>.] - ETA: 23s - loss: 0.6426 - regression_loss: 0.4370 - classification_loss: 0.2056
2944/3000 [============================>.] - ETA: 23s - loss: 0.6425 - regression_loss: 0.4370 - classification_loss: 0.2055
2945/3000 [============================>.] - ETA: 22s - loss: 0.6425 - regression_loss: 0.4370 - classification_loss: 0.2055
2946/3000 [============================>.] - ETA: 22s - loss: 0.6424 - regression_loss: 0.4370 - classification_loss: 0.2054
2947/3000 [============================>.] - ETA: 21s - loss: 0.6423 - regression_loss: 0.4369 - classification_loss: 0.2054
2948/3000 [============================>.] - ETA: 21s - loss: 0.6423 - regression_loss: 0.4369 - classification_loss: 0.2053
2949/3000 [============================>.] - ETA: 20s - loss: 0.6422 - regression_loss: 0.4369 - classification_loss: 0.2053
2950/3000 [============================>.] - ETA: 20s - loss: 0.6421 - regression_loss: 0.4369 - classification_loss: 0.2053
2951/3000 [============================>.] - ETA: 20s - loss: 0.6422 - regression_loss: 0.4370 - classification_loss: 0.2052
2952/3000 [============================>.] - ETA: 19s - loss: 0.6421 - regression_loss: 0.4370 - classification_loss: 0.2052
2953/3000 [============================>.] - ETA: 19s - loss: 0.6422 - regression_loss: 0.4370 - classification_loss: 0.2051
2954/3000 [============================>.] - ETA: 18s - loss: 0.6421 - regression_loss: 0.4370 - classification_loss: 0.2051
2955/3000 [============================>.] - ETA: 18s - loss: 0.6420 - regression_loss: 0.4370 - classification_loss: 0.2050
2956/3000 [============================>.] - ETA: 18s - loss: 0.6419 - regression_loss: 0.4369 - classification_loss: 0.2050
2957/3000 [============================>.] - ETA: 17s - loss: 0.6418 - regression_loss: 0.4369 - classification_loss: 0.2049
2958/3000 [============================>.] - ETA: 17s - loss: 0.6417 - regression_loss: 0.4368 - classification_loss: 0.2049
2959/3000 [============================>.] - ETA: 16s - loss: 0.6417 - regression_loss: 0.4369 - classification_loss: 0.2049
2960/3000 [============================>.] - ETA: 16s - loss: 0.6416 - regression_loss: 0.4368 - classification_loss: 0.2048
2961/3000 [============================>.] - ETA: 16s - loss: 0.6415 - regression_loss: 0.4367 - classification_loss: 0.2048
2962/3000 [============================>.] - ETA: 15s - loss: 0.6414 - regression_loss: 0.4367 - classification_loss: 0.2047
2963/3000 [============================>.] - ETA: 15s - loss: 0.6413 - regression_loss: 0.4366 - classification_loss: 0.2047
2964/3000 [============================>.] - ETA: 14s - loss: 0.6413 - regression_loss: 0.4366 - classification_loss: 0.2047
2965/3000 [============================>.] - ETA: 14s - loss: 0.6413 - regression_loss: 0.4367 - classification_loss: 0.2046
2966/3000 [============================>.] - ETA: 13s - loss: 0.6412 - regression_loss: 0.4367 - classification_loss: 0.2046
2967/3000 [============================>.] - ETA: 13s - loss: 0.6412 - regression_loss: 0.4367 - classification_loss: 0.2045
2968/3000 [============================>.] - ETA: 13s - loss: 0.6411 - regression_loss: 0.4367 - classification_loss: 0.2045
2969/3000 [============================>.] - ETA: 12s - loss: 0.6410 - regression_loss: 0.4366 - classification_loss: 0.2044
2970/3000 [============================>.] - ETA: 12s - loss: 0.6409 - regression_loss: 0.4365 - classification_loss: 0.2044
2971/3000 [============================>.] - ETA: 11s - loss: 0.6409 - regression_loss: 0.4365 - classification_loss: 0.2043
2972/3000 [============================>.] - ETA: 11s - loss: 0.6408 - regression_loss: 0.4365 - classification_loss: 0.2043
2973/3000 [============================>.] - ETA: 11s - loss: 0.6407 - regression_loss: 0.4365 - classification_loss: 0.2042
2974/3000 [============================>.] - ETA: 10s - loss: 0.6407 - regression_loss: 0.4365 - classification_loss: 0.2042
2975/3000 [============================>.] - ETA: 10s - loss: 0.6407 - regression_loss: 0.4365 - classification_loss: 0.2042
2976/3000 [============================>.] - ETA: 9s - loss: 0.6407 - regression_loss: 0.4365 - classification_loss: 0.2042 
2977/3000 [============================>.] - ETA: 9s - loss: 0.6406 - regression_loss: 0.4365 - classification_loss: 0.2041
2978/3000 [============================>.] - ETA: 9s - loss: 0.6405 - regression_loss: 0.4365 - classification_loss: 0.2041
2979/3000 [============================>.] - ETA: 8s - loss: 0.6405 - regression_loss: 0.4364 - classification_loss: 0.2040
2980/3000 [============================>.] - ETA: 8s - loss: 0.6403 - regression_loss: 0.4364 - classification_loss: 0.2040
2981/3000 [============================>.] - ETA: 7s - loss: 0.6402 - regression_loss: 0.4363 - classification_loss: 0.2039
2982/3000 [============================>.] - ETA: 7s - loss: 0.6401 - regression_loss: 0.4362 - classification_loss: 0.2039
2983/3000 [============================>.] - ETA: 6s - loss: 0.6401 - regression_loss: 0.4362 - classification_loss: 0.2038
2984/3000 [============================>.] - ETA: 6s - loss: 0.6401 - regression_loss: 0.4363 - classification_loss: 0.2038
2985/3000 [============================>.] - ETA: 6s - loss: 0.6400 - regression_loss: 0.4362 - classification_loss: 0.2037
2986/3000 [============================>.] - ETA: 5s - loss: 0.6399 - regression_loss: 0.4362 - classification_loss: 0.2037
2987/3000 [============================>.] - ETA: 5s - loss: 0.6398 - regression_loss: 0.4361 - classification_loss: 0.2037
2988/3000 [============================>.] - ETA: 4s - loss: 0.6397 - regression_loss: 0.4361 - classification_loss: 0.2036
2989/3000 [============================>.] - ETA: 4s - loss: 0.6396 - regression_loss: 0.4360 - classification_loss: 0.2036
2990/3000 [============================>.] - ETA: 4s - loss: 0.6395 - regression_loss: 0.4360 - classification_loss: 0.2035
2991/3000 [============================>.] - ETA: 3s - loss: 0.6394 - regression_loss: 0.4359 - classification_loss: 0.2035
2992/3000 [============================>.] - ETA: 3s - loss: 0.6392 - regression_loss: 0.4358 - classification_loss: 0.2034
2993/3000 [============================>.] - ETA: 2s - loss: 0.6392 - regression_loss: 0.4358 - classification_loss: 0.2034
2994/3000 [============================>.] - ETA: 2s - loss: 0.6392 - regression_loss: 0.4359 - classification_loss: 0.2033
2995/3000 [============================>.] - ETA: 2s - loss: 0.6391 - regression_loss: 0.4359 - classification_loss: 0.2032
2996/3000 [============================>.] - ETA: 1s - loss: 0.6390 - regression_loss: 0.4358 - classification_loss: 0.2032
2997/3000 [============================>.] - ETA: 1s - loss: 0.6390 - regression_loss: 0.4358 - classification_loss: 0.2032
2998/3000 [============================>.] - ETA: 0s - loss: 0.6389 - regression_loss: 0.4358 - classification_loss: 0.2031
2999/3000 [============================>.] - ETA: 0s - loss: 0.6388 - regression_loss: 0.4357 - classification_loss: 0.2031Epoch 00002: saving model to ./snapshots/resnet50_csv_02.h5

3000/3000 [==============================] - 1234s 411ms/step - loss: 0.6387 - regression_loss: 0.4357 - classification_loss: 0.2030
Epoch 3/50

   1/3000 [..............................] - ETA: 20:15 - loss: 0.3906 - regression_loss: 0.3300 - classification_loss: 0.0606
   2/3000 [..............................] - ETA: 20:06 - loss: 0.4570 - regression_loss: 0.3984 - classification_loss: 0.0587
   3/3000 [..............................] - ETA: 20:06 - loss: 0.5135 - regression_loss: 0.4133 - classification_loss: 0.1002
   4/3000 [..............................] - ETA: 20:03 - loss: 0.5021 - regression_loss: 0.4062 - classification_loss: 0.0959
   5/3000 [..............................] - ETA: 20:01 - loss: 0.4764 - regression_loss: 0.3795 - classification_loss: 0.0969
   6/3000 [..............................] - ETA: 19:47 - loss: 0.4264 - regression_loss: 0.3399 - classification_loss: 0.0866
   7/3000 [..............................] - ETA: 19:40 - loss: 0.4188 - regression_loss: 0.3350 - classification_loss: 0.0838
   8/3000 [..............................] - ETA: 19:46 - loss: 0.4484 - regression_loss: 0.3660 - classification_loss: 0.0824
   9/3000 [..............................] - ETA: 19:46 - loss: 0.4235 - regression_loss: 0.3438 - classification_loss: 0.0797
  10/3000 [..............................] - ETA: 19:44 - loss: 0.4284 - regression_loss: 0.3517 - classification_loss: 0.0767
  11/3000 [..............................] - ETA: 19:47 - loss: 0.4158 - regression_loss: 0.3418 - classification_loss: 0.0740
  12/3000 [..............................] - ETA: 19:45 - loss: 0.4217 - regression_loss: 0.3477 - classification_loss: 0.0740
  13/3000 [..............................] - ETA: 19:45 - loss: 0.4406 - regression_loss: 0.3635 - classification_loss: 0.0771
  14/3000 [..............................] - ETA: 19:48 - loss: 0.4376 - regression_loss: 0.3596 - classification_loss: 0.0779
  15/3000 [..............................] - ETA: 19:52 - loss: 0.4448 - regression_loss: 0.3664 - classification_loss: 0.0784
  16/3000 [..............................] - ETA: 19:55 - loss: 0.4443 - regression_loss: 0.3651 - classification_loss: 0.0792
  17/3000 [..............................] - ETA: 19:56 - loss: 0.4362 - regression_loss: 0.3576 - classification_loss: 0.0786
  18/3000 [..............................] - ETA: 19:58 - loss: 0.4407 - regression_loss: 0.3625 - classification_loss: 0.0782
  19/3000 [..............................] - ETA: 20:00 - loss: 0.4370 - regression_loss: 0.3608 - classification_loss: 0.0762
  20/3000 [..............................] - ETA: 19:59 - loss: 0.4397 - regression_loss: 0.3629 - classification_loss: 0.0769
  21/3000 [..............................] - ETA: 20:01 - loss: 0.4303 - regression_loss: 0.3542 - classification_loss: 0.0761
  22/3000 [..............................] - ETA: 20:02 - loss: 0.4412 - regression_loss: 0.3646 - classification_loss: 0.0766
  23/3000 [..............................] - ETA: 20:03 - loss: 0.4348 - regression_loss: 0.3585 - classification_loss: 0.0763
  24/3000 [..............................] - ETA: 20:03 - loss: 0.4406 - regression_loss: 0.3612 - classification_loss: 0.0794
  25/3000 [..............................] - ETA: 20:05 - loss: 0.4399 - regression_loss: 0.3597 - classification_loss: 0.0802
  26/3000 [..............................] - ETA: 20:06 - loss: 0.4397 - regression_loss: 0.3588 - classification_loss: 0.0809
  27/3000 [..............................] - ETA: 20:05 - loss: 0.4394 - regression_loss: 0.3593 - classification_loss: 0.0801
  28/3000 [..............................] - ETA: 20:04 - loss: 0.4388 - regression_loss: 0.3585 - classification_loss: 0.0804
  29/3000 [..............................] - ETA: 20:01 - loss: 0.4365 - regression_loss: 0.3521 - classification_loss: 0.0844
  30/3000 [..............................] - ETA: 20:01 - loss: 0.4283 - regression_loss: 0.3459 - classification_loss: 0.0824
  31/3000 [..............................] - ETA: 20:00 - loss: 0.4255 - regression_loss: 0.3437 - classification_loss: 0.0818
  32/3000 [..............................] - ETA: 20:00 - loss: 0.4351 - regression_loss: 0.3525 - classification_loss: 0.0826
  33/3000 [..............................] - ETA: 20:00 - loss: 0.4311 - regression_loss: 0.3490 - classification_loss: 0.0821
  34/3000 [..............................] - ETA: 20:00 - loss: 0.4346 - regression_loss: 0.3509 - classification_loss: 0.0837
  35/3000 [..............................] - ETA: 20:00 - loss: 0.4312 - regression_loss: 0.3482 - classification_loss: 0.0830
  36/3000 [..............................] - ETA: 20:01 - loss: 0.4329 - regression_loss: 0.3507 - classification_loss: 0.0821
  37/3000 [..............................] - ETA: 20:01 - loss: 0.4268 - regression_loss: 0.3461 - classification_loss: 0.0807
  38/3000 [..............................] - ETA: 20:01 - loss: 0.4204 - regression_loss: 0.3411 - classification_loss: 0.0793
  39/3000 [..............................] - ETA: 20:00 - loss: 0.4230 - regression_loss: 0.3437 - classification_loss: 0.0793
  40/3000 [..............................] - ETA: 19:59 - loss: 0.4222 - regression_loss: 0.3418 - classification_loss: 0.0805
  41/3000 [..............................] - ETA: 20:00 - loss: 0.4246 - regression_loss: 0.3441 - classification_loss: 0.0805
  42/3000 [..............................] - ETA: 20:01 - loss: 0.4210 - regression_loss: 0.3409 - classification_loss: 0.0801
  43/3000 [..............................] - ETA: 20:00 - loss: 0.4154 - regression_loss: 0.3358 - classification_loss: 0.0795
  44/3000 [..............................] - ETA: 20:00 - loss: 0.4137 - regression_loss: 0.3344 - classification_loss: 0.0793
  45/3000 [..............................] - ETA: 20:00 - loss: 0.4151 - regression_loss: 0.3360 - classification_loss: 0.0791
  46/3000 [..............................] - ETA: 20:00 - loss: 0.4147 - regression_loss: 0.3364 - classification_loss: 0.0782
  47/3000 [..............................] - ETA: 20:01 - loss: 0.4104 - regression_loss: 0.3324 - classification_loss: 0.0780
  48/3000 [..............................] - ETA: 20:00 - loss: 0.4085 - regression_loss: 0.3311 - classification_loss: 0.0774
  49/3000 [..............................] - ETA: 20:00 - loss: 0.4071 - regression_loss: 0.3291 - classification_loss: 0.0781
  50/3000 [..............................] - ETA: 20:00 - loss: 0.4052 - regression_loss: 0.3282 - classification_loss: 0.0770
  51/3000 [..............................] - ETA: 19:59 - loss: 0.4020 - regression_loss: 0.3252 - classification_loss: 0.0769
  52/3000 [..............................] - ETA: 19:58 - loss: 0.3985 - regression_loss: 0.3220 - classification_loss: 0.0766
  53/3000 [..............................] - ETA: 19:57 - loss: 0.4001 - regression_loss: 0.3223 - classification_loss: 0.0779
  54/3000 [..............................] - ETA: 19:57 - loss: 0.3998 - regression_loss: 0.3214 - classification_loss: 0.0784
  55/3000 [..............................] - ETA: 19:57 - loss: 0.4029 - regression_loss: 0.3224 - classification_loss: 0.0804
  56/3000 [..............................] - ETA: 19:57 - loss: 0.4038 - regression_loss: 0.3241 - classification_loss: 0.0797
  57/3000 [..............................] - ETA: 19:57 - loss: 0.4034 - regression_loss: 0.3240 - classification_loss: 0.0793
  58/3000 [..............................] - ETA: 19:57 - loss: 0.4050 - regression_loss: 0.3262 - classification_loss: 0.0788
  59/3000 [..............................] - ETA: 19:57 - loss: 0.4038 - regression_loss: 0.3254 - classification_loss: 0.0784
  60/3000 [..............................] - ETA: 19:57 - loss: 0.4072 - regression_loss: 0.3292 - classification_loss: 0.0780
  61/3000 [..............................] - ETA: 19:57 - loss: 0.4150 - regression_loss: 0.3359 - classification_loss: 0.0791
  62/3000 [..............................] - ETA: 19:58 - loss: 0.4134 - regression_loss: 0.3346 - classification_loss: 0.0788
  63/3000 [..............................] - ETA: 19:58 - loss: 0.4124 - regression_loss: 0.3344 - classification_loss: 0.0781
  64/3000 [..............................] - ETA: 19:58 - loss: 0.4124 - regression_loss: 0.3337 - classification_loss: 0.0788
  65/3000 [..............................] - ETA: 19:58 - loss: 0.4125 - regression_loss: 0.3334 - classification_loss: 0.0790
  66/3000 [..............................] - ETA: 19:58 - loss: 0.4124 - regression_loss: 0.3332 - classification_loss: 0.0792
  67/3000 [..............................] - ETA: 19:58 - loss: 0.4124 - regression_loss: 0.3332 - classification_loss: 0.0792
  68/3000 [..............................] - ETA: 19:58 - loss: 0.4114 - regression_loss: 0.3318 - classification_loss: 0.0796
  69/3000 [..............................] - ETA: 19:58 - loss: 0.4107 - regression_loss: 0.3308 - classification_loss: 0.0798
  70/3000 [..............................] - ETA: 19:58 - loss: 0.4161 - regression_loss: 0.3356 - classification_loss: 0.0805
  71/3000 [..............................] - ETA: 19:58 - loss: 0.4146 - regression_loss: 0.3342 - classification_loss: 0.0804
  72/3000 [..............................] - ETA: 19:59 - loss: 0.4158 - regression_loss: 0.3360 - classification_loss: 0.0799
  73/3000 [..............................] - ETA: 19:59 - loss: 0.4151 - regression_loss: 0.3355 - classification_loss: 0.0795
  74/3000 [..............................] - ETA: 19:59 - loss: 0.4151 - regression_loss: 0.3358 - classification_loss: 0.0793
  75/3000 [..............................] - ETA: 19:58 - loss: 0.4139 - regression_loss: 0.3344 - classification_loss: 0.0795
  76/3000 [..............................] - ETA: 19:57 - loss: 0.4139 - regression_loss: 0.3341 - classification_loss: 0.0798
  77/3000 [..............................] - ETA: 19:57 - loss: 0.4186 - regression_loss: 0.3388 - classification_loss: 0.0798
  78/3000 [..............................] - ETA: 19:57 - loss: 0.4201 - regression_loss: 0.3402 - classification_loss: 0.0799
  79/3000 [..............................] - ETA: 19:56 - loss: 0.4218 - regression_loss: 0.3419 - classification_loss: 0.0799
  80/3000 [..............................] - ETA: 19:56 - loss: 0.4207 - regression_loss: 0.3411 - classification_loss: 0.0796
  81/3000 [..............................] - ETA: 19:55 - loss: 0.4200 - regression_loss: 0.3409 - classification_loss: 0.0791
  82/3000 [..............................] - ETA: 19:55 - loss: 0.4231 - regression_loss: 0.3421 - classification_loss: 0.0811
  83/3000 [..............................] - ETA: 19:55 - loss: 0.4224 - regression_loss: 0.3417 - classification_loss: 0.0807
  84/3000 [..............................] - ETA: 19:54 - loss: 0.4235 - regression_loss: 0.3430 - classification_loss: 0.0805
  85/3000 [..............................] - ETA: 19:54 - loss: 0.4239 - regression_loss: 0.3426 - classification_loss: 0.0813
  86/3000 [..............................] - ETA: 19:54 - loss: 0.4262 - regression_loss: 0.3450 - classification_loss: 0.0812
  87/3000 [..............................] - ETA: 19:54 - loss: 0.4263 - regression_loss: 0.3454 - classification_loss: 0.0809
  88/3000 [..............................] - ETA: 19:53 - loss: 0.4254 - regression_loss: 0.3451 - classification_loss: 0.0803
  89/3000 [..............................] - ETA: 19:53 - loss: 0.4247 - regression_loss: 0.3449 - classification_loss: 0.0798
  90/3000 [..............................] - ETA: 19:52 - loss: 0.4296 - regression_loss: 0.3501 - classification_loss: 0.0795
  91/3000 [..............................] - ETA: 19:52 - loss: 0.4311 - regression_loss: 0.3514 - classification_loss: 0.0797
  92/3000 [..............................] - ETA: 19:51 - loss: 0.4304 - regression_loss: 0.3512 - classification_loss: 0.0792
  93/3000 [..............................] - ETA: 19:50 - loss: 0.4310 - regression_loss: 0.3520 - classification_loss: 0.0790
  94/3000 [..............................] - ETA: 19:50 - loss: 0.4305 - regression_loss: 0.3516 - classification_loss: 0.0789
  95/3000 [..............................] - ETA: 19:49 - loss: 0.4305 - regression_loss: 0.3513 - classification_loss: 0.0792
  96/3000 [..............................] - ETA: 19:49 - loss: 0.4296 - regression_loss: 0.3508 - classification_loss: 0.0788
  97/3000 [..............................] - ETA: 19:48 - loss: 0.4274 - regression_loss: 0.3488 - classification_loss: 0.0786
  98/3000 [..............................] - ETA: 19:48 - loss: 0.4267 - regression_loss: 0.3481 - classification_loss: 0.0786
  99/3000 [..............................] - ETA: 19:48 - loss: 0.4268 - regression_loss: 0.3482 - classification_loss: 0.0786
 100/3000 [>.............................] - ETA: 19:47 - loss: 0.4258 - regression_loss: 0.3476 - classification_loss: 0.0782
 101/3000 [>.............................] - ETA: 19:47 - loss: 0.4254 - regression_loss: 0.3469 - classification_loss: 0.0785
 102/3000 [>.............................] - ETA: 19:46 - loss: 0.4249 - regression_loss: 0.3467 - classification_loss: 0.0783
 103/3000 [>.............................] - ETA: 19:46 - loss: 0.4248 - regression_loss: 0.3455 - classification_loss: 0.0792
 104/3000 [>.............................] - ETA: 19:46 - loss: 0.4243 - regression_loss: 0.3446 - classification_loss: 0.0796
 105/3000 [>.............................] - ETA: 19:46 - loss: 0.4231 - regression_loss: 0.3432 - classification_loss: 0.0799
 106/3000 [>.............................] - ETA: 19:45 - loss: 0.4257 - regression_loss: 0.3458 - classification_loss: 0.0799
 107/3000 [>.............................] - ETA: 19:45 - loss: 0.4253 - regression_loss: 0.3454 - classification_loss: 0.0799
 108/3000 [>.............................] - ETA: 19:45 - loss: 0.4270 - regression_loss: 0.3474 - classification_loss: 0.0796
 109/3000 [>.............................] - ETA: 19:44 - loss: 0.4268 - regression_loss: 0.3476 - classification_loss: 0.0792
 110/3000 [>.............................] - ETA: 19:44 - loss: 0.4252 - regression_loss: 0.3465 - classification_loss: 0.0788
 111/3000 [>.............................] - ETA: 19:44 - loss: 0.4268 - regression_loss: 0.3481 - classification_loss: 0.0787
 112/3000 [>.............................] - ETA: 19:44 - loss: 0.4249 - regression_loss: 0.3464 - classification_loss: 0.0785
 113/3000 [>.............................] - ETA: 19:43 - loss: 0.4266 - regression_loss: 0.3483 - classification_loss: 0.0783
 114/3000 [>.............................] - ETA: 19:43 - loss: 0.4282 - regression_loss: 0.3499 - classification_loss: 0.0783
 115/3000 [>.............................] - ETA: 19:42 - loss: 0.4279 - regression_loss: 0.3495 - classification_loss: 0.0784
 116/3000 [>.............................] - ETA: 19:42 - loss: 0.4268 - regression_loss: 0.3485 - classification_loss: 0.0783
 117/3000 [>.............................] - ETA: 19:42 - loss: 0.4267 - regression_loss: 0.3486 - classification_loss: 0.0781
 118/3000 [>.............................] - ETA: 19:41 - loss: 0.4256 - regression_loss: 0.3478 - classification_loss: 0.0779
 119/3000 [>.............................] - ETA: 19:41 - loss: 0.4254 - regression_loss: 0.3476 - classification_loss: 0.0778
 120/3000 [>.............................] - ETA: 19:40 - loss: 0.4250 - regression_loss: 0.3476 - classification_loss: 0.0774
 121/3000 [>.............................] - ETA: 19:40 - loss: 0.4271 - regression_loss: 0.3490 - classification_loss: 0.0782
 122/3000 [>.............................] - ETA: 19:39 - loss: 0.4257 - regression_loss: 0.3480 - classification_loss: 0.0778
 123/3000 [>.............................] - ETA: 19:38 - loss: 0.4246 - regression_loss: 0.3468 - classification_loss: 0.0778
 124/3000 [>.............................] - ETA: 19:37 - loss: 0.4237 - regression_loss: 0.3457 - classification_loss: 0.0780
 125/3000 [>.............................] - ETA: 19:37 - loss: 0.4221 - regression_loss: 0.3442 - classification_loss: 0.0779
 126/3000 [>.............................] - ETA: 19:37 - loss: 0.4221 - regression_loss: 0.3444 - classification_loss: 0.0777
 127/3000 [>.............................] - ETA: 19:37 - loss: 0.4208 - regression_loss: 0.3435 - classification_loss: 0.0773
 128/3000 [>.............................] - ETA: 19:37 - loss: 0.4220 - regression_loss: 0.3445 - classification_loss: 0.0775
 129/3000 [>.............................] - ETA: 19:37 - loss: 0.4239 - regression_loss: 0.3460 - classification_loss: 0.0780
 130/3000 [>.............................] - ETA: 19:36 - loss: 0.4227 - regression_loss: 0.3450 - classification_loss: 0.0777
 131/3000 [>.............................] - ETA: 19:36 - loss: 0.4217 - regression_loss: 0.3442 - classification_loss: 0.0775
 132/3000 [>.............................] - ETA: 19:36 - loss: 0.4211 - regression_loss: 0.3434 - classification_loss: 0.0777
 133/3000 [>.............................] - ETA: 19:35 - loss: 0.4195 - regression_loss: 0.3421 - classification_loss: 0.0774
 134/3000 [>.............................] - ETA: 19:35 - loss: 0.4190 - regression_loss: 0.3420 - classification_loss: 0.0771
 135/3000 [>.............................] - ETA: 19:35 - loss: 0.4182 - regression_loss: 0.3413 - classification_loss: 0.0769
 136/3000 [>.............................] - ETA: 19:35 - loss: 0.4184 - regression_loss: 0.3409 - classification_loss: 0.0775
 137/3000 [>.............................] - ETA: 19:34 - loss: 0.4178 - regression_loss: 0.3399 - classification_loss: 0.0779
 138/3000 [>.............................] - ETA: 19:34 - loss: 0.4165 - regression_loss: 0.3390 - classification_loss: 0.0775
 139/3000 [>.............................] - ETA: 19:33 - loss: 0.4156 - regression_loss: 0.3384 - classification_loss: 0.0771
 140/3000 [>.............................] - ETA: 19:33 - loss: 0.4150 - regression_loss: 0.3375 - classification_loss: 0.0775
 141/3000 [>.............................] - ETA: 19:33 - loss: 0.4149 - regression_loss: 0.3376 - classification_loss: 0.0773
 142/3000 [>.............................] - ETA: 19:32 - loss: 0.4149 - regression_loss: 0.3377 - classification_loss: 0.0772
 143/3000 [>.............................] - ETA: 19:32 - loss: 0.4151 - regression_loss: 0.3376 - classification_loss: 0.0774
 144/3000 [>.............................] - ETA: 19:32 - loss: 0.4159 - regression_loss: 0.3379 - classification_loss: 0.0780
 145/3000 [>.............................] - ETA: 19:31 - loss: 0.4180 - regression_loss: 0.3397 - classification_loss: 0.0783
 146/3000 [>.............................] - ETA: 19:31 - loss: 0.4181 - regression_loss: 0.3399 - classification_loss: 0.0782
 147/3000 [>.............................] - ETA: 19:30 - loss: 0.4197 - regression_loss: 0.3409 - classification_loss: 0.0789
 148/3000 [>.............................] - ETA: 19:30 - loss: 0.4200 - regression_loss: 0.3413 - classification_loss: 0.0786
 149/3000 [>.............................] - ETA: 19:29 - loss: 0.4215 - regression_loss: 0.3428 - classification_loss: 0.0787
 150/3000 [>.............................] - ETA: 19:29 - loss: 0.4221 - regression_loss: 0.3435 - classification_loss: 0.0786
 151/3000 [>.............................] - ETA: 19:29 - loss: 0.4215 - regression_loss: 0.3432 - classification_loss: 0.0783
 152/3000 [>.............................] - ETA: 19:29 - loss: 0.4213 - regression_loss: 0.3432 - classification_loss: 0.0781
 153/3000 [>.............................] - ETA: 19:28 - loss: 0.4217 - regression_loss: 0.3437 - classification_loss: 0.0780
 154/3000 [>.............................] - ETA: 19:28 - loss: 0.4210 - regression_loss: 0.3433 - classification_loss: 0.0777
 155/3000 [>.............................] - ETA: 19:28 - loss: 0.4203 - regression_loss: 0.3425 - classification_loss: 0.0778
 156/3000 [>.............................] - ETA: 19:27 - loss: 0.4196 - regression_loss: 0.3419 - classification_loss: 0.0777
 157/3000 [>.............................] - ETA: 19:27 - loss: 0.4208 - regression_loss: 0.3431 - classification_loss: 0.0777
 158/3000 [>.............................] - ETA: 19:27 - loss: 0.4201 - regression_loss: 0.3425 - classification_loss: 0.0776
 159/3000 [>.............................] - ETA: 19:26 - loss: 0.4191 - regression_loss: 0.3416 - classification_loss: 0.0774
 160/3000 [>.............................] - ETA: 19:26 - loss: 0.4207 - regression_loss: 0.3428 - classification_loss: 0.0779
 161/3000 [>.............................] - ETA: 19:26 - loss: 0.4194 - regression_loss: 0.3418 - classification_loss: 0.0776
 162/3000 [>.............................] - ETA: 19:25 - loss: 0.4187 - regression_loss: 0.3412 - classification_loss: 0.0775
 163/3000 [>.............................] - ETA: 19:25 - loss: 0.4177 - regression_loss: 0.3404 - classification_loss: 0.0773
 164/3000 [>.............................] - ETA: 19:24 - loss: 0.4172 - regression_loss: 0.3397 - classification_loss: 0.0775
 165/3000 [>.............................] - ETA: 19:24 - loss: 0.4178 - regression_loss: 0.3402 - classification_loss: 0.0776
 166/3000 [>.............................] - ETA: 19:23 - loss: 0.4169 - regression_loss: 0.3393 - classification_loss: 0.0776
 167/3000 [>.............................] - ETA: 19:23 - loss: 0.4159 - regression_loss: 0.3385 - classification_loss: 0.0774
 168/3000 [>.............................] - ETA: 19:23 - loss: 0.4151 - regression_loss: 0.3379 - classification_loss: 0.0772
 169/3000 [>.............................] - ETA: 19:23 - loss: 0.4141 - regression_loss: 0.3369 - classification_loss: 0.0772
 170/3000 [>.............................] - ETA: 19:22 - loss: 0.4141 - regression_loss: 0.3372 - classification_loss: 0.0770
 171/3000 [>.............................] - ETA: 19:22 - loss: 0.4145 - regression_loss: 0.3373 - classification_loss: 0.0772
 172/3000 [>.............................] - ETA: 19:21 - loss: 0.4142 - regression_loss: 0.3371 - classification_loss: 0.0770
 173/3000 [>.............................] - ETA: 19:21 - loss: 0.4134 - regression_loss: 0.3364 - classification_loss: 0.0770
 174/3000 [>.............................] - ETA: 19:20 - loss: 0.4153 - regression_loss: 0.3381 - classification_loss: 0.0772
 175/3000 [>.............................] - ETA: 19:20 - loss: 0.4158 - regression_loss: 0.3383 - classification_loss: 0.0775
 176/3000 [>.............................] - ETA: 19:20 - loss: 0.4169 - regression_loss: 0.3394 - classification_loss: 0.0775
 177/3000 [>.............................] - ETA: 19:19 - loss: 0.4164 - regression_loss: 0.3390 - classification_loss: 0.0774
 178/3000 [>.............................] - ETA: 19:18 - loss: 0.4154 - regression_loss: 0.3381 - classification_loss: 0.0773
 179/3000 [>.............................] - ETA: 19:18 - loss: 0.4163 - regression_loss: 0.3389 - classification_loss: 0.0774
 180/3000 [>.............................] - ETA: 19:17 - loss: 0.4164 - regression_loss: 0.3389 - classification_loss: 0.0774
 181/3000 [>.............................] - ETA: 19:17 - loss: 0.4154 - regression_loss: 0.3380 - classification_loss: 0.0774
 182/3000 [>.............................] - ETA: 19:16 - loss: 0.4149 - regression_loss: 0.3376 - classification_loss: 0.0773
 183/3000 [>.............................] - ETA: 19:16 - loss: 0.4156 - regression_loss: 0.3384 - classification_loss: 0.0773
 184/3000 [>.............................] - ETA: 19:16 - loss: 0.4151 - regression_loss: 0.3379 - classification_loss: 0.0772
 185/3000 [>.............................] - ETA: 19:15 - loss: 0.4140 - regression_loss: 0.3370 - classification_loss: 0.0770
 186/3000 [>.............................] - ETA: 19:15 - loss: 0.4151 - regression_loss: 0.3379 - classification_loss: 0.0771
 187/3000 [>.............................] - ETA: 19:15 - loss: 0.4153 - regression_loss: 0.3384 - classification_loss: 0.0769
 188/3000 [>.............................] - ETA: 19:14 - loss: 0.4152 - regression_loss: 0.3386 - classification_loss: 0.0767
 189/3000 [>.............................] - ETA: 19:14 - loss: 0.4153 - regression_loss: 0.3388 - classification_loss: 0.0765
 190/3000 [>.............................] - ETA: 19:14 - loss: 0.4159 - regression_loss: 0.3396 - classification_loss: 0.0763
 191/3000 [>.............................] - ETA: 19:13 - loss: 0.4151 - regression_loss: 0.3388 - classification_loss: 0.0763
 192/3000 [>.............................] - ETA: 19:13 - loss: 0.4146 - regression_loss: 0.3383 - classification_loss: 0.0762
 193/3000 [>.............................] - ETA: 19:12 - loss: 0.4149 - regression_loss: 0.3388 - classification_loss: 0.0761
 194/3000 [>.............................] - ETA: 19:12 - loss: 0.4152 - regression_loss: 0.3391 - classification_loss: 0.0761
 195/3000 [>.............................] - ETA: 19:12 - loss: 0.4147 - regression_loss: 0.3385 - classification_loss: 0.0762
 196/3000 [>.............................] - ETA: 19:11 - loss: 0.4168 - regression_loss: 0.3403 - classification_loss: 0.0765
 197/3000 [>.............................] - ETA: 19:11 - loss: 0.4164 - regression_loss: 0.3400 - classification_loss: 0.0765
 198/3000 [>.............................] - ETA: 19:11 - loss: 0.4174 - regression_loss: 0.3411 - classification_loss: 0.0763
 199/3000 [>.............................] - ETA: 19:10 - loss: 0.4173 - regression_loss: 0.3410 - classification_loss: 0.0763
 200/3000 [=>............................] - ETA: 19:09 - loss: 0.4168 - regression_loss: 0.3406 - classification_loss: 0.0762
 201/3000 [=>............................] - ETA: 19:09 - loss: 0.4176 - regression_loss: 0.3416 - classification_loss: 0.0761
 202/3000 [=>............................] - ETA: 19:08 - loss: 0.4168 - regression_loss: 0.3406 - classification_loss: 0.0762
 203/3000 [=>............................] - ETA: 19:08 - loss: 0.4175 - regression_loss: 0.3412 - classification_loss: 0.0762
 204/3000 [=>............................] - ETA: 19:08 - loss: 0.4173 - regression_loss: 0.3412 - classification_loss: 0.0762
 205/3000 [=>............................] - ETA: 19:08 - loss: 0.4172 - regression_loss: 0.3412 - classification_loss: 0.0760
 206/3000 [=>............................] - ETA: 19:07 - loss: 0.4173 - regression_loss: 0.3414 - classification_loss: 0.0758
 207/3000 [=>............................] - ETA: 19:07 - loss: 0.4169 - regression_loss: 0.3413 - classification_loss: 0.0756
 208/3000 [=>............................] - ETA: 19:06 - loss: 0.4175 - regression_loss: 0.3418 - classification_loss: 0.0757
 209/3000 [=>............................] - ETA: 19:06 - loss: 0.4179 - regression_loss: 0.3422 - classification_loss: 0.0757
 210/3000 [=>............................] - ETA: 19:06 - loss: 0.4178 - regression_loss: 0.3420 - classification_loss: 0.0758
 211/3000 [=>............................] - ETA: 19:05 - loss: 0.4171 - regression_loss: 0.3413 - classification_loss: 0.0758
 212/3000 [=>............................] - ETA: 19:05 - loss: 0.4183 - regression_loss: 0.3424 - classification_loss: 0.0759
 213/3000 [=>............................] - ETA: 19:04 - loss: 0.4171 - regression_loss: 0.3414 - classification_loss: 0.0756
 214/3000 [=>............................] - ETA: 19:04 - loss: 0.4171 - regression_loss: 0.3415 - classification_loss: 0.0755
 215/3000 [=>............................] - ETA: 19:03 - loss: 0.4161 - regression_loss: 0.3408 - classification_loss: 0.0753
 216/3000 [=>............................] - ETA: 19:03 - loss: 0.4149 - regression_loss: 0.3399 - classification_loss: 0.0750
 217/3000 [=>............................] - ETA: 19:03 - loss: 0.4149 - regression_loss: 0.3397 - classification_loss: 0.0752
 218/3000 [=>............................] - ETA: 19:02 - loss: 0.4161 - regression_loss: 0.3407 - classification_loss: 0.0754
 219/3000 [=>............................] - ETA: 19:01 - loss: 0.4157 - regression_loss: 0.3405 - classification_loss: 0.0752
 220/3000 [=>............................] - ETA: 19:01 - loss: 0.4149 - regression_loss: 0.3397 - classification_loss: 0.0752
 221/3000 [=>............................] - ETA: 19:01 - loss: 0.4156 - regression_loss: 0.3404 - classification_loss: 0.0752
 222/3000 [=>............................] - ETA: 19:00 - loss: 0.4150 - regression_loss: 0.3400 - classification_loss: 0.0750
 223/3000 [=>............................] - ETA: 19:00 - loss: 0.4146 - regression_loss: 0.3397 - classification_loss: 0.0749
 224/3000 [=>............................] - ETA: 19:00 - loss: 0.4144 - regression_loss: 0.3395 - classification_loss: 0.0749
 225/3000 [=>............................] - ETA: 18:59 - loss: 0.4138 - regression_loss: 0.3389 - classification_loss: 0.0749
 226/3000 [=>............................] - ETA: 18:59 - loss: 0.4138 - regression_loss: 0.3385 - classification_loss: 0.0753
 227/3000 [=>............................] - ETA: 18:59 - loss: 0.4130 - regression_loss: 0.3379 - classification_loss: 0.0751
 228/3000 [=>............................] - ETA: 18:58 - loss: 0.4138 - regression_loss: 0.3386 - classification_loss: 0.0752
 229/3000 [=>............................] - ETA: 18:58 - loss: 0.4130 - regression_loss: 0.3380 - classification_loss: 0.0750
 230/3000 [=>............................] - ETA: 18:58 - loss: 0.4130 - regression_loss: 0.3377 - classification_loss: 0.0753
 231/3000 [=>............................] - ETA: 18:58 - loss: 0.4125 - regression_loss: 0.3373 - classification_loss: 0.0752
 232/3000 [=>............................] - ETA: 18:57 - loss: 0.4124 - regression_loss: 0.3373 - classification_loss: 0.0751
 233/3000 [=>............................] - ETA: 18:57 - loss: 0.4118 - regression_loss: 0.3366 - classification_loss: 0.0751
 234/3000 [=>............................] - ETA: 18:56 - loss: 0.4110 - regression_loss: 0.3358 - classification_loss: 0.0751
 235/3000 [=>............................] - ETA: 18:56 - loss: 0.4104 - regression_loss: 0.3354 - classification_loss: 0.0750
 236/3000 [=>............................] - ETA: 18:56 - loss: 0.4100 - regression_loss: 0.3351 - classification_loss: 0.0750
 237/3000 [=>............................] - ETA: 18:55 - loss: 0.4103 - regression_loss: 0.3355 - classification_loss: 0.0749
 238/3000 [=>............................] - ETA: 18:55 - loss: 0.4103 - regression_loss: 0.3355 - classification_loss: 0.0748
 239/3000 [=>............................] - ETA: 18:54 - loss: 0.4102 - regression_loss: 0.3355 - classification_loss: 0.0747
 240/3000 [=>............................] - ETA: 18:53 - loss: 0.4104 - regression_loss: 0.3356 - classification_loss: 0.0748
 241/3000 [=>............................] - ETA: 18:53 - loss: 0.4102 - regression_loss: 0.3357 - classification_loss: 0.0746
 242/3000 [=>............................] - ETA: 18:53 - loss: 0.4104 - regression_loss: 0.3357 - classification_loss: 0.0747
 243/3000 [=>............................] - ETA: 18:52 - loss: 0.4099 - regression_loss: 0.3353 - classification_loss: 0.0746
 244/3000 [=>............................] - ETA: 18:52 - loss: 0.4111 - regression_loss: 0.3364 - classification_loss: 0.0747
 245/3000 [=>............................] - ETA: 18:51 - loss: 0.4111 - regression_loss: 0.3360 - classification_loss: 0.0751
 246/3000 [=>............................] - ETA: 18:51 - loss: 0.4136 - regression_loss: 0.3384 - classification_loss: 0.0752
 247/3000 [=>............................] - ETA: 18:51 - loss: 0.4133 - regression_loss: 0.3383 - classification_loss: 0.0750
 248/3000 [=>............................] - ETA: 18:51 - loss: 0.4135 - regression_loss: 0.3386 - classification_loss: 0.0749
 249/3000 [=>............................] - ETA: 18:50 - loss: 0.4129 - regression_loss: 0.3380 - classification_loss: 0.0749
 250/3000 [=>............................] - ETA: 18:50 - loss: 0.4130 - regression_loss: 0.3382 - classification_loss: 0.0748
 251/3000 [=>............................] - ETA: 18:50 - loss: 0.4122 - regression_loss: 0.3374 - classification_loss: 0.0747
 252/3000 [=>............................] - ETA: 18:49 - loss: 0.4111 - regression_loss: 0.3366 - classification_loss: 0.0746
 253/3000 [=>............................] - ETA: 18:49 - loss: 0.4110 - regression_loss: 0.3360 - classification_loss: 0.0750
 254/3000 [=>............................] - ETA: 18:48 - loss: 0.4103 - regression_loss: 0.3352 - classification_loss: 0.0751
 255/3000 [=>............................] - ETA: 18:48 - loss: 0.4095 - regression_loss: 0.3345 - classification_loss: 0.0750
 256/3000 [=>............................] - ETA: 18:48 - loss: 0.4106 - regression_loss: 0.3354 - classification_loss: 0.0752
 257/3000 [=>............................] - ETA: 18:47 - loss: 0.4099 - regression_loss: 0.3348 - classification_loss: 0.0750
 258/3000 [=>............................] - ETA: 18:47 - loss: 0.4092 - regression_loss: 0.3343 - classification_loss: 0.0749
 259/3000 [=>............................] - ETA: 18:46 - loss: 0.4087 - regression_loss: 0.3338 - classification_loss: 0.0749
 260/3000 [=>............................] - ETA: 18:46 - loss: 0.4079 - regression_loss: 0.3332 - classification_loss: 0.0748
 261/3000 [=>............................] - ETA: 18:45 - loss: 0.4078 - regression_loss: 0.3327 - classification_loss: 0.0751
 262/3000 [=>............................] - ETA: 18:45 - loss: 0.4076 - regression_loss: 0.3325 - classification_loss: 0.0751
 263/3000 [=>............................] - ETA: 18:44 - loss: 0.4074 - regression_loss: 0.3323 - classification_loss: 0.0751
 264/3000 [=>............................] - ETA: 18:44 - loss: 0.4072 - regression_loss: 0.3318 - classification_loss: 0.0754
 265/3000 [=>............................] - ETA: 18:44 - loss: 0.4067 - regression_loss: 0.3313 - classification_loss: 0.0755
 266/3000 [=>............................] - ETA: 18:43 - loss: 0.4074 - regression_loss: 0.3321 - classification_loss: 0.0754
 267/3000 [=>............................] - ETA: 18:43 - loss: 0.4071 - regression_loss: 0.3318 - classification_loss: 0.0752
 268/3000 [=>............................] - ETA: 18:42 - loss: 0.4074 - regression_loss: 0.3322 - classification_loss: 0.0752
 269/3000 [=>............................] - ETA: 18:43 - loss: 0.4080 - regression_loss: 0.3328 - classification_loss: 0.0752
 270/3000 [=>............................] - ETA: 18:42 - loss: 0.4078 - regression_loss: 0.3323 - classification_loss: 0.0755
 271/3000 [=>............................] - ETA: 18:42 - loss: 0.4084 - regression_loss: 0.3329 - classification_loss: 0.0755
 272/3000 [=>............................] - ETA: 18:41 - loss: 0.4087 - regression_loss: 0.3332 - classification_loss: 0.0755
 273/3000 [=>............................] - ETA: 18:41 - loss: 0.4079 - regression_loss: 0.3325 - classification_loss: 0.0754
 274/3000 [=>............................] - ETA: 18:40 - loss: 0.4075 - regression_loss: 0.3321 - classification_loss: 0.0753
 275/3000 [=>............................] - ETA: 18:40 - loss: 0.4085 - regression_loss: 0.3331 - classification_loss: 0.0754
 276/3000 [=>............................] - ETA: 18:39 - loss: 0.4087 - regression_loss: 0.3335 - classification_loss: 0.0753
 277/3000 [=>............................] - ETA: 18:39 - loss: 0.4082 - regression_loss: 0.3330 - classification_loss: 0.0752
 278/3000 [=>............................] - ETA: 18:39 - loss: 0.4075 - regression_loss: 0.3325 - classification_loss: 0.0750
 279/3000 [=>............................] - ETA: 18:38 - loss: 0.4080 - regression_loss: 0.3325 - classification_loss: 0.0755
 280/3000 [=>............................] - ETA: 18:38 - loss: 0.4079 - regression_loss: 0.3325 - classification_loss: 0.0754
 281/3000 [=>............................] - ETA: 18:37 - loss: 0.4081 - regression_loss: 0.3325 - classification_loss: 0.0755
 282/3000 [=>............................] - ETA: 18:37 - loss: 0.4074 - regression_loss: 0.3320 - classification_loss: 0.0755
 283/3000 [=>............................] - ETA: 18:36 - loss: 0.4068 - regression_loss: 0.3315 - classification_loss: 0.0753
 284/3000 [=>............................] - ETA: 18:36 - loss: 0.4071 - regression_loss: 0.3318 - classification_loss: 0.0752
 285/3000 [=>............................] - ETA: 18:35 - loss: 0.4081 - regression_loss: 0.3327 - classification_loss: 0.0754
 286/3000 [=>............................] - ETA: 18:35 - loss: 0.4079 - regression_loss: 0.3325 - classification_loss: 0.0754
 287/3000 [=>............................] - ETA: 18:35 - loss: 0.4076 - regression_loss: 0.3323 - classification_loss: 0.0753
 288/3000 [=>............................] - ETA: 18:34 - loss: 0.4082 - regression_loss: 0.3331 - classification_loss: 0.0752
 289/3000 [=>............................] - ETA: 18:34 - loss: 0.4081 - regression_loss: 0.3329 - classification_loss: 0.0752
 290/3000 [=>............................] - ETA: 18:33 - loss: 0.4079 - regression_loss: 0.3327 - classification_loss: 0.0752
 291/3000 [=>............................] - ETA: 18:33 - loss: 0.4080 - regression_loss: 0.3328 - classification_loss: 0.0753
 292/3000 [=>............................] - ETA: 18:32 - loss: 0.4088 - regression_loss: 0.3336 - classification_loss: 0.0752
 293/3000 [=>............................] - ETA: 18:32 - loss: 0.4080 - regression_loss: 0.3330 - classification_loss: 0.0751
 294/3000 [=>............................] - ETA: 18:32 - loss: 0.4081 - regression_loss: 0.3330 - classification_loss: 0.0751
 295/3000 [=>............................] - ETA: 18:31 - loss: 0.4084 - regression_loss: 0.3334 - classification_loss: 0.0751
 296/3000 [=>............................] - ETA: 18:31 - loss: 0.4084 - regression_loss: 0.3334 - classification_loss: 0.0750
 297/3000 [=>............................] - ETA: 18:30 - loss: 0.4086 - regression_loss: 0.3334 - classification_loss: 0.0752
 298/3000 [=>............................] - ETA: 18:30 - loss: 0.4093 - regression_loss: 0.3340 - classification_loss: 0.0752
 299/3000 [=>............................] - ETA: 18:30 - loss: 0.4094 - regression_loss: 0.3342 - classification_loss: 0.0752
 300/3000 [==>...........................] - ETA: 18:29 - loss: 0.4107 - regression_loss: 0.3354 - classification_loss: 0.0752
 301/3000 [==>...........................] - ETA: 18:29 - loss: 0.4113 - regression_loss: 0.3361 - classification_loss: 0.0752
 302/3000 [==>...........................] - ETA: 18:28 - loss: 0.4113 - regression_loss: 0.3362 - classification_loss: 0.0750
 303/3000 [==>...........................] - ETA: 18:28 - loss: 0.4118 - regression_loss: 0.3367 - classification_loss: 0.0751
 304/3000 [==>...........................] - ETA: 18:28 - loss: 0.4121 - regression_loss: 0.3370 - classification_loss: 0.0752
 305/3000 [==>...........................] - ETA: 18:27 - loss: 0.4116 - regression_loss: 0.3366 - classification_loss: 0.0750
 306/3000 [==>...........................] - ETA: 18:27 - loss: 0.4118 - regression_loss: 0.3366 - classification_loss: 0.0751
 307/3000 [==>...........................] - ETA: 18:26 - loss: 0.4115 - regression_loss: 0.3364 - classification_loss: 0.0751
 308/3000 [==>...........................] - ETA: 18:26 - loss: 0.4110 - regression_loss: 0.3360 - classification_loss: 0.0750
 309/3000 [==>...........................] - ETA: 18:25 - loss: 0.4104 - regression_loss: 0.3356 - classification_loss: 0.0748
 310/3000 [==>...........................] - ETA: 18:25 - loss: 0.4101 - regression_loss: 0.3354 - classification_loss: 0.0747
 311/3000 [==>...........................] - ETA: 18:24 - loss: 0.4098 - regression_loss: 0.3351 - classification_loss: 0.0747
 312/3000 [==>...........................] - ETA: 18:24 - loss: 0.4095 - regression_loss: 0.3349 - classification_loss: 0.0746
 313/3000 [==>...........................] - ETA: 18:23 - loss: 0.4093 - regression_loss: 0.3347 - classification_loss: 0.0747
 314/3000 [==>...........................] - ETA: 18:23 - loss: 0.4105 - regression_loss: 0.3357 - classification_loss: 0.0748
 315/3000 [==>...........................] - ETA: 18:22 - loss: 0.4105 - regression_loss: 0.3358 - classification_loss: 0.0747
 316/3000 [==>...........................] - ETA: 18:22 - loss: 0.4104 - regression_loss: 0.3357 - classification_loss: 0.0747
 317/3000 [==>...........................] - ETA: 18:21 - loss: 0.4111 - regression_loss: 0.3364 - classification_loss: 0.0747
 318/3000 [==>...........................] - ETA: 18:21 - loss: 0.4111 - regression_loss: 0.3365 - classification_loss: 0.0746
 319/3000 [==>...........................] - ETA: 18:21 - loss: 0.4108 - regression_loss: 0.3364 - classification_loss: 0.0744
 320/3000 [==>...........................] - ETA: 18:20 - loss: 0.4109 - regression_loss: 0.3366 - classification_loss: 0.0743
 321/3000 [==>...........................] - ETA: 18:20 - loss: 0.4104 - regression_loss: 0.3363 - classification_loss: 0.0742
 322/3000 [==>...........................] - ETA: 18:20 - loss: 0.4107 - regression_loss: 0.3361 - classification_loss: 0.0746
 323/3000 [==>...........................] - ETA: 18:19 - loss: 0.4102 - regression_loss: 0.3357 - classification_loss: 0.0745
 324/3000 [==>...........................] - ETA: 18:19 - loss: 0.4102 - regression_loss: 0.3357 - classification_loss: 0.0745
 325/3000 [==>...........................] - ETA: 18:18 - loss: 0.4110 - regression_loss: 0.3365 - classification_loss: 0.0746
 326/3000 [==>...........................] - ETA: 18:18 - loss: 0.4110 - regression_loss: 0.3365 - classification_loss: 0.0745
 327/3000 [==>...........................] - ETA: 18:17 - loss: 0.4106 - regression_loss: 0.3361 - classification_loss: 0.0744
 328/3000 [==>...........................] - ETA: 18:17 - loss: 0.4111 - regression_loss: 0.3366 - classification_loss: 0.0745
 329/3000 [==>...........................] - ETA: 18:16 - loss: 0.4114 - regression_loss: 0.3364 - classification_loss: 0.0749
 330/3000 [==>...........................] - ETA: 18:16 - loss: 0.4111 - regression_loss: 0.3363 - classification_loss: 0.0748
 331/3000 [==>...........................] - ETA: 18:16 - loss: 0.4108 - regression_loss: 0.3361 - classification_loss: 0.0747
 332/3000 [==>...........................] - ETA: 18:15 - loss: 0.4103 - regression_loss: 0.3356 - classification_loss: 0.0747
 333/3000 [==>...........................] - ETA: 18:15 - loss: 0.4101 - regression_loss: 0.3354 - classification_loss: 0.0747
 334/3000 [==>...........................] - ETA: 18:15 - loss: 0.4100 - regression_loss: 0.3353 - classification_loss: 0.0746
 335/3000 [==>...........................] - ETA: 18:14 - loss: 0.4102 - regression_loss: 0.3356 - classification_loss: 0.0746
 336/3000 [==>...........................] - ETA: 18:14 - loss: 0.4108 - regression_loss: 0.3359 - classification_loss: 0.0749
 337/3000 [==>...........................] - ETA: 18:14 - loss: 0.4109 - regression_loss: 0.3360 - classification_loss: 0.0749
 338/3000 [==>...........................] - ETA: 18:13 - loss: 0.4114 - regression_loss: 0.3365 - classification_loss: 0.0749
 339/3000 [==>...........................] - ETA: 18:13 - loss: 0.4112 - regression_loss: 0.3362 - classification_loss: 0.0750
 340/3000 [==>...........................] - ETA: 18:13 - loss: 0.4119 - regression_loss: 0.3369 - classification_loss: 0.0750
 341/3000 [==>...........................] - ETA: 18:12 - loss: 0.4120 - regression_loss: 0.3370 - classification_loss: 0.0750
 342/3000 [==>...........................] - ETA: 18:12 - loss: 0.4118 - regression_loss: 0.3368 - classification_loss: 0.0750
 343/3000 [==>...........................] - ETA: 18:12 - loss: 0.4113 - regression_loss: 0.3364 - classification_loss: 0.0749
 344/3000 [==>...........................] - ETA: 18:11 - loss: 0.4107 - regression_loss: 0.3359 - classification_loss: 0.0749
 345/3000 [==>...........................] - ETA: 18:11 - loss: 0.4109 - regression_loss: 0.3361 - classification_loss: 0.0749
 346/3000 [==>...........................] - ETA: 18:10 - loss: 0.4111 - regression_loss: 0.3364 - classification_loss: 0.0748
 347/3000 [==>...........................] - ETA: 18:10 - loss: 0.4112 - regression_loss: 0.3363 - classification_loss: 0.0750
 348/3000 [==>...........................] - ETA: 18:09 - loss: 0.4111 - regression_loss: 0.3359 - classification_loss: 0.0752
 349/3000 [==>...........................] - ETA: 18:09 - loss: 0.4116 - regression_loss: 0.3364 - classification_loss: 0.0752
 350/3000 [==>...........................] - ETA: 18:08 - loss: 0.4114 - regression_loss: 0.3363 - classification_loss: 0.0750
 351/3000 [==>...........................] - ETA: 18:08 - loss: 0.4112 - regression_loss: 0.3361 - classification_loss: 0.0750
 352/3000 [==>...........................] - ETA: 18:07 - loss: 0.4113 - regression_loss: 0.3362 - classification_loss: 0.0751
 353/3000 [==>...........................] - ETA: 18:07 - loss: 0.4116 - regression_loss: 0.3363 - classification_loss: 0.0753
 354/3000 [==>...........................] - ETA: 18:06 - loss: 0.4115 - regression_loss: 0.3362 - classification_loss: 0.0753
 355/3000 [==>...........................] - ETA: 18:06 - loss: 0.4115 - regression_loss: 0.3361 - classification_loss: 0.0753
 356/3000 [==>...........................] - ETA: 18:06 - loss: 0.4110 - regression_loss: 0.3358 - classification_loss: 0.0752
 357/3000 [==>...........................] - ETA: 18:05 - loss: 0.4107 - regression_loss: 0.3356 - classification_loss: 0.0752
 358/3000 [==>...........................] - ETA: 18:05 - loss: 0.4107 - regression_loss: 0.3356 - classification_loss: 0.0751
 359/3000 [==>...........................] - ETA: 18:04 - loss: 0.4102 - regression_loss: 0.3352 - classification_loss: 0.0750
 360/3000 [==>...........................] - ETA: 18:04 - loss: 0.4095 - regression_loss: 0.3345 - classification_loss: 0.0750
 361/3000 [==>...........................] - ETA: 18:04 - loss: 0.4089 - regression_loss: 0.3340 - classification_loss: 0.0749
 362/3000 [==>...........................] - ETA: 18:03 - loss: 0.4092 - regression_loss: 0.3342 - classification_loss: 0.0750
 363/3000 [==>...........................] - ETA: 18:03 - loss: 0.4088 - regression_loss: 0.3339 - classification_loss: 0.0749
 364/3000 [==>...........................] - ETA: 18:02 - loss: 0.4087 - regression_loss: 0.3338 - classification_loss: 0.0749
 365/3000 [==>...........................] - ETA: 18:02 - loss: 0.4083 - regression_loss: 0.3335 - classification_loss: 0.0748
 366/3000 [==>...........................] - ETA: 18:01 - loss: 0.4085 - regression_loss: 0.3338 - classification_loss: 0.0747
 367/3000 [==>...........................] - ETA: 18:01 - loss: 0.4085 - regression_loss: 0.3338 - classification_loss: 0.0747
 368/3000 [==>...........................] - ETA: 18:01 - loss: 0.4087 - regression_loss: 0.3340 - classification_loss: 0.0747
 369/3000 [==>...........................] - ETA: 18:00 - loss: 0.4089 - regression_loss: 0.3343 - classification_loss: 0.0747
 370/3000 [==>...........................] - ETA: 18:00 - loss: 0.4088 - regression_loss: 0.3342 - classification_loss: 0.0746
 371/3000 [==>...........................] - ETA: 17:59 - loss: 0.4091 - regression_loss: 0.3343 - classification_loss: 0.0748
 372/3000 [==>...........................] - ETA: 17:59 - loss: 0.4091 - regression_loss: 0.3343 - classification_loss: 0.0748
 373/3000 [==>...........................] - ETA: 17:58 - loss: 0.4087 - regression_loss: 0.3340 - classification_loss: 0.0747
 374/3000 [==>...........................] - ETA: 17:58 - loss: 0.4093 - regression_loss: 0.3346 - classification_loss: 0.0747
 375/3000 [==>...........................] - ETA: 17:57 - loss: 0.4090 - regression_loss: 0.3345 - classification_loss: 0.0746
 376/3000 [==>...........................] - ETA: 17:57 - loss: 0.4091 - regression_loss: 0.3344 - classification_loss: 0.0746
 377/3000 [==>...........................] - ETA: 17:57 - loss: 0.4091 - regression_loss: 0.3345 - classification_loss: 0.0746
 378/3000 [==>...........................] - ETA: 17:56 - loss: 0.4093 - regression_loss: 0.3347 - classification_loss: 0.0746
 379/3000 [==>...........................] - ETA: 17:56 - loss: 0.4089 - regression_loss: 0.3345 - classification_loss: 0.0745
 380/3000 [==>...........................] - ETA: 17:56 - loss: 0.4089 - regression_loss: 0.3345 - classification_loss: 0.0743
 381/3000 [==>...........................] - ETA: 17:56 - loss: 0.4084 - regression_loss: 0.3341 - classification_loss: 0.0743
 382/3000 [==>...........................] - ETA: 17:55 - loss: 0.4085 - regression_loss: 0.3340 - classification_loss: 0.0744
 383/3000 [==>...........................] - ETA: 17:55 - loss: 0.4091 - regression_loss: 0.3346 - classification_loss: 0.0744
 384/3000 [==>...........................] - ETA: 17:55 - loss: 0.4091 - regression_loss: 0.3347 - classification_loss: 0.0744
 385/3000 [==>...........................] - ETA: 17:54 - loss: 0.4090 - regression_loss: 0.3346 - classification_loss: 0.0744
 386/3000 [==>...........................] - ETA: 17:54 - loss: 0.4087 - regression_loss: 0.3344 - classification_loss: 0.0743
 387/3000 [==>...........................] - ETA: 17:53 - loss: 0.4084 - regression_loss: 0.3341 - classification_loss: 0.0744
 388/3000 [==>...........................] - ETA: 17:53 - loss: 0.4081 - regression_loss: 0.3338 - classification_loss: 0.0743
 389/3000 [==>...........................] - ETA: 17:53 - loss: 0.4076 - regression_loss: 0.3334 - classification_loss: 0.0741
 390/3000 [==>...........................] - ETA: 17:52 - loss: 0.4072 - regression_loss: 0.3332 - classification_loss: 0.0740
 391/3000 [==>...........................] - ETA: 17:52 - loss: 0.4074 - regression_loss: 0.3334 - classification_loss: 0.0740
 392/3000 [==>...........................] - ETA: 17:51 - loss: 0.4075 - regression_loss: 0.3335 - classification_loss: 0.0740
 393/3000 [==>...........................] - ETA: 17:51 - loss: 0.4076 - regression_loss: 0.3337 - classification_loss: 0.0739
 394/3000 [==>...........................] - ETA: 17:50 - loss: 0.4078 - regression_loss: 0.3338 - classification_loss: 0.0740
 395/3000 [==>...........................] - ETA: 17:50 - loss: 0.4073 - regression_loss: 0.3334 - classification_loss: 0.0738
 396/3000 [==>...........................] - ETA: 17:50 - loss: 0.4073 - regression_loss: 0.3335 - classification_loss: 0.0738
 397/3000 [==>...........................] - ETA: 17:49 - loss: 0.4080 - regression_loss: 0.3340 - classification_loss: 0.0740
 398/3000 [==>...........................] - ETA: 17:49 - loss: 0.4083 - regression_loss: 0.3342 - classification_loss: 0.0740
 399/3000 [==>...........................] - ETA: 17:48 - loss: 0.4081 - regression_loss: 0.3341 - classification_loss: 0.0741
 400/3000 [===>..........................] - ETA: 17:48 - loss: 0.4084 - regression_loss: 0.3344 - classification_loss: 0.0740
 401/3000 [===>..........................] - ETA: 17:47 - loss: 0.4084 - regression_loss: 0.3345 - classification_loss: 0.0740
 402/3000 [===>..........................] - ETA: 17:47 - loss: 0.4085 - regression_loss: 0.3346 - classification_loss: 0.0739
 403/3000 [===>..........................] - ETA: 17:47 - loss: 0.4089 - regression_loss: 0.3348 - classification_loss: 0.0740
 404/3000 [===>..........................] - ETA: 17:46 - loss: 0.4085 - regression_loss: 0.3346 - classification_loss: 0.0739
 405/3000 [===>..........................] - ETA: 17:46 - loss: 0.4085 - regression_loss: 0.3346 - classification_loss: 0.0739
 406/3000 [===>..........................] - ETA: 17:45 - loss: 0.4080 - regression_loss: 0.3342 - classification_loss: 0.0738
 407/3000 [===>..........................] - ETA: 17:45 - loss: 0.4075 - regression_loss: 0.3338 - classification_loss: 0.0736
 408/3000 [===>..........................] - ETA: 17:44 - loss: 0.4077 - regression_loss: 0.3340 - classification_loss: 0.0736
 409/3000 [===>..........................] - ETA: 17:44 - loss: 0.4072 - regression_loss: 0.3336 - classification_loss: 0.0736
 410/3000 [===>..........................] - ETA: 17:44 - loss: 0.4075 - regression_loss: 0.3339 - classification_loss: 0.0736
 411/3000 [===>..........................] - ETA: 17:43 - loss: 0.4072 - regression_loss: 0.3337 - classification_loss: 0.0735
 412/3000 [===>..........................] - ETA: 17:43 - loss: 0.4069 - regression_loss: 0.3334 - classification_loss: 0.0735
 413/3000 [===>..........................] - ETA: 17:43 - loss: 0.4072 - regression_loss: 0.3337 - classification_loss: 0.0735
 414/3000 [===>..........................] - ETA: 17:42 - loss: 0.4075 - regression_loss: 0.3340 - classification_loss: 0.0735
 415/3000 [===>..........................] - ETA: 17:42 - loss: 0.4072 - regression_loss: 0.3339 - classification_loss: 0.0733
 416/3000 [===>..........................] - ETA: 17:41 - loss: 0.4076 - regression_loss: 0.3341 - classification_loss: 0.0735
 417/3000 [===>..........................] - ETA: 17:41 - loss: 0.4073 - regression_loss: 0.3339 - classification_loss: 0.0734
 418/3000 [===>..........................] - ETA: 17:41 - loss: 0.4068 - regression_loss: 0.3335 - classification_loss: 0.0733
 419/3000 [===>..........................] - ETA: 17:40 - loss: 0.4068 - regression_loss: 0.3336 - classification_loss: 0.0732
 420/3000 [===>..........................] - ETA: 17:40 - loss: 0.4075 - regression_loss: 0.3342 - classification_loss: 0.0732
 421/3000 [===>..........................] - ETA: 17:39 - loss: 0.4080 - regression_loss: 0.3347 - classification_loss: 0.0733
 422/3000 [===>..........................] - ETA: 17:39 - loss: 0.4076 - regression_loss: 0.3344 - classification_loss: 0.0732
 423/3000 [===>..........................] - ETA: 17:39 - loss: 0.4075 - regression_loss: 0.3344 - classification_loss: 0.0731
 424/3000 [===>..........................] - ETA: 17:38 - loss: 0.4076 - regression_loss: 0.3345 - classification_loss: 0.0731
 425/3000 [===>..........................] - ETA: 17:38 - loss: 0.4076 - regression_loss: 0.3345 - classification_loss: 0.0731
 426/3000 [===>..........................] - ETA: 17:37 - loss: 0.4075 - regression_loss: 0.3344 - classification_loss: 0.0731
 427/3000 [===>..........................] - ETA: 17:37 - loss: 0.4076 - regression_loss: 0.3344 - classification_loss: 0.0731
 428/3000 [===>..........................] - ETA: 17:36 - loss: 0.4072 - regression_loss: 0.3341 - classification_loss: 0.0731
 429/3000 [===>..........................] - ETA: 17:36 - loss: 0.4069 - regression_loss: 0.3339 - classification_loss: 0.0731
 430/3000 [===>..........................] - ETA: 17:36 - loss: 0.4071 - regression_loss: 0.3341 - classification_loss: 0.0730
 431/3000 [===>..........................] - ETA: 17:35 - loss: 0.4065 - regression_loss: 0.3336 - classification_loss: 0.0729
 432/3000 [===>..........................] - ETA: 17:35 - loss: 0.4061 - regression_loss: 0.3332 - classification_loss: 0.0728
 433/3000 [===>..........................] - ETA: 17:34 - loss: 0.4057 - regression_loss: 0.3329 - classification_loss: 0.0728
 434/3000 [===>..........................] - ETA: 17:34 - loss: 0.4063 - regression_loss: 0.3335 - classification_loss: 0.0728
 435/3000 [===>..........................] - ETA: 17:34 - loss: 0.4058 - regression_loss: 0.3331 - classification_loss: 0.0727
 436/3000 [===>..........................] - ETA: 17:33 - loss: 0.4059 - regression_loss: 0.3331 - classification_loss: 0.0728
 437/3000 [===>..........................] - ETA: 17:33 - loss: 0.4056 - regression_loss: 0.3329 - classification_loss: 0.0727
 438/3000 [===>..........................] - ETA: 17:32 - loss: 0.4052 - regression_loss: 0.3325 - classification_loss: 0.0727
 439/3000 [===>..........................] - ETA: 17:32 - loss: 0.4050 - regression_loss: 0.3324 - classification_loss: 0.0726
 440/3000 [===>..........................] - ETA: 17:32 - loss: 0.4048 - regression_loss: 0.3322 - classification_loss: 0.0726
 441/3000 [===>..........................] - ETA: 17:31 - loss: 0.4055 - regression_loss: 0.3329 - classification_loss: 0.0726
 442/3000 [===>..........................] - ETA: 17:31 - loss: 0.4056 - regression_loss: 0.3331 - classification_loss: 0.0725
 443/3000 [===>..........................] - ETA: 17:30 - loss: 0.4052 - regression_loss: 0.3327 - classification_loss: 0.0725
 444/3000 [===>..........................] - ETA: 17:30 - loss: 0.4048 - regression_loss: 0.3324 - classification_loss: 0.0724
 445/3000 [===>..........................] - ETA: 17:30 - loss: 0.4044 - regression_loss: 0.3320 - classification_loss: 0.0724
 446/3000 [===>..........................] - ETA: 17:29 - loss: 0.4042 - regression_loss: 0.3317 - classification_loss: 0.0724
 447/3000 [===>..........................] - ETA: 17:29 - loss: 0.4036 - regression_loss: 0.3312 - classification_loss: 0.0724
 448/3000 [===>..........................] - ETA: 17:28 - loss: 0.4036 - regression_loss: 0.3312 - classification_loss: 0.0723
 449/3000 [===>..........................] - ETA: 17:28 - loss: 0.4036 - regression_loss: 0.3313 - classification_loss: 0.0723
 450/3000 [===>..........................] - ETA: 17:28 - loss: 0.4037 - regression_loss: 0.3314 - classification_loss: 0.0723
 451/3000 [===>..........................] - ETA: 17:27 - loss: 0.4034 - regression_loss: 0.3312 - classification_loss: 0.0722
 452/3000 [===>..........................] - ETA: 17:27 - loss: 0.4031 - regression_loss: 0.3309 - classification_loss: 0.0721
 453/3000 [===>..........................] - ETA: 17:26 - loss: 0.4027 - regression_loss: 0.3306 - classification_loss: 0.0721
 454/3000 [===>..........................] - ETA: 17:26 - loss: 0.4025 - regression_loss: 0.3304 - classification_loss: 0.0721
 455/3000 [===>..........................] - ETA: 17:25 - loss: 0.4024 - regression_loss: 0.3304 - classification_loss: 0.0721
 456/3000 [===>..........................] - ETA: 17:25 - loss: 0.4024 - regression_loss: 0.3303 - classification_loss: 0.0721
 457/3000 [===>..........................] - ETA: 17:24 - loss: 0.4020 - regression_loss: 0.3300 - classification_loss: 0.0720
 458/3000 [===>..........................] - ETA: 17:24 - loss: 0.4021 - regression_loss: 0.3300 - classification_loss: 0.0721
 459/3000 [===>..........................] - ETA: 17:24 - loss: 0.4018 - regression_loss: 0.3298 - classification_loss: 0.0720
 460/3000 [===>..........................] - ETA: 17:23 - loss: 0.4026 - regression_loss: 0.3304 - classification_loss: 0.0721
 461/3000 [===>..........................] - ETA: 17:23 - loss: 0.4027 - regression_loss: 0.3306 - classification_loss: 0.0721
 462/3000 [===>..........................] - ETA: 17:22 - loss: 0.4025 - regression_loss: 0.3304 - classification_loss: 0.0721
 463/3000 [===>..........................] - ETA: 17:22 - loss: 0.4022 - regression_loss: 0.3302 - classification_loss: 0.0721
 464/3000 [===>..........................] - ETA: 17:21 - loss: 0.4025 - regression_loss: 0.3304 - classification_loss: 0.0720
 465/3000 [===>..........................] - ETA: 17:21 - loss: 0.4024 - regression_loss: 0.3303 - classification_loss: 0.0721
 466/3000 [===>..........................] - ETA: 17:21 - loss: 0.4023 - regression_loss: 0.3302 - classification_loss: 0.0722
 467/3000 [===>..........................] - ETA: 17:20 - loss: 0.4026 - regression_loss: 0.3304 - classification_loss: 0.0722
 468/3000 [===>..........................] - ETA: 17:20 - loss: 0.4021 - regression_loss: 0.3300 - classification_loss: 0.0721
 469/3000 [===>..........................] - ETA: 17:20 - loss: 0.4022 - regression_loss: 0.3301 - classification_loss: 0.0720
 470/3000 [===>..........................] - ETA: 17:19 - loss: 0.4020 - regression_loss: 0.3300 - classification_loss: 0.0720
 471/3000 [===>..........................] - ETA: 17:19 - loss: 0.4017 - regression_loss: 0.3298 - classification_loss: 0.0719
 472/3000 [===>..........................] - ETA: 17:18 - loss: 0.4021 - regression_loss: 0.3302 - classification_loss: 0.0719
 473/3000 [===>..........................] - ETA: 17:18 - loss: 0.4023 - regression_loss: 0.3302 - classification_loss: 0.0721
 474/3000 [===>..........................] - ETA: 17:18 - loss: 0.4027 - regression_loss: 0.3307 - classification_loss: 0.0720
 475/3000 [===>..........................] - ETA: 17:17 - loss: 0.4022 - regression_loss: 0.3303 - classification_loss: 0.0719
 476/3000 [===>..........................] - ETA: 17:17 - loss: 0.4019 - regression_loss: 0.3301 - classification_loss: 0.0719
 477/3000 [===>..........................] - ETA: 17:17 - loss: 0.4019 - regression_loss: 0.3301 - classification_loss: 0.0718
 478/3000 [===>..........................] - ETA: 17:16 - loss: 0.4020 - regression_loss: 0.3303 - classification_loss: 0.0717
 479/3000 [===>..........................] - ETA: 17:16 - loss: 0.4019 - regression_loss: 0.3302 - classification_loss: 0.0717
 480/3000 [===>..........................] - ETA: 17:15 - loss: 0.4015 - regression_loss: 0.3299 - classification_loss: 0.0716
 481/3000 [===>..........................] - ETA: 17:15 - loss: 0.4011 - regression_loss: 0.3296 - classification_loss: 0.0715
 482/3000 [===>..........................] - ETA: 17:15 - loss: 0.4012 - regression_loss: 0.3297 - classification_loss: 0.0715
 483/3000 [===>..........................] - ETA: 17:14 - loss: 0.4014 - regression_loss: 0.3298 - classification_loss: 0.0716
 484/3000 [===>..........................] - ETA: 17:14 - loss: 0.4010 - regression_loss: 0.3295 - classification_loss: 0.0715
 485/3000 [===>..........................] - ETA: 17:13 - loss: 0.4010 - regression_loss: 0.3296 - classification_loss: 0.0714
 486/3000 [===>..........................] - ETA: 17:13 - loss: 0.4009 - regression_loss: 0.3295 - classification_loss: 0.0713
 487/3000 [===>..........................] - ETA: 17:13 - loss: 0.4008 - regression_loss: 0.3294 - classification_loss: 0.0714
 488/3000 [===>..........................] - ETA: 17:12 - loss: 0.4007 - regression_loss: 0.3292 - classification_loss: 0.0714
 489/3000 [===>..........................] - ETA: 17:12 - loss: 0.4004 - regression_loss: 0.3291 - classification_loss: 0.0713
 490/3000 [===>..........................] - ETA: 17:12 - loss: 0.4001 - regression_loss: 0.3289 - classification_loss: 0.0713
 491/3000 [===>..........................] - ETA: 17:11 - loss: 0.3999 - regression_loss: 0.3287 - classification_loss: 0.0713
 492/3000 [===>..........................] - ETA: 17:11 - loss: 0.4004 - regression_loss: 0.3291 - classification_loss: 0.0713
 493/3000 [===>..........................] - ETA: 17:11 - loss: 0.4001 - regression_loss: 0.3289 - classification_loss: 0.0712
 494/3000 [===>..........................] - ETA: 17:10 - loss: 0.3999 - regression_loss: 0.3287 - classification_loss: 0.0711
 495/3000 [===>..........................] - ETA: 17:09 - loss: 0.3998 - regression_loss: 0.3287 - classification_loss: 0.0711
 496/3000 [===>..........................] - ETA: 17:09 - loss: 0.3994 - regression_loss: 0.3284 - classification_loss: 0.0710
 497/3000 [===>..........................] - ETA: 17:09 - loss: 0.3990 - regression_loss: 0.3281 - classification_loss: 0.0709
 498/3000 [===>..........................] - ETA: 17:08 - loss: 0.3988 - regression_loss: 0.3279 - classification_loss: 0.0709
 499/3000 [===>..........................] - ETA: 17:08 - loss: 0.3990 - regression_loss: 0.3281 - classification_loss: 0.0709
 500/3000 [====>.........................] - ETA: 17:08 - loss: 0.3992 - regression_loss: 0.3282 - classification_loss: 0.0710
 501/3000 [====>.........................] - ETA: 17:07 - loss: 0.3994 - regression_loss: 0.3285 - classification_loss: 0.0709
 502/3000 [====>.........................] - ETA: 17:07 - loss: 0.3993 - regression_loss: 0.3285 - classification_loss: 0.0708
 503/3000 [====>.........................] - ETA: 17:06 - loss: 0.3995 - regression_loss: 0.3287 - classification_loss: 0.0708
 504/3000 [====>.........................] - ETA: 17:06 - loss: 0.3996 - regression_loss: 0.3287 - classification_loss: 0.0709
 505/3000 [====>.........................] - ETA: 17:05 - loss: 0.3994 - regression_loss: 0.3285 - classification_loss: 0.0709
 506/3000 [====>.........................] - ETA: 17:05 - loss: 0.3994 - regression_loss: 0.3286 - classification_loss: 0.0708
 507/3000 [====>.........................] - ETA: 17:05 - loss: 0.3997 - regression_loss: 0.3288 - classification_loss: 0.0709
 508/3000 [====>.........................] - ETA: 17:04 - loss: 0.3998 - regression_loss: 0.3289 - classification_loss: 0.0709
 509/3000 [====>.........................] - ETA: 17:04 - loss: 0.3995 - regression_loss: 0.3287 - classification_loss: 0.0708
 510/3000 [====>.........................] - ETA: 17:03 - loss: 0.3999 - regression_loss: 0.3290 - classification_loss: 0.0709
 511/3000 [====>.........................] - ETA: 17:03 - loss: 0.3997 - regression_loss: 0.3290 - classification_loss: 0.0708
 512/3000 [====>.........................] - ETA: 17:02 - loss: 0.3998 - regression_loss: 0.3291 - classification_loss: 0.0707
 513/3000 [====>.........................] - ETA: 17:02 - loss: 0.3998 - regression_loss: 0.3291 - classification_loss: 0.0707
 514/3000 [====>.........................] - ETA: 17:02 - loss: 0.3996 - regression_loss: 0.3290 - classification_loss: 0.0706
 515/3000 [====>.........................] - ETA: 17:01 - loss: 0.3993 - regression_loss: 0.3287 - classification_loss: 0.0705
 516/3000 [====>.........................] - ETA: 17:01 - loss: 0.3993 - regression_loss: 0.3288 - classification_loss: 0.0705
 517/3000 [====>.........................] - ETA: 17:00 - loss: 0.3992 - regression_loss: 0.3288 - classification_loss: 0.0704
 518/3000 [====>.........................] - ETA: 17:00 - loss: 0.3989 - regression_loss: 0.3286 - classification_loss: 0.0703
 519/3000 [====>.........................] - ETA: 17:00 - loss: 0.3986 - regression_loss: 0.3284 - classification_loss: 0.0703
 520/3000 [====>.........................] - ETA: 16:59 - loss: 0.3983 - regression_loss: 0.3282 - classification_loss: 0.0702
 521/3000 [====>.........................] - ETA: 16:59 - loss: 0.3987 - regression_loss: 0.3285 - classification_loss: 0.0702
 522/3000 [====>.........................] - ETA: 16:58 - loss: 0.3989 - regression_loss: 0.3287 - classification_loss: 0.0702
 523/3000 [====>.........................] - ETA: 16:58 - loss: 0.3990 - regression_loss: 0.3288 - classification_loss: 0.0703
 524/3000 [====>.........................] - ETA: 16:58 - loss: 0.3993 - regression_loss: 0.3291 - classification_loss: 0.0702
 525/3000 [====>.........................] - ETA: 16:57 - loss: 0.3992 - regression_loss: 0.3291 - classification_loss: 0.0701
 526/3000 [====>.........................] - ETA: 16:57 - loss: 0.3990 - regression_loss: 0.3289 - classification_loss: 0.0701
 527/3000 [====>.........................] - ETA: 16:56 - loss: 0.3985 - regression_loss: 0.3285 - classification_loss: 0.0700
 528/3000 [====>.........................] - ETA: 16:56 - loss: 0.3988 - regression_loss: 0.3288 - classification_loss: 0.0700
 529/3000 [====>.........................] - ETA: 16:56 - loss: 0.3995 - regression_loss: 0.3295 - classification_loss: 0.0700
 530/3000 [====>.........................] - ETA: 16:55 - loss: 0.3997 - regression_loss: 0.3296 - classification_loss: 0.0701
 531/3000 [====>.........................] - ETA: 16:55 - loss: 0.3994 - regression_loss: 0.3294 - classification_loss: 0.0700
 532/3000 [====>.........................] - ETA: 16:54 - loss: 0.3994 - regression_loss: 0.3295 - classification_loss: 0.0700
 533/3000 [====>.........................] - ETA: 16:54 - loss: 0.3995 - regression_loss: 0.3296 - classification_loss: 0.0699
 534/3000 [====>.........................] - ETA: 16:54 - loss: 0.3997 - regression_loss: 0.3298 - classification_loss: 0.0699
 535/3000 [====>.........................] - ETA: 16:53 - loss: 0.3996 - regression_loss: 0.3297 - classification_loss: 0.0698
 536/3000 [====>.........................] - ETA: 16:53 - loss: 0.3992 - regression_loss: 0.3294 - classification_loss: 0.0698
 537/3000 [====>.........................] - ETA: 16:52 - loss: 0.3990 - regression_loss: 0.3292 - classification_loss: 0.0698
 538/3000 [====>.........................] - ETA: 16:52 - loss: 0.3991 - regression_loss: 0.3294 - classification_loss: 0.0698
 539/3000 [====>.........................] - ETA: 16:51 - loss: 0.3992 - regression_loss: 0.3293 - classification_loss: 0.0699
 540/3000 [====>.........................] - ETA: 16:51 - loss: 0.3990 - regression_loss: 0.3291 - classification_loss: 0.0699
 541/3000 [====>.........................] - ETA: 16:51 - loss: 0.3987 - regression_loss: 0.3288 - classification_loss: 0.0699
 542/3000 [====>.........................] - ETA: 16:50 - loss: 0.3991 - regression_loss: 0.3292 - classification_loss: 0.0699
 543/3000 [====>.........................] - ETA: 16:50 - loss: 0.3989 - regression_loss: 0.3290 - classification_loss: 0.0699
 544/3000 [====>.........................] - ETA: 16:50 - loss: 0.3990 - regression_loss: 0.3290 - classification_loss: 0.0701
 545/3000 [====>.........................] - ETA: 16:49 - loss: 0.3987 - regression_loss: 0.3288 - classification_loss: 0.0700
 546/3000 [====>.........................] - ETA: 16:49 - loss: 0.3989 - regression_loss: 0.3290 - classification_loss: 0.0699
 547/3000 [====>.........................] - ETA: 16:48 - loss: 0.3986 - regression_loss: 0.3287 - classification_loss: 0.0698
 548/3000 [====>.........................] - ETA: 16:48 - loss: 0.3986 - regression_loss: 0.3288 - classification_loss: 0.0698
 549/3000 [====>.........................] - ETA: 16:47 - loss: 0.3984 - regression_loss: 0.3286 - classification_loss: 0.0698
 550/3000 [====>.........................] - ETA: 16:47 - loss: 0.3982 - regression_loss: 0.3284 - classification_loss: 0.0699
 551/3000 [====>.........................] - ETA: 16:46 - loss: 0.3979 - regression_loss: 0.3281 - classification_loss: 0.0698
 552/3000 [====>.........................] - ETA: 16:46 - loss: 0.3978 - regression_loss: 0.3280 - classification_loss: 0.0698
 553/3000 [====>.........................] - ETA: 16:46 - loss: 0.3975 - regression_loss: 0.3278 - classification_loss: 0.0697
 554/3000 [====>.........................] - ETA: 16:45 - loss: 0.3976 - regression_loss: 0.3279 - classification_loss: 0.0697
 555/3000 [====>.........................] - ETA: 16:45 - loss: 0.3974 - regression_loss: 0.3276 - classification_loss: 0.0698
 556/3000 [====>.........................] - ETA: 16:44 - loss: 0.3973 - regression_loss: 0.3275 - classification_loss: 0.0698
 557/3000 [====>.........................] - ETA: 16:44 - loss: 0.3971 - regression_loss: 0.3273 - classification_loss: 0.0698
 558/3000 [====>.........................] - ETA: 16:44 - loss: 0.3972 - regression_loss: 0.3273 - classification_loss: 0.0699
 559/3000 [====>.........................] - ETA: 16:43 - loss: 0.3971 - regression_loss: 0.3271 - classification_loss: 0.0700
 560/3000 [====>.........................] - ETA: 16:43 - loss: 0.3969 - regression_loss: 0.3268 - classification_loss: 0.0700
 561/3000 [====>.........................] - ETA: 16:42 - loss: 0.3967 - regression_loss: 0.3267 - classification_loss: 0.0701
 562/3000 [====>.........................] - ETA: 16:42 - loss: 0.3969 - regression_loss: 0.3268 - classification_loss: 0.0701
 563/3000 [====>.........................] - ETA: 16:42 - loss: 0.3975 - regression_loss: 0.3273 - classification_loss: 0.0702
 564/3000 [====>.........................] - ETA: 16:41 - loss: 0.3974 - regression_loss: 0.3273 - classification_loss: 0.0701
 565/3000 [====>.........................] - ETA: 16:41 - loss: 0.3975 - regression_loss: 0.3271 - classification_loss: 0.0704
 566/3000 [====>.........................] - ETA: 16:40 - loss: 0.3977 - regression_loss: 0.3271 - classification_loss: 0.0706
 567/3000 [====>.........................] - ETA: 16:40 - loss: 0.3977 - regression_loss: 0.3269 - classification_loss: 0.0708
 568/3000 [====>.........................] - ETA: 16:40 - loss: 0.3975 - regression_loss: 0.3266 - classification_loss: 0.0708
 569/3000 [====>.........................] - ETA: 16:39 - loss: 0.3977 - regression_loss: 0.3268 - classification_loss: 0.0708
 570/3000 [====>.........................] - ETA: 16:39 - loss: 0.3973 - regression_loss: 0.3265 - classification_loss: 0.0708
 571/3000 [====>.........................] - ETA: 16:38 - loss: 0.3971 - regression_loss: 0.3263 - classification_loss: 0.0708
 572/3000 [====>.........................] - ETA: 16:38 - loss: 0.3975 - regression_loss: 0.3260 - classification_loss: 0.0715
 573/3000 [====>.........................] - ETA: 16:38 - loss: 0.3977 - regression_loss: 0.3260 - classification_loss: 0.0717
 574/3000 [====>.........................] - ETA: 16:37 - loss: 0.3979 - regression_loss: 0.3259 - classification_loss: 0.0721
 575/3000 [====>.........................] - ETA: 16:37 - loss: 0.3978 - regression_loss: 0.3257 - classification_loss: 0.0721
 576/3000 [====>.........................] - ETA: 16:36 - loss: 0.3975 - regression_loss: 0.3254 - classification_loss: 0.0720
 577/3000 [====>.........................] - ETA: 16:36 - loss: 0.3983 - regression_loss: 0.3260 - classification_loss: 0.0723
 578/3000 [====>.........................] - ETA: 16:36 - loss: 0.3981 - regression_loss: 0.3259 - classification_loss: 0.0722
 579/3000 [====>.........................] - ETA: 16:35 - loss: 0.3978 - regression_loss: 0.3256 - classification_loss: 0.0722
 580/3000 [====>.........................] - ETA: 16:35 - loss: 0.3977 - regression_loss: 0.3254 - classification_loss: 0.0724
 581/3000 [====>.........................] - ETA: 16:34 - loss: 0.3981 - regression_loss: 0.3257 - classification_loss: 0.0725
 582/3000 [====>.........................] - ETA: 16:34 - loss: 0.3982 - regression_loss: 0.3257 - classification_loss: 0.0724
 583/3000 [====>.........................] - ETA: 16:34 - loss: 0.3986 - regression_loss: 0.3258 - classification_loss: 0.0728
 584/3000 [====>.........................] - ETA: 16:33 - loss: 0.3984 - regression_loss: 0.3256 - classification_loss: 0.0727
 585/3000 [====>.........................] - ETA: 16:33 - loss: 0.3984 - regression_loss: 0.3257 - classification_loss: 0.0727
 586/3000 [====>.........................] - ETA: 16:32 - loss: 0.3989 - regression_loss: 0.3262 - classification_loss: 0.0727
 587/3000 [====>.........................] - ETA: 16:32 - loss: 0.3992 - regression_loss: 0.3263 - classification_loss: 0.0728
 588/3000 [====>.........................] - ETA: 16:32 - loss: 0.3992 - regression_loss: 0.3264 - classification_loss: 0.0728
 589/3000 [====>.........................] - ETA: 16:31 - loss: 0.3989 - regression_loss: 0.3261 - classification_loss: 0.0727
 590/3000 [====>.........................] - ETA: 16:31 - loss: 0.3990 - regression_loss: 0.3260 - classification_loss: 0.0730
 591/3000 [====>.........................] - ETA: 16:30 - loss: 0.3993 - regression_loss: 0.3263 - classification_loss: 0.0731
 592/3000 [====>.........................] - ETA: 16:30 - loss: 0.3993 - regression_loss: 0.3262 - classification_loss: 0.0731
 593/3000 [====>.........................] - ETA: 16:30 - loss: 0.3995 - regression_loss: 0.3264 - classification_loss: 0.0731
 594/3000 [====>.........................] - ETA: 16:29 - loss: 0.3993 - regression_loss: 0.3261 - classification_loss: 0.0731
 595/3000 [====>.........................] - ETA: 16:29 - loss: 0.3993 - regression_loss: 0.3262 - classification_loss: 0.0731
 596/3000 [====>.........................] - ETA: 16:28 - loss: 0.3994 - regression_loss: 0.3263 - classification_loss: 0.0731
 597/3000 [====>.........................] - ETA: 16:28 - loss: 0.3993 - regression_loss: 0.3263 - classification_loss: 0.0730
 598/3000 [====>.........................] - ETA: 16:27 - loss: 0.3991 - regression_loss: 0.3261 - classification_loss: 0.0731
 599/3000 [====>.........................] - ETA: 16:27 - loss: 0.3989 - regression_loss: 0.3259 - classification_loss: 0.0730
 600/3000 [=====>........................] - ETA: 16:27 - loss: 0.3994 - regression_loss: 0.3261 - classification_loss: 0.0733
 601/3000 [=====>........................] - ETA: 16:26 - loss: 0.3991 - regression_loss: 0.3259 - classification_loss: 0.0733
 602/3000 [=====>........................] - ETA: 16:26 - loss: 0.3993 - regression_loss: 0.3260 - classification_loss: 0.0733
 603/3000 [=====>........................] - ETA: 16:25 - loss: 0.3991 - regression_loss: 0.3258 - classification_loss: 0.0733
 604/3000 [=====>........................] - ETA: 16:25 - loss: 0.3989 - regression_loss: 0.3256 - classification_loss: 0.0733
 605/3000 [=====>........................] - ETA: 16:25 - loss: 0.3985 - regression_loss: 0.3253 - classification_loss: 0.0732
 606/3000 [=====>........................] - ETA: 16:24 - loss: 0.3984 - regression_loss: 0.3253 - classification_loss: 0.0731
 607/3000 [=====>........................] - ETA: 16:24 - loss: 0.3981 - regression_loss: 0.3250 - classification_loss: 0.0731
 608/3000 [=====>........................] - ETA: 16:23 - loss: 0.3982 - regression_loss: 0.3251 - classification_loss: 0.0731
 609/3000 [=====>........................] - ETA: 16:23 - loss: 0.3984 - regression_loss: 0.3253 - classification_loss: 0.0731
 610/3000 [=====>........................] - ETA: 16:23 - loss: 0.3987 - regression_loss: 0.3254 - classification_loss: 0.0733
 611/3000 [=====>........................] - ETA: 16:22 - loss: 0.3987 - regression_loss: 0.3253 - classification_loss: 0.0733
 612/3000 [=====>........................] - ETA: 16:22 - loss: 0.3988 - regression_loss: 0.3255 - classification_loss: 0.0733
 613/3000 [=====>........................] - ETA: 16:21 - loss: 0.3986 - regression_loss: 0.3254 - classification_loss: 0.0733
 614/3000 [=====>........................] - ETA: 16:21 - loss: 0.3984 - regression_loss: 0.3252 - classification_loss: 0.0732
 615/3000 [=====>........................] - ETA: 16:20 - loss: 0.3981 - regression_loss: 0.3249 - classification_loss: 0.0732
 616/3000 [=====>........................] - ETA: 16:20 - loss: 0.3979 - regression_loss: 0.3247 - classification_loss: 0.0731
 617/3000 [=====>........................] - ETA: 16:20 - loss: 0.3976 - regression_loss: 0.3245 - classification_loss: 0.0731
 618/3000 [=====>........................] - ETA: 16:19 - loss: 0.3972 - regression_loss: 0.3242 - classification_loss: 0.0731
 619/3000 [=====>........................] - ETA: 16:19 - loss: 0.3974 - regression_loss: 0.3243 - classification_loss: 0.0730
 620/3000 [=====>........................] - ETA: 16:18 - loss: 0.3973 - regression_loss: 0.3243 - classification_loss: 0.0730
 621/3000 [=====>........................] - ETA: 16:18 - loss: 0.3976 - regression_loss: 0.3247 - classification_loss: 0.0730
 622/3000 [=====>........................] - ETA: 16:17 - loss: 0.3980 - regression_loss: 0.3250 - classification_loss: 0.0730
 623/3000 [=====>........................] - ETA: 16:17 - loss: 0.3982 - regression_loss: 0.3252 - classification_loss: 0.0730
 624/3000 [=====>........................] - ETA: 16:16 - loss: 0.3980 - regression_loss: 0.3250 - classification_loss: 0.0730
 625/3000 [=====>........................] - ETA: 16:16 - loss: 0.3977 - regression_loss: 0.3248 - classification_loss: 0.0729
 626/3000 [=====>........................] - ETA: 16:16 - loss: 0.3977 - regression_loss: 0.3247 - classification_loss: 0.0729
 627/3000 [=====>........................] - ETA: 16:15 - loss: 0.3975 - regression_loss: 0.3247 - classification_loss: 0.0729
 628/3000 [=====>........................] - ETA: 16:15 - loss: 0.3973 - regression_loss: 0.3245 - classification_loss: 0.0728
 629/3000 [=====>........................] - ETA: 16:14 - loss: 0.3971 - regression_loss: 0.3243 - classification_loss: 0.0728
 630/3000 [=====>........................] - ETA: 16:14 - loss: 0.3969 - regression_loss: 0.3242 - classification_loss: 0.0727
 631/3000 [=====>........................] - ETA: 16:14 - loss: 0.3969 - regression_loss: 0.3242 - classification_loss: 0.0727
 632/3000 [=====>........................] - ETA: 16:13 - loss: 0.3968 - regression_loss: 0.3241 - classification_loss: 0.0727
 633/3000 [=====>........................] - ETA: 16:13 - loss: 0.3964 - regression_loss: 0.3238 - classification_loss: 0.0726
 634/3000 [=====>........................] - ETA: 16:12 - loss: 0.3964 - regression_loss: 0.3238 - classification_loss: 0.0726
 635/3000 [=====>........................] - ETA: 16:12 - loss: 0.3966 - regression_loss: 0.3237 - classification_loss: 0.0729
 636/3000 [=====>........................] - ETA: 16:12 - loss: 0.3964 - regression_loss: 0.3236 - classification_loss: 0.0728
 637/3000 [=====>........................] - ETA: 16:11 - loss: 0.3961 - regression_loss: 0.3233 - classification_loss: 0.0727
 638/3000 [=====>........................] - ETA: 16:11 - loss: 0.3966 - regression_loss: 0.3238 - classification_loss: 0.0728
 639/3000 [=====>........................] - ETA: 16:10 - loss: 0.3964 - regression_loss: 0.3237 - classification_loss: 0.0727
 640/3000 [=====>........................] - ETA: 16:10 - loss: 0.3966 - regression_loss: 0.3238 - classification_loss: 0.0728
 641/3000 [=====>........................] - ETA: 16:09 - loss: 0.3965 - regression_loss: 0.3237 - classification_loss: 0.0728
 642/3000 [=====>........................] - ETA: 16:09 - loss: 0.3963 - regression_loss: 0.3236 - classification_loss: 0.0727
 643/3000 [=====>........................] - ETA: 16:09 - loss: 0.3962 - regression_loss: 0.3235 - classification_loss: 0.0727
 644/3000 [=====>........................] - ETA: 16:08 - loss: 0.3960 - regression_loss: 0.3234 - classification_loss: 0.0726
 645/3000 [=====>........................] - ETA: 16:08 - loss: 0.3962 - regression_loss: 0.3236 - classification_loss: 0.0726
 646/3000 [=====>........................] - ETA: 16:08 - loss: 0.3965 - regression_loss: 0.3239 - classification_loss: 0.0726
 647/3000 [=====>........................] - ETA: 16:07 - loss: 0.3963 - regression_loss: 0.3237 - classification_loss: 0.0726
 648/3000 [=====>........................] - ETA: 16:07 - loss: 0.3965 - regression_loss: 0.3239 - classification_loss: 0.0726
 649/3000 [=====>........................] - ETA: 16:06 - loss: 0.3967 - regression_loss: 0.3240 - classification_loss: 0.0726
 650/3000 [=====>........................] - ETA: 16:06 - loss: 0.3969 - regression_loss: 0.3243 - classification_loss: 0.0726
 651/3000 [=====>........................] - ETA: 16:05 - loss: 0.3966 - regression_loss: 0.3241 - classification_loss: 0.0726
 652/3000 [=====>........................] - ETA: 16:05 - loss: 0.3965 - regression_loss: 0.3239 - classification_loss: 0.0726
 653/3000 [=====>........................] - ETA: 16:05 - loss: 0.3964 - regression_loss: 0.3239 - classification_loss: 0.0725
 654/3000 [=====>........................] - ETA: 16:04 - loss: 0.3962 - regression_loss: 0.3237 - classification_loss: 0.0725
 655/3000 [=====>........................] - ETA: 16:04 - loss: 0.3959 - regression_loss: 0.3235 - classification_loss: 0.0724
 656/3000 [=====>........................] - ETA: 16:03 - loss: 0.3957 - regression_loss: 0.3233 - classification_loss: 0.0724
 657/3000 [=====>........................] - ETA: 16:03 - loss: 0.3960 - regression_loss: 0.3237 - classification_loss: 0.0724
 658/3000 [=====>........................] - ETA: 16:03 - loss: 0.3958 - regression_loss: 0.3234 - classification_loss: 0.0723
 659/3000 [=====>........................] - ETA: 16:02 - loss: 0.3956 - regression_loss: 0.3233 - classification_loss: 0.0723
 660/3000 [=====>........................] - ETA: 16:02 - loss: 0.3952 - regression_loss: 0.3230 - classification_loss: 0.0722
 661/3000 [=====>........................] - ETA: 16:01 - loss: 0.3951 - regression_loss: 0.3229 - classification_loss: 0.0722
 662/3000 [=====>........................] - ETA: 16:01 - loss: 0.3951 - regression_loss: 0.3229 - classification_loss: 0.0722
 663/3000 [=====>........................] - ETA: 16:00 - loss: 0.3953 - regression_loss: 0.3231 - classification_loss: 0.0722
 664/3000 [=====>........................] - ETA: 16:00 - loss: 0.3958 - regression_loss: 0.3236 - classification_loss: 0.0722
 665/3000 [=====>........................] - ETA: 16:00 - loss: 0.3959 - regression_loss: 0.3237 - classification_loss: 0.0722
 666/3000 [=====>........................] - ETA: 15:59 - loss: 0.3957 - regression_loss: 0.3235 - classification_loss: 0.0721
 667/3000 [=====>........................] - ETA: 15:59 - loss: 0.3954 - regression_loss: 0.3233 - classification_loss: 0.0721
 668/3000 [=====>........................] - ETA: 15:58 - loss: 0.3952 - regression_loss: 0.3231 - classification_loss: 0.0721
 669/3000 [=====>........................] - ETA: 15:58 - loss: 0.3958 - regression_loss: 0.3237 - classification_loss: 0.0721
 670/3000 [=====>........................] - ETA: 15:57 - loss: 0.3955 - regression_loss: 0.3235 - classification_loss: 0.0720
 671/3000 [=====>........................] - ETA: 15:57 - loss: 0.3954 - regression_loss: 0.3234 - classification_loss: 0.0720
 672/3000 [=====>........................] - ETA: 15:57 - loss: 0.3952 - regression_loss: 0.3233 - classification_loss: 0.0720
 673/3000 [=====>........................] - ETA: 15:56 - loss: 0.3951 - regression_loss: 0.3232 - classification_loss: 0.0719
 674/3000 [=====>........................] - ETA: 15:56 - loss: 0.3951 - regression_loss: 0.3232 - classification_loss: 0.0719
 675/3000 [=====>........................] - ETA: 15:55 - loss: 0.3954 - regression_loss: 0.3235 - classification_loss: 0.0719
 676/3000 [=====>........................] - ETA: 15:55 - loss: 0.3951 - regression_loss: 0.3233 - classification_loss: 0.0719
 677/3000 [=====>........................] - ETA: 15:54 - loss: 0.3949 - regression_loss: 0.3231 - classification_loss: 0.0718
 678/3000 [=====>........................] - ETA: 15:54 - loss: 0.3947 - regression_loss: 0.3230 - classification_loss: 0.0717
 679/3000 [=====>........................] - ETA: 15:54 - loss: 0.3953 - regression_loss: 0.3235 - classification_loss: 0.0718
 680/3000 [=====>........................] - ETA: 15:53 - loss: 0.3955 - regression_loss: 0.3238 - classification_loss: 0.0717
 681/3000 [=====>........................] - ETA: 15:53 - loss: 0.3956 - regression_loss: 0.3240 - classification_loss: 0.0717
 682/3000 [=====>........................] - ETA: 15:52 - loss: 0.3955 - regression_loss: 0.3239 - classification_loss: 0.0716
 683/3000 [=====>........................] - ETA: 15:52 - loss: 0.3954 - regression_loss: 0.3238 - classification_loss: 0.0715
 684/3000 [=====>........................] - ETA: 15:52 - loss: 0.3952 - regression_loss: 0.3237 - classification_loss: 0.0715
 685/3000 [=====>........................] - ETA: 15:51 - loss: 0.3950 - regression_loss: 0.3235 - classification_loss: 0.0715
 686/3000 [=====>........................] - ETA: 15:51 - loss: 0.3953 - regression_loss: 0.3239 - classification_loss: 0.0714
 687/3000 [=====>........................] - ETA: 15:50 - loss: 0.3953 - regression_loss: 0.3239 - classification_loss: 0.0714
 688/3000 [=====>........................] - ETA: 15:50 - loss: 0.3951 - regression_loss: 0.3237 - classification_loss: 0.0714
 689/3000 [=====>........................] - ETA: 15:50 - loss: 0.3950 - regression_loss: 0.3236 - classification_loss: 0.0713
 690/3000 [=====>........................] - ETA: 15:49 - loss: 0.3949 - regression_loss: 0.3236 - classification_loss: 0.0713
 691/3000 [=====>........................] - ETA: 15:49 - loss: 0.3951 - regression_loss: 0.3238 - classification_loss: 0.0713
 692/3000 [=====>........................] - ETA: 15:48 - loss: 0.3949 - regression_loss: 0.3237 - classification_loss: 0.0712
 693/3000 [=====>........................] - ETA: 15:48 - loss: 0.3955 - regression_loss: 0.3243 - classification_loss: 0.0712
 694/3000 [=====>........................] - ETA: 15:48 - loss: 0.3954 - regression_loss: 0.3242 - classification_loss: 0.0711
 695/3000 [=====>........................] - ETA: 15:47 - loss: 0.3953 - regression_loss: 0.3242 - classification_loss: 0.0711
 696/3000 [=====>........................] - ETA: 15:47 - loss: 0.3954 - regression_loss: 0.3243 - classification_loss: 0.0711
 697/3000 [=====>........................] - ETA: 15:46 - loss: 0.3952 - regression_loss: 0.3241 - classification_loss: 0.0711
 698/3000 [=====>........................] - ETA: 15:46 - loss: 0.3951 - regression_loss: 0.3240 - classification_loss: 0.0711
 699/3000 [=====>........................] - ETA: 15:46 - loss: 0.3949 - regression_loss: 0.3238 - classification_loss: 0.0711
 700/3000 [======>.......................] - ETA: 15:45 - loss: 0.3950 - regression_loss: 0.3239 - classification_loss: 0.0711
 701/3000 [======>.......................] - ETA: 15:45 - loss: 0.3952 - regression_loss: 0.3242 - classification_loss: 0.0711
 702/3000 [======>.......................] - ETA: 15:44 - loss: 0.3953 - regression_loss: 0.3242 - classification_loss: 0.0710
 703/3000 [======>.......................] - ETA: 15:44 - loss: 0.3953 - regression_loss: 0.3241 - classification_loss: 0.0711
 704/3000 [======>.......................] - ETA: 15:44 - loss: 0.3955 - regression_loss: 0.3244 - classification_loss: 0.0711
 705/3000 [======>.......................] - ETA: 15:43 - loss: 0.3954 - regression_loss: 0.3243 - classification_loss: 0.0711
 706/3000 [======>.......................] - ETA: 15:43 - loss: 0.3956 - regression_loss: 0.3244 - classification_loss: 0.0712
 707/3000 [======>.......................] - ETA: 15:42 - loss: 0.3953 - regression_loss: 0.3242 - classification_loss: 0.0711
 708/3000 [======>.......................] - ETA: 15:42 - loss: 0.3953 - regression_loss: 0.3242 - classification_loss: 0.0711
 709/3000 [======>.......................] - ETA: 15:41 - loss: 0.3953 - regression_loss: 0.3242 - classification_loss: 0.0711
 710/3000 [======>.......................] - ETA: 15:41 - loss: 0.3957 - regression_loss: 0.3244 - classification_loss: 0.0712
 711/3000 [======>.......................] - ETA: 15:41 - loss: 0.3957 - regression_loss: 0.3245 - classification_loss: 0.0712
 712/3000 [======>.......................] - ETA: 15:40 - loss: 0.3955 - regression_loss: 0.3243 - classification_loss: 0.0712
 713/3000 [======>.......................] - ETA: 15:40 - loss: 0.3955 - regression_loss: 0.3243 - classification_loss: 0.0712
 714/3000 [======>.......................] - ETA: 15:39 - loss: 0.3955 - regression_loss: 0.3244 - classification_loss: 0.0711
 715/3000 [======>.......................] - ETA: 15:39 - loss: 0.3954 - regression_loss: 0.3243 - classification_loss: 0.0711
 716/3000 [======>.......................] - ETA: 15:39 - loss: 0.3954 - regression_loss: 0.3242 - classification_loss: 0.0712
 717/3000 [======>.......................] - ETA: 15:38 - loss: 0.3953 - regression_loss: 0.3242 - classification_loss: 0.0711
 718/3000 [======>.......................] - ETA: 15:38 - loss: 0.3954 - regression_loss: 0.3242 - classification_loss: 0.0712
 719/3000 [======>.......................] - ETA: 15:37 - loss: 0.3959 - regression_loss: 0.3247 - classification_loss: 0.0712
 720/3000 [======>.......................] - ETA: 15:37 - loss: 0.3959 - regression_loss: 0.3247 - classification_loss: 0.0712
 721/3000 [======>.......................] - ETA: 15:37 - loss: 0.3959 - regression_loss: 0.3247 - classification_loss: 0.0711
 722/3000 [======>.......................] - ETA: 15:36 - loss: 0.3957 - regression_loss: 0.3245 - classification_loss: 0.0712
 723/3000 [======>.......................] - ETA: 15:36 - loss: 0.3959 - regression_loss: 0.3247 - classification_loss: 0.0712
 724/3000 [======>.......................] - ETA: 15:35 - loss: 0.3958 - regression_loss: 0.3247 - classification_loss: 0.0711
 725/3000 [======>.......................] - ETA: 15:35 - loss: 0.3959 - regression_loss: 0.3248 - classification_loss: 0.0711
 726/3000 [======>.......................] - ETA: 15:34 - loss: 0.3960 - regression_loss: 0.3249 - classification_loss: 0.0711
 727/3000 [======>.......................] - ETA: 15:34 - loss: 0.3959 - regression_loss: 0.3248 - classification_loss: 0.0711
 728/3000 [======>.......................] - ETA: 15:34 - loss: 0.3961 - regression_loss: 0.3250 - classification_loss: 0.0710
 729/3000 [======>.......................] - ETA: 15:33 - loss: 0.3958 - regression_loss: 0.3248 - classification_loss: 0.0710
 730/3000 [======>.......................] - ETA: 15:33 - loss: 0.3959 - regression_loss: 0.3250 - classification_loss: 0.0710
 731/3000 [======>.......................] - ETA: 15:33 - loss: 0.3962 - regression_loss: 0.3252 - classification_loss: 0.0710
 732/3000 [======>.......................] - ETA: 15:32 - loss: 0.3962 - regression_loss: 0.3253 - classification_loss: 0.0709
 733/3000 [======>.......................] - ETA: 15:32 - loss: 0.3964 - regression_loss: 0.3255 - classification_loss: 0.0709
 734/3000 [======>.......................] - ETA: 15:31 - loss: 0.3966 - regression_loss: 0.3257 - classification_loss: 0.0709
 735/3000 [======>.......................] - ETA: 15:31 - loss: 0.3967 - regression_loss: 0.3259 - classification_loss: 0.0708
 736/3000 [======>.......................] - ETA: 15:30 - loss: 0.3965 - regression_loss: 0.3258 - classification_loss: 0.0708
 737/3000 [======>.......................] - ETA: 15:30 - loss: 0.3965 - regression_loss: 0.3257 - classification_loss: 0.0707
 738/3000 [======>.......................] - ETA: 15:30 - loss: 0.3964 - regression_loss: 0.3257 - classification_loss: 0.0707
 739/3000 [======>.......................] - ETA: 15:29 - loss: 0.3965 - regression_loss: 0.3258 - classification_loss: 0.0707
 740/3000 [======>.......................] - ETA: 15:29 - loss: 0.3966 - regression_loss: 0.3259 - classification_loss: 0.0707
 741/3000 [======>.......................] - ETA: 15:28 - loss: 0.3964 - regression_loss: 0.3258 - classification_loss: 0.0706
 742/3000 [======>.......................] - ETA: 15:28 - loss: 0.3964 - regression_loss: 0.3258 - classification_loss: 0.0706
 743/3000 [======>.......................] - ETA: 15:28 - loss: 0.3962 - regression_loss: 0.3256 - classification_loss: 0.0705
 744/3000 [======>.......................] - ETA: 15:27 - loss: 0.3962 - regression_loss: 0.3256 - classification_loss: 0.0705
 745/3000 [======>.......................] - ETA: 15:27 - loss: 0.3959 - regression_loss: 0.3255 - classification_loss: 0.0705
 746/3000 [======>.......................] - ETA: 15:26 - loss: 0.3957 - regression_loss: 0.3253 - classification_loss: 0.0704
 747/3000 [======>.......................] - ETA: 15:26 - loss: 0.3955 - regression_loss: 0.3251 - classification_loss: 0.0704
 748/3000 [======>.......................] - ETA: 15:26 - loss: 0.3955 - regression_loss: 0.3251 - classification_loss: 0.0703
 749/3000 [======>.......................] - ETA: 15:25 - loss: 0.3952 - regression_loss: 0.3250 - classification_loss: 0.0703
 750/3000 [======>.......................] - ETA: 15:25 - loss: 0.3955 - regression_loss: 0.3252 - classification_loss: 0.0702
 751/3000 [======>.......................] - ETA: 15:24 - loss: 0.3954 - regression_loss: 0.3251 - classification_loss: 0.0703
 752/3000 [======>.......................] - ETA: 15:24 - loss: 0.3952 - regression_loss: 0.3251 - classification_loss: 0.0702
 753/3000 [======>.......................] - ETA: 15:24 - loss: 0.3951 - regression_loss: 0.3249 - classification_loss: 0.0701
 754/3000 [======>.......................] - ETA: 15:23 - loss: 0.3949 - regression_loss: 0.3248 - classification_loss: 0.0701
 755/3000 [======>.......................] - ETA: 15:23 - loss: 0.3950 - regression_loss: 0.3249 - classification_loss: 0.0701
 756/3000 [======>.......................] - ETA: 15:22 - loss: 0.3949 - regression_loss: 0.3249 - classification_loss: 0.0700
 757/3000 [======>.......................] - ETA: 15:22 - loss: 0.3946 - regression_loss: 0.3247 - classification_loss: 0.0700
 758/3000 [======>.......................] - ETA: 15:22 - loss: 0.3946 - regression_loss: 0.3247 - classification_loss: 0.0699
 759/3000 [======>.......................] - ETA: 15:21 - loss: 0.3947 - regression_loss: 0.3248 - classification_loss: 0.0699
 760/3000 [======>.......................] - ETA: 15:21 - loss: 0.3945 - regression_loss: 0.3247 - classification_loss: 0.0698
 761/3000 [======>.......................] - ETA: 15:20 - loss: 0.3946 - regression_loss: 0.3248 - classification_loss: 0.0698
 762/3000 [======>.......................] - ETA: 15:20 - loss: 0.3944 - regression_loss: 0.3246 - classification_loss: 0.0698
 763/3000 [======>.......................] - ETA: 15:19 - loss: 0.3944 - regression_loss: 0.3247 - classification_loss: 0.0697
 764/3000 [======>.......................] - ETA: 15:19 - loss: 0.3946 - regression_loss: 0.3248 - classification_loss: 0.0698
 765/3000 [======>.......................] - ETA: 15:19 - loss: 0.3944 - regression_loss: 0.3247 - classification_loss: 0.0698
 766/3000 [======>.......................] - ETA: 15:18 - loss: 0.3945 - regression_loss: 0.3247 - classification_loss: 0.0698
 767/3000 [======>.......................] - ETA: 15:18 - loss: 0.3946 - regression_loss: 0.3248 - classification_loss: 0.0697
 768/3000 [======>.......................] - ETA: 15:17 - loss: 0.3946 - regression_loss: 0.3249 - classification_loss: 0.0697
 769/3000 [======>.......................] - ETA: 15:17 - loss: 0.3944 - regression_loss: 0.3247 - classification_loss: 0.0697
 770/3000 [======>.......................] - ETA: 15:17 - loss: 0.3945 - regression_loss: 0.3248 - classification_loss: 0.0696
 771/3000 [======>.......................] - ETA: 15:16 - loss: 0.3943 - regression_loss: 0.3247 - classification_loss: 0.0696
 772/3000 [======>.......................] - ETA: 15:16 - loss: 0.3942 - regression_loss: 0.3246 - classification_loss: 0.0696
 773/3000 [======>.......................] - ETA: 15:15 - loss: 0.3941 - regression_loss: 0.3246 - classification_loss: 0.0695
 774/3000 [======>.......................] - ETA: 15:15 - loss: 0.3941 - regression_loss: 0.3247 - classification_loss: 0.0695
 775/3000 [======>.......................] - ETA: 15:15 - loss: 0.3943 - regression_loss: 0.3248 - classification_loss: 0.0695
 776/3000 [======>.......................] - ETA: 15:14 - loss: 0.3941 - regression_loss: 0.3247 - classification_loss: 0.0694
 777/3000 [======>.......................] - ETA: 15:14 - loss: 0.3941 - regression_loss: 0.3247 - classification_loss: 0.0694
 778/3000 [======>.......................] - ETA: 15:13 - loss: 0.3940 - regression_loss: 0.3247 - classification_loss: 0.0693
 779/3000 [======>.......................] - ETA: 15:13 - loss: 0.3937 - regression_loss: 0.3244 - classification_loss: 0.0693
 780/3000 [======>.......................] - ETA: 15:12 - loss: 0.3935 - regression_loss: 0.3243 - classification_loss: 0.0692
 781/3000 [======>.......................] - ETA: 15:12 - loss: 0.3935 - regression_loss: 0.3243 - classification_loss: 0.0692
 782/3000 [======>.......................] - ETA: 15:12 - loss: 0.3933 - regression_loss: 0.3241 - classification_loss: 0.0692
 783/3000 [======>.......................] - ETA: 15:11 - loss: 0.3932 - regression_loss: 0.3241 - classification_loss: 0.0691
 784/3000 [======>.......................] - ETA: 15:11 - loss: 0.3931 - regression_loss: 0.3240 - classification_loss: 0.0691
 785/3000 [======>.......................] - ETA: 15:10 - loss: 0.3930 - regression_loss: 0.3240 - classification_loss: 0.0690
 786/3000 [======>.......................] - ETA: 15:10 - loss: 0.3927 - regression_loss: 0.3238 - classification_loss: 0.0690
 787/3000 [======>.......................] - ETA: 15:09 - loss: 0.3925 - regression_loss: 0.3236 - classification_loss: 0.0689
 788/3000 [======>.......................] - ETA: 15:09 - loss: 0.3927 - regression_loss: 0.3238 - classification_loss: 0.0689
 789/3000 [======>.......................] - ETA: 15:09 - loss: 0.3927 - regression_loss: 0.3238 - classification_loss: 0.0689
 790/3000 [======>.......................] - ETA: 15:08 - loss: 0.3924 - regression_loss: 0.3236 - classification_loss: 0.0689
 791/3000 [======>.......................] - ETA: 15:08 - loss: 0.3922 - regression_loss: 0.3234 - classification_loss: 0.0688
 792/3000 [======>.......................] - ETA: 15:07 - loss: 0.3920 - regression_loss: 0.3232 - classification_loss: 0.0688
 793/3000 [======>.......................] - ETA: 15:07 - loss: 0.3920 - regression_loss: 0.3232 - classification_loss: 0.0688
 794/3000 [======>.......................] - ETA: 15:07 - loss: 0.3917 - regression_loss: 0.3230 - classification_loss: 0.0687
 795/3000 [======>.......................] - ETA: 15:06 - loss: 0.3916 - regression_loss: 0.3229 - classification_loss: 0.0687
 796/3000 [======>.......................] - ETA: 15:06 - loss: 0.3921 - regression_loss: 0.3234 - classification_loss: 0.0686
 797/3000 [======>.......................] - ETA: 15:05 - loss: 0.3922 - regression_loss: 0.3236 - classification_loss: 0.0686
 798/3000 [======>.......................] - ETA: 15:05 - loss: 0.3922 - regression_loss: 0.3236 - classification_loss: 0.0685
 799/3000 [======>.......................] - ETA: 15:05 - loss: 0.3921 - regression_loss: 0.3236 - classification_loss: 0.0685
 800/3000 [=======>......................] - ETA: 15:04 - loss: 0.3921 - regression_loss: 0.3236 - classification_loss: 0.0685
 801/3000 [=======>......................] - ETA: 15:04 - loss: 0.3921 - regression_loss: 0.3237 - classification_loss: 0.0684
 802/3000 [=======>......................] - ETA: 15:03 - loss: 0.3919 - regression_loss: 0.3235 - classification_loss: 0.0684
 803/3000 [=======>......................] - ETA: 15:03 - loss: 0.3918 - regression_loss: 0.3234 - classification_loss: 0.0684
 804/3000 [=======>......................] - ETA: 15:02 - loss: 0.3919 - regression_loss: 0.3236 - classification_loss: 0.0683
 805/3000 [=======>......................] - ETA: 15:02 - loss: 0.3920 - regression_loss: 0.3237 - classification_loss: 0.0683
 806/3000 [=======>......................] - ETA: 15:02 - loss: 0.3920 - regression_loss: 0.3237 - classification_loss: 0.0683
 807/3000 [=======>......................] - ETA: 15:01 - loss: 0.3916 - regression_loss: 0.3234 - classification_loss: 0.0682
 808/3000 [=======>......................] - ETA: 15:01 - loss: 0.3914 - regression_loss: 0.3232 - classification_loss: 0.0682
 809/3000 [=======>......................] - ETA: 15:01 - loss: 0.3915 - regression_loss: 0.3233 - classification_loss: 0.0682
 810/3000 [=======>......................] - ETA: 15:00 - loss: 0.3913 - regression_loss: 0.3231 - classification_loss: 0.0681
 811/3000 [=======>......................] - ETA: 15:00 - loss: 0.3913 - regression_loss: 0.3231 - classification_loss: 0.0682
 812/3000 [=======>......................] - ETA: 14:59 - loss: 0.3912 - regression_loss: 0.3230 - classification_loss: 0.0681
 813/3000 [=======>......................] - ETA: 14:59 - loss: 0.3910 - regression_loss: 0.3229 - classification_loss: 0.0681
 814/3000 [=======>......................] - ETA: 14:58 - loss: 0.3911 - regression_loss: 0.3230 - classification_loss: 0.0681
 815/3000 [=======>......................] - ETA: 14:58 - loss: 0.3909 - regression_loss: 0.3229 - classification_loss: 0.0680
 816/3000 [=======>......................] - ETA: 14:57 - loss: 0.3907 - regression_loss: 0.3227 - classification_loss: 0.0680
 817/3000 [=======>......................] - ETA: 14:57 - loss: 0.3907 - regression_loss: 0.3228 - classification_loss: 0.0679
 818/3000 [=======>......................] - ETA: 14:57 - loss: 0.3905 - regression_loss: 0.3226 - classification_loss: 0.0679
 819/3000 [=======>......................] - ETA: 14:56 - loss: 0.3903 - regression_loss: 0.3225 - classification_loss: 0.0679
 820/3000 [=======>......................] - ETA: 14:56 - loss: 0.3900 - regression_loss: 0.3222 - classification_loss: 0.0678
 821/3000 [=======>......................] - ETA: 14:55 - loss: 0.3901 - regression_loss: 0.3223 - classification_loss: 0.0678
 822/3000 [=======>......................] - ETA: 14:55 - loss: 0.3900 - regression_loss: 0.3223 - classification_loss: 0.0678
 823/3000 [=======>......................] - ETA: 14:55 - loss: 0.3900 - regression_loss: 0.3223 - classification_loss: 0.0677
 824/3000 [=======>......................] - ETA: 14:54 - loss: 0.3900 - regression_loss: 0.3223 - classification_loss: 0.0677
 825/3000 [=======>......................] - ETA: 14:54 - loss: 0.3903 - regression_loss: 0.3227 - classification_loss: 0.0677
 826/3000 [=======>......................] - ETA: 14:53 - loss: 0.3902 - regression_loss: 0.3226 - classification_loss: 0.0676
 827/3000 [=======>......................] - ETA: 14:53 - loss: 0.3899 - regression_loss: 0.3223 - classification_loss: 0.0675
 828/3000 [=======>......................] - ETA: 14:52 - loss: 0.3898 - regression_loss: 0.3224 - classification_loss: 0.0675
 829/3000 [=======>......................] - ETA: 14:52 - loss: 0.3896 - regression_loss: 0.3222 - classification_loss: 0.0674
 830/3000 [=======>......................] - ETA: 14:52 - loss: 0.3897 - regression_loss: 0.3222 - classification_loss: 0.0674
 831/3000 [=======>......................] - ETA: 14:51 - loss: 0.3897 - regression_loss: 0.3223 - classification_loss: 0.0674
 832/3000 [=======>......................] - ETA: 14:51 - loss: 0.3895 - regression_loss: 0.3222 - classification_loss: 0.0673
 833/3000 [=======>......................] - ETA: 14:50 - loss: 0.3895 - regression_loss: 0.3222 - classification_loss: 0.0673
 834/3000 [=======>......................] - ETA: 14:50 - loss: 0.3893 - regression_loss: 0.3221 - classification_loss: 0.0672
 835/3000 [=======>......................] - ETA: 14:50 - loss: 0.3895 - regression_loss: 0.3223 - classification_loss: 0.0672
 836/3000 [=======>......................] - ETA: 14:49 - loss: 0.3894 - regression_loss: 0.3221 - classification_loss: 0.0672
 837/3000 [=======>......................] - ETA: 14:49 - loss: 0.3894 - regression_loss: 0.3222 - classification_loss: 0.0672
 838/3000 [=======>......................] - ETA: 14:48 - loss: 0.3893 - regression_loss: 0.3221 - classification_loss: 0.0671
 839/3000 [=======>......................] - ETA: 14:48 - loss: 0.3894 - regression_loss: 0.3223 - classification_loss: 0.0671
 840/3000 [=======>......................] - ETA: 14:47 - loss: 0.3892 - regression_loss: 0.3222 - classification_loss: 0.0671
 841/3000 [=======>......................] - ETA: 14:47 - loss: 0.3890 - regression_loss: 0.3220 - classification_loss: 0.0670
 842/3000 [=======>......................] - ETA: 14:47 - loss: 0.3887 - regression_loss: 0.3217 - classification_loss: 0.0670
 843/3000 [=======>......................] - ETA: 14:46 - loss: 0.3886 - regression_loss: 0.3217 - classification_loss: 0.0669
 844/3000 [=======>......................] - ETA: 14:46 - loss: 0.3885 - regression_loss: 0.3216 - classification_loss: 0.0669
 845/3000 [=======>......................] - ETA: 14:45 - loss: 0.3884 - regression_loss: 0.3215 - classification_loss: 0.0669
 846/3000 [=======>......................] - ETA: 14:45 - loss: 0.3882 - regression_loss: 0.3213 - classification_loss: 0.0669
 847/3000 [=======>......................] - ETA: 14:45 - loss: 0.3880 - regression_loss: 0.3212 - classification_loss: 0.0668
 848/3000 [=======>......................] - ETA: 14:44 - loss: 0.3883 - regression_loss: 0.3214 - classification_loss: 0.0669
 849/3000 [=======>......................] - ETA: 14:44 - loss: 0.3881 - regression_loss: 0.3213 - classification_loss: 0.0668
 850/3000 [=======>......................] - ETA: 14:43 - loss: 0.3879 - regression_loss: 0.3211 - classification_loss: 0.0668
 851/3000 [=======>......................] - ETA: 14:43 - loss: 0.3878 - regression_loss: 0.3211 - classification_loss: 0.0667
 852/3000 [=======>......................] - ETA: 14:42 - loss: 0.3878 - regression_loss: 0.3212 - classification_loss: 0.0667
 853/3000 [=======>......................] - ETA: 14:42 - loss: 0.3878 - regression_loss: 0.3212 - classification_loss: 0.0666
 854/3000 [=======>......................] - ETA: 14:42 - loss: 0.3876 - regression_loss: 0.3211 - classification_loss: 0.0666
 855/3000 [=======>......................] - ETA: 14:41 - loss: 0.3877 - regression_loss: 0.3211 - classification_loss: 0.0666
 856/3000 [=======>......................] - ETA: 14:41 - loss: 0.3876 - regression_loss: 0.3210 - classification_loss: 0.0666
 857/3000 [=======>......................] - ETA: 14:40 - loss: 0.3874 - regression_loss: 0.3208 - classification_loss: 0.0665
 858/3000 [=======>......................] - ETA: 14:40 - loss: 0.3875 - regression_loss: 0.3210 - classification_loss: 0.0665
 859/3000 [=======>......................] - ETA: 14:40 - loss: 0.3878 - regression_loss: 0.3212 - classification_loss: 0.0666
 860/3000 [=======>......................] - ETA: 14:39 - loss: 0.3878 - regression_loss: 0.3211 - classification_loss: 0.0666
 861/3000 [=======>......................] - ETA: 14:39 - loss: 0.3878 - regression_loss: 0.3212 - classification_loss: 0.0666
 862/3000 [=======>......................] - ETA: 14:38 - loss: 0.3879 - regression_loss: 0.3213 - classification_loss: 0.0666
 863/3000 [=======>......................] - ETA: 14:38 - loss: 0.3881 - regression_loss: 0.3214 - classification_loss: 0.0667
 864/3000 [=======>......................] - ETA: 14:38 - loss: 0.3881 - regression_loss: 0.3215 - classification_loss: 0.0666
 865/3000 [=======>......................] - ETA: 14:37 - loss: 0.3879 - regression_loss: 0.3213 - classification_loss: 0.0666
 866/3000 [=======>......................] - ETA: 14:37 - loss: 0.3877 - regression_loss: 0.3211 - classification_loss: 0.0666
 867/3000 [=======>......................] - ETA: 14:36 - loss: 0.3878 - regression_loss: 0.3212 - classification_loss: 0.0665
 868/3000 [=======>......................] - ETA: 14:36 - loss: 0.3879 - regression_loss: 0.3214 - classification_loss: 0.0665
 869/3000 [=======>......................] - ETA: 14:36 - loss: 0.3878 - regression_loss: 0.3213 - classification_loss: 0.0665
 870/3000 [=======>......................] - ETA: 14:35 - loss: 0.3879 - regression_loss: 0.3214 - classification_loss: 0.0665
 871/3000 [=======>......................] - ETA: 14:35 - loss: 0.3879 - regression_loss: 0.3214 - classification_loss: 0.0665
 872/3000 [=======>......................] - ETA: 14:34 - loss: 0.3877 - regression_loss: 0.3213 - classification_loss: 0.0665
 873/3000 [=======>......................] - ETA: 14:34 - loss: 0.3877 - regression_loss: 0.3213 - classification_loss: 0.0664
 874/3000 [=======>......................] - ETA: 14:34 - loss: 0.3876 - regression_loss: 0.3212 - classification_loss: 0.0664
 875/3000 [=======>......................] - ETA: 14:33 - loss: 0.3875 - regression_loss: 0.3211 - classification_loss: 0.0664
 876/3000 [=======>......................] - ETA: 14:33 - loss: 0.3874 - regression_loss: 0.3210 - classification_loss: 0.0664
 877/3000 [=======>......................] - ETA: 14:32 - loss: 0.3872 - regression_loss: 0.3208 - classification_loss: 0.0664
 878/3000 [=======>......................] - ETA: 14:32 - loss: 0.3871 - regression_loss: 0.3207 - classification_loss: 0.0664
 879/3000 [=======>......................] - ETA: 14:32 - loss: 0.3869 - regression_loss: 0.3205 - classification_loss: 0.0664
 880/3000 [=======>......................] - ETA: 14:31 - loss: 0.3867 - regression_loss: 0.3203 - classification_loss: 0.0664
 881/3000 [=======>......................] - ETA: 14:31 - loss: 0.3866 - regression_loss: 0.3203 - classification_loss: 0.0663
 882/3000 [=======>......................] - ETA: 14:30 - loss: 0.3864 - regression_loss: 0.3201 - classification_loss: 0.0663
 883/3000 [=======>......................] - ETA: 14:30 - loss: 0.3862 - regression_loss: 0.3200 - classification_loss: 0.0662
 884/3000 [=======>......................] - ETA: 14:30 - loss: 0.3860 - regression_loss: 0.3198 - classification_loss: 0.0662
 885/3000 [=======>......................] - ETA: 14:29 - loss: 0.3859 - regression_loss: 0.3197 - classification_loss: 0.0662
 886/3000 [=======>......................] - ETA: 14:29 - loss: 0.3860 - regression_loss: 0.3198 - classification_loss: 0.0662
 887/3000 [=======>......................] - ETA: 14:28 - loss: 0.3862 - regression_loss: 0.3200 - classification_loss: 0.0663
 888/3000 [=======>......................] - ETA: 14:28 - loss: 0.3862 - regression_loss: 0.3199 - classification_loss: 0.0663
 889/3000 [=======>......................] - ETA: 14:27 - loss: 0.3859 - regression_loss: 0.3197 - classification_loss: 0.0662
 890/3000 [=======>......................] - ETA: 14:27 - loss: 0.3860 - regression_loss: 0.3198 - classification_loss: 0.0662
 891/3000 [=======>......................] - ETA: 14:27 - loss: 0.3859 - regression_loss: 0.3197 - classification_loss: 0.0662
 892/3000 [=======>......................] - ETA: 14:26 - loss: 0.3857 - regression_loss: 0.3195 - classification_loss: 0.0662
 893/3000 [=======>......................] - ETA: 14:26 - loss: 0.3855 - regression_loss: 0.3193 - classification_loss: 0.0661
 894/3000 [=======>......................] - ETA: 14:25 - loss: 0.3855 - regression_loss: 0.3194 - classification_loss: 0.0661
 895/3000 [=======>......................] - ETA: 14:25 - loss: 0.3854 - regression_loss: 0.3193 - classification_loss: 0.0661
 896/3000 [=======>......................] - ETA: 14:25 - loss: 0.3853 - regression_loss: 0.3192 - classification_loss: 0.0661
 897/3000 [=======>......................] - ETA: 14:24 - loss: 0.3852 - regression_loss: 0.3192 - classification_loss: 0.0660
 898/3000 [=======>......................] - ETA: 14:24 - loss: 0.3850 - regression_loss: 0.3190 - classification_loss: 0.0660
 899/3000 [=======>......................] - ETA: 14:23 - loss: 0.3852 - regression_loss: 0.3192 - classification_loss: 0.0660
 900/3000 [========>.....................] - ETA: 14:23 - loss: 0.3851 - regression_loss: 0.3191 - classification_loss: 0.0660
 901/3000 [========>.....................] - ETA: 14:23 - loss: 0.3852 - regression_loss: 0.3192 - classification_loss: 0.0660
 902/3000 [========>.....................] - ETA: 14:22 - loss: 0.3855 - regression_loss: 0.3195 - classification_loss: 0.0659
 903/3000 [========>.....................] - ETA: 14:22 - loss: 0.3853 - regression_loss: 0.3194 - classification_loss: 0.0659
 904/3000 [========>.....................] - ETA: 14:21 - loss: 0.3852 - regression_loss: 0.3193 - classification_loss: 0.0659
 905/3000 [========>.....................] - ETA: 14:21 - loss: 0.3854 - regression_loss: 0.3195 - classification_loss: 0.0659
 906/3000 [========>.....................] - ETA: 14:20 - loss: 0.3855 - regression_loss: 0.3196 - classification_loss: 0.0659
 907/3000 [========>.....................] - ETA: 14:20 - loss: 0.3856 - regression_loss: 0.3198 - classification_loss: 0.0658
 908/3000 [========>.....................] - ETA: 14:20 - loss: 0.3855 - regression_loss: 0.3198 - classification_loss: 0.0658
 909/3000 [========>.....................] - ETA: 14:19 - loss: 0.3854 - regression_loss: 0.3196 - classification_loss: 0.0658
 910/3000 [========>.....................] - ETA: 14:19 - loss: 0.3854 - regression_loss: 0.3197 - classification_loss: 0.0657
 911/3000 [========>.....................] - ETA: 14:18 - loss: 0.3853 - regression_loss: 0.3195 - classification_loss: 0.0657
 912/3000 [========>.....................] - ETA: 14:18 - loss: 0.3852 - regression_loss: 0.3195 - classification_loss: 0.0657
 913/3000 [========>.....................] - ETA: 14:18 - loss: 0.3852 - regression_loss: 0.3195 - classification_loss: 0.0657
 914/3000 [========>.....................] - ETA: 14:17 - loss: 0.3849 - regression_loss: 0.3193 - classification_loss: 0.0656
 915/3000 [========>.....................] - ETA: 14:17 - loss: 0.3848 - regression_loss: 0.3192 - classification_loss: 0.0656
 916/3000 [========>.....................] - ETA: 14:16 - loss: 0.3848 - regression_loss: 0.3193 - classification_loss: 0.0655
 917/3000 [========>.....................] - ETA: 14:16 - loss: 0.3846 - regression_loss: 0.3191 - classification_loss: 0.0655
 918/3000 [========>.....................] - ETA: 14:16 - loss: 0.3846 - regression_loss: 0.3191 - classification_loss: 0.0655
 919/3000 [========>.....................] - ETA: 14:15 - loss: 0.3845 - regression_loss: 0.3190 - classification_loss: 0.0654
 920/3000 [========>.....................] - ETA: 14:15 - loss: 0.3844 - regression_loss: 0.3190 - classification_loss: 0.0654
 921/3000 [========>.....................] - ETA: 14:14 - loss: 0.3843 - regression_loss: 0.3189 - classification_loss: 0.0654
 922/3000 [========>.....................] - ETA: 14:14 - loss: 0.3842 - regression_loss: 0.3188 - classification_loss: 0.0653
 923/3000 [========>.....................] - ETA: 14:13 - loss: 0.3840 - regression_loss: 0.3187 - classification_loss: 0.0653
 924/3000 [========>.....................] - ETA: 14:13 - loss: 0.3840 - regression_loss: 0.3187 - classification_loss: 0.0653
 925/3000 [========>.....................] - ETA: 14:13 - loss: 0.3838 - regression_loss: 0.3186 - classification_loss: 0.0652
 926/3000 [========>.....................] - ETA: 14:12 - loss: 0.3837 - regression_loss: 0.3186 - classification_loss: 0.0652
 927/3000 [========>.....................] - ETA: 14:12 - loss: 0.3836 - regression_loss: 0.3185 - classification_loss: 0.0651
 928/3000 [========>.....................] - ETA: 14:11 - loss: 0.3834 - regression_loss: 0.3183 - classification_loss: 0.0651
 929/3000 [========>.....................] - ETA: 14:11 - loss: 0.3832 - regression_loss: 0.3181 - classification_loss: 0.0651
 930/3000 [========>.....................] - ETA: 14:11 - loss: 0.3831 - regression_loss: 0.3181 - classification_loss: 0.0650
 931/3000 [========>.....................] - ETA: 14:10 - loss: 0.3831 - regression_loss: 0.3181 - classification_loss: 0.0650
 932/3000 [========>.....................] - ETA: 14:10 - loss: 0.3833 - regression_loss: 0.3182 - classification_loss: 0.0650
 933/3000 [========>.....................] - ETA: 14:09 - loss: 0.3832 - regression_loss: 0.3182 - classification_loss: 0.0650
 934/3000 [========>.....................] - ETA: 14:09 - loss: 0.3833 - regression_loss: 0.3183 - classification_loss: 0.0650
 935/3000 [========>.....................] - ETA: 14:08 - loss: 0.3833 - regression_loss: 0.3184 - classification_loss: 0.0649
 936/3000 [========>.....................] - ETA: 14:08 - loss: 0.3833 - regression_loss: 0.3183 - classification_loss: 0.0650
 937/3000 [========>.....................] - ETA: 14:08 - loss: 0.3831 - regression_loss: 0.3182 - classification_loss: 0.0649
 938/3000 [========>.....................] - ETA: 14:07 - loss: 0.3831 - regression_loss: 0.3182 - classification_loss: 0.0649
 939/3000 [========>.....................] - ETA: 14:07 - loss: 0.3828 - regression_loss: 0.3180 - classification_loss: 0.0648
 940/3000 [========>.....................] - ETA: 14:06 - loss: 0.3828 - regression_loss: 0.3180 - classification_loss: 0.0648
 941/3000 [========>.....................] - ETA: 14:06 - loss: 0.3828 - regression_loss: 0.3180 - classification_loss: 0.0648
 942/3000 [========>.....................] - ETA: 14:06 - loss: 0.3830 - regression_loss: 0.3182 - classification_loss: 0.0648
 943/3000 [========>.....................] - ETA: 14:05 - loss: 0.3829 - regression_loss: 0.3182 - classification_loss: 0.0648
 944/3000 [========>.....................] - ETA: 14:05 - loss: 0.3830 - regression_loss: 0.3181 - classification_loss: 0.0649
 945/3000 [========>.....................] - ETA: 14:04 - loss: 0.3832 - regression_loss: 0.3183 - classification_loss: 0.0649
 946/3000 [========>.....................] - ETA: 14:04 - loss: 0.3832 - regression_loss: 0.3183 - classification_loss: 0.0649
 947/3000 [========>.....................] - ETA: 14:04 - loss: 0.3832 - regression_loss: 0.3183 - classification_loss: 0.0648
 948/3000 [========>.....................] - ETA: 14:03 - loss: 0.3832 - regression_loss: 0.3184 - classification_loss: 0.0648
 949/3000 [========>.....................] - ETA: 14:03 - loss: 0.3833 - regression_loss: 0.3186 - classification_loss: 0.0648
 950/3000 [========>.....................] - ETA: 14:02 - loss: 0.3831 - regression_loss: 0.3184 - classification_loss: 0.0648
 951/3000 [========>.....................] - ETA: 14:02 - loss: 0.3833 - regression_loss: 0.3185 - classification_loss: 0.0648
 952/3000 [========>.....................] - ETA: 14:02 - loss: 0.3835 - regression_loss: 0.3187 - classification_loss: 0.0648
 953/3000 [========>.....................] - ETA: 14:01 - loss: 0.3836 - regression_loss: 0.3187 - classification_loss: 0.0649
 954/3000 [========>.....................] - ETA: 14:01 - loss: 0.3835 - regression_loss: 0.3186 - classification_loss: 0.0649
 955/3000 [========>.....................] - ETA: 14:00 - loss: 0.3836 - regression_loss: 0.3187 - classification_loss: 0.0649
 956/3000 [========>.....................] - ETA: 14:00 - loss: 0.3836 - regression_loss: 0.3188 - classification_loss: 0.0648
 957/3000 [========>.....................] - ETA: 13:59 - loss: 0.3837 - regression_loss: 0.3189 - classification_loss: 0.0648
 958/3000 [========>.....................] - ETA: 13:59 - loss: 0.3836 - regression_loss: 0.3188 - classification_loss: 0.0648
 959/3000 [========>.....................] - ETA: 13:59 - loss: 0.3834 - regression_loss: 0.3187 - classification_loss: 0.0647
 960/3000 [========>.....................] - ETA: 13:58 - loss: 0.3832 - regression_loss: 0.3185 - classification_loss: 0.0647
 961/3000 [========>.....................] - ETA: 13:58 - loss: 0.3831 - regression_loss: 0.3184 - classification_loss: 0.0647
 962/3000 [========>.....................] - ETA: 13:57 - loss: 0.3831 - regression_loss: 0.3184 - classification_loss: 0.0647
 963/3000 [========>.....................] - ETA: 13:57 - loss: 0.3829 - regression_loss: 0.3182 - classification_loss: 0.0647
 964/3000 [========>.....................] - ETA: 13:57 - loss: 0.3828 - regression_loss: 0.3181 - classification_loss: 0.0647
 965/3000 [========>.....................] - ETA: 13:56 - loss: 0.3826 - regression_loss: 0.3180 - classification_loss: 0.0646
 966/3000 [========>.....................] - ETA: 13:56 - loss: 0.3823 - regression_loss: 0.3177 - classification_loss: 0.0646
 967/3000 [========>.....................] - ETA: 13:55 - loss: 0.3821 - regression_loss: 0.3175 - classification_loss: 0.0646
 968/3000 [========>.....................] - ETA: 13:55 - loss: 0.3821 - regression_loss: 0.3175 - classification_loss: 0.0646
 969/3000 [========>.....................] - ETA: 13:55 - loss: 0.3821 - regression_loss: 0.3175 - classification_loss: 0.0646
 970/3000 [========>.....................] - ETA: 13:54 - loss: 0.3819 - regression_loss: 0.3174 - classification_loss: 0.0646
 971/3000 [========>.....................] - ETA: 13:54 - loss: 0.3817 - regression_loss: 0.3172 - classification_loss: 0.0645
 972/3000 [========>.....................] - ETA: 13:53 - loss: 0.3816 - regression_loss: 0.3171 - classification_loss: 0.0645
 973/3000 [========>.....................] - ETA: 13:53 - loss: 0.3818 - regression_loss: 0.3172 - classification_loss: 0.0645
 974/3000 [========>.....................] - ETA: 13:52 - loss: 0.3816 - regression_loss: 0.3171 - classification_loss: 0.0646
 975/3000 [========>.....................] - ETA: 13:52 - loss: 0.3815 - regression_loss: 0.3170 - classification_loss: 0.0645
 976/3000 [========>.....................] - ETA: 13:52 - loss: 0.3815 - regression_loss: 0.3170 - classification_loss: 0.0645
 977/3000 [========>.....................] - ETA: 13:51 - loss: 0.3814 - regression_loss: 0.3170 - classification_loss: 0.0645
 978/3000 [========>.....................] - ETA: 13:51 - loss: 0.3812 - regression_loss: 0.3168 - classification_loss: 0.0644
 979/3000 [========>.....................] - ETA: 13:50 - loss: 0.3812 - regression_loss: 0.3167 - classification_loss: 0.0644
 980/3000 [========>.....................] - ETA: 13:50 - loss: 0.3815 - regression_loss: 0.3170 - classification_loss: 0.0645
 981/3000 [========>.....................] - ETA: 13:50 - loss: 0.3815 - regression_loss: 0.3170 - classification_loss: 0.0645
 982/3000 [========>.....................] - ETA: 13:49 - loss: 0.3815 - regression_loss: 0.3171 - classification_loss: 0.0644
 983/3000 [========>.....................] - ETA: 13:49 - loss: 0.3816 - regression_loss: 0.3171 - classification_loss: 0.0645
 984/3000 [========>.....................] - ETA: 13:48 - loss: 0.3816 - regression_loss: 0.3171 - classification_loss: 0.0644
 985/3000 [========>.....................] - ETA: 13:48 - loss: 0.3816 - regression_loss: 0.3172 - classification_loss: 0.0644
 986/3000 [========>.....................] - ETA: 13:48 - loss: 0.3815 - regression_loss: 0.3171 - classification_loss: 0.0644
 987/3000 [========>.....................] - ETA: 13:47 - loss: 0.3816 - regression_loss: 0.3172 - classification_loss: 0.0644
 988/3000 [========>.....................] - ETA: 13:47 - loss: 0.3817 - regression_loss: 0.3173 - classification_loss: 0.0644
 989/3000 [========>.....................] - ETA: 13:46 - loss: 0.3817 - regression_loss: 0.3173 - classification_loss: 0.0643
 990/3000 [========>.....................] - ETA: 13:46 - loss: 0.3818 - regression_loss: 0.3175 - classification_loss: 0.0643
 991/3000 [========>.....................] - ETA: 13:46 - loss: 0.3817 - regression_loss: 0.3174 - classification_loss: 0.0643
 992/3000 [========>.....................] - ETA: 13:45 - loss: 0.3814 - regression_loss: 0.3172 - classification_loss: 0.0643
 993/3000 [========>.....................] - ETA: 13:45 - loss: 0.3815 - regression_loss: 0.3172 - classification_loss: 0.0643
 994/3000 [========>.....................] - ETA: 13:44 - loss: 0.3816 - regression_loss: 0.3173 - classification_loss: 0.0643
 995/3000 [========>.....................] - ETA: 13:44 - loss: 0.3814 - regression_loss: 0.3172 - classification_loss: 0.0643
 996/3000 [========>.....................] - ETA: 13:44 - loss: 0.3814 - regression_loss: 0.3172 - classification_loss: 0.0642
 997/3000 [========>.....................] - ETA: 13:43 - loss: 0.3815 - regression_loss: 0.3173 - classification_loss: 0.0642
 998/3000 [========>.....................] - ETA: 13:43 - loss: 0.3814 - regression_loss: 0.3172 - classification_loss: 0.0641
 999/3000 [========>.....................] - ETA: 13:42 - loss: 0.3813 - regression_loss: 0.3172 - classification_loss: 0.0641
1000/3000 [=========>....................] - ETA: 13:42 - loss: 0.3812 - regression_loss: 0.3171 - classification_loss: 0.0641
1001/3000 [=========>....................] - ETA: 13:42 - loss: 0.3811 - regression_loss: 0.3171 - classification_loss: 0.0640
1002/3000 [=========>....................] - ETA: 13:41 - loss: 0.3810 - regression_loss: 0.3170 - classification_loss: 0.0640
1003/3000 [=========>....................] - ETA: 13:41 - loss: 0.3808 - regression_loss: 0.3169 - classification_loss: 0.0640
1004/3000 [=========>....................] - ETA: 13:40 - loss: 0.3810 - regression_loss: 0.3170 - classification_loss: 0.0640
1005/3000 [=========>....................] - ETA: 13:40 - loss: 0.3809 - regression_loss: 0.3170 - classification_loss: 0.0639
1006/3000 [=========>....................] - ETA: 13:39 - loss: 0.3808 - regression_loss: 0.3169 - classification_loss: 0.0639
1007/3000 [=========>....................] - ETA: 13:39 - loss: 0.3807 - regression_loss: 0.3168 - classification_loss: 0.0639
1008/3000 [=========>....................] - ETA: 13:39 - loss: 0.3808 - regression_loss: 0.3169 - classification_loss: 0.0639
1009/3000 [=========>....................] - ETA: 13:38 - loss: 0.3806 - regression_loss: 0.3168 - classification_loss: 0.0638
1010/3000 [=========>....................] - ETA: 13:38 - loss: 0.3805 - regression_loss: 0.3167 - classification_loss: 0.0638
1011/3000 [=========>....................] - ETA: 13:37 - loss: 0.3803 - regression_loss: 0.3165 - classification_loss: 0.0638
1012/3000 [=========>....................] - ETA: 13:37 - loss: 0.3802 - regression_loss: 0.3164 - classification_loss: 0.0638
1013/3000 [=========>....................] - ETA: 13:37 - loss: 0.3800 - regression_loss: 0.3162 - classification_loss: 0.0637
1014/3000 [=========>....................] - ETA: 13:36 - loss: 0.3799 - regression_loss: 0.3162 - classification_loss: 0.0637
1015/3000 [=========>....................] - ETA: 13:36 - loss: 0.3802 - regression_loss: 0.3164 - classification_loss: 0.0637
1016/3000 [=========>....................] - ETA: 13:35 - loss: 0.3802 - regression_loss: 0.3164 - classification_loss: 0.0638
1017/3000 [=========>....................] - ETA: 13:35 - loss: 0.3802 - regression_loss: 0.3164 - classification_loss: 0.0637
1018/3000 [=========>....................] - ETA: 13:34 - loss: 0.3804 - regression_loss: 0.3166 - classification_loss: 0.0638
1019/3000 [=========>....................] - ETA: 13:34 - loss: 0.3804 - regression_loss: 0.3166 - classification_loss: 0.0638
1020/3000 [=========>....................] - ETA: 13:34 - loss: 0.3804 - regression_loss: 0.3166 - classification_loss: 0.0638
1021/3000 [=========>....................] - ETA: 13:33 - loss: 0.3805 - regression_loss: 0.3168 - classification_loss: 0.0637
1022/3000 [=========>....................] - ETA: 13:33 - loss: 0.3804 - regression_loss: 0.3167 - classification_loss: 0.0637
1023/3000 [=========>....................] - ETA: 13:32 - loss: 0.3805 - regression_loss: 0.3168 - classification_loss: 0.0637
1024/3000 [=========>....................] - ETA: 13:32 - loss: 0.3806 - regression_loss: 0.3169 - classification_loss: 0.0637
1025/3000 [=========>....................] - ETA: 13:32 - loss: 0.3808 - regression_loss: 0.3171 - classification_loss: 0.0637
1026/3000 [=========>....................] - ETA: 13:31 - loss: 0.3809 - regression_loss: 0.3172 - classification_loss: 0.0637
1027/3000 [=========>....................] - ETA: 13:31 - loss: 0.3811 - regression_loss: 0.3174 - classification_loss: 0.0637
1028/3000 [=========>....................] - ETA: 13:30 - loss: 0.3812 - regression_loss: 0.3176 - classification_loss: 0.0636
1029/3000 [=========>....................] - ETA: 13:30 - loss: 0.3815 - regression_loss: 0.3179 - classification_loss: 0.0637
1030/3000 [=========>....................] - ETA: 13:30 - loss: 0.3814 - regression_loss: 0.3178 - classification_loss: 0.0637
1031/3000 [=========>....................] - ETA: 13:29 - loss: 0.3812 - regression_loss: 0.3176 - classification_loss: 0.0636
1032/3000 [=========>....................] - ETA: 13:29 - loss: 0.3813 - regression_loss: 0.3177 - classification_loss: 0.0636
1033/3000 [=========>....................] - ETA: 13:28 - loss: 0.3812 - regression_loss: 0.3176 - classification_loss: 0.0636
1034/3000 [=========>....................] - ETA: 13:28 - loss: 0.3811 - regression_loss: 0.3175 - classification_loss: 0.0636
1035/3000 [=========>....................] - ETA: 13:27 - loss: 0.3810 - regression_loss: 0.3174 - classification_loss: 0.0635
1036/3000 [=========>....................] - ETA: 13:27 - loss: 0.3809 - regression_loss: 0.3174 - classification_loss: 0.0635
1037/3000 [=========>....................] - ETA: 13:27 - loss: 0.3809 - regression_loss: 0.3174 - classification_loss: 0.0635
1038/3000 [=========>....................] - ETA: 13:26 - loss: 0.3811 - regression_loss: 0.3176 - classification_loss: 0.0635
1039/3000 [=========>....................] - ETA: 13:26 - loss: 0.3810 - regression_loss: 0.3175 - classification_loss: 0.0634
1040/3000 [=========>....................] - ETA: 13:25 - loss: 0.3811 - regression_loss: 0.3177 - classification_loss: 0.0635
1041/3000 [=========>....................] - ETA: 13:25 - loss: 0.3812 - regression_loss: 0.3178 - classification_loss: 0.0634
1042/3000 [=========>....................] - ETA: 13:25 - loss: 0.3813 - regression_loss: 0.3179 - classification_loss: 0.0634
1043/3000 [=========>....................] - ETA: 13:24 - loss: 0.3813 - regression_loss: 0.3180 - classification_loss: 0.0634
1044/3000 [=========>....................] - ETA: 13:24 - loss: 0.3812 - regression_loss: 0.3178 - classification_loss: 0.0633
1045/3000 [=========>....................] - ETA: 13:23 - loss: 0.3811 - regression_loss: 0.3178 - classification_loss: 0.0633
1046/3000 [=========>....................] - ETA: 13:23 - loss: 0.3811 - regression_loss: 0.3178 - classification_loss: 0.0633
1047/3000 [=========>....................] - ETA: 13:23 - loss: 0.3809 - regression_loss: 0.3177 - classification_loss: 0.0633
1048/3000 [=========>....................] - ETA: 13:22 - loss: 0.3809 - regression_loss: 0.3177 - classification_loss: 0.0632
1049/3000 [=========>....................] - ETA: 13:22 - loss: 0.3812 - regression_loss: 0.3180 - classification_loss: 0.0632
1050/3000 [=========>....................] - ETA: 13:21 - loss: 0.3813 - regression_loss: 0.3181 - classification_loss: 0.0632
1051/3000 [=========>....................] - ETA: 13:21 - loss: 0.3815 - regression_loss: 0.3183 - classification_loss: 0.0632
1052/3000 [=========>....................] - ETA: 13:21 - loss: 0.3814 - regression_loss: 0.3183 - classification_loss: 0.0632
1053/3000 [=========>....................] - ETA: 13:20 - loss: 0.3814 - regression_loss: 0.3183 - classification_loss: 0.0632
1054/3000 [=========>....................] - ETA: 13:20 - loss: 0.3815 - regression_loss: 0.3183 - classification_loss: 0.0632
1055/3000 [=========>....................] - ETA: 13:19 - loss: 0.3815 - regression_loss: 0.3183 - classification_loss: 0.0632
1056/3000 [=========>....................] - ETA: 13:19 - loss: 0.3815 - regression_loss: 0.3184 - classification_loss: 0.0631
1057/3000 [=========>....................] - ETA: 13:18 - loss: 0.3815 - regression_loss: 0.3184 - classification_loss: 0.0631
1058/3000 [=========>....................] - ETA: 13:18 - loss: 0.3813 - regression_loss: 0.3183 - classification_loss: 0.0630
1059/3000 [=========>....................] - ETA: 13:18 - loss: 0.3814 - regression_loss: 0.3184 - classification_loss: 0.0630
1060/3000 [=========>....................] - ETA: 13:17 - loss: 0.3817 - regression_loss: 0.3186 - classification_loss: 0.0630
1061/3000 [=========>....................] - ETA: 13:17 - loss: 0.3815 - regression_loss: 0.3185 - classification_loss: 0.0630
1062/3000 [=========>....................] - ETA: 13:16 - loss: 0.3813 - regression_loss: 0.3184 - classification_loss: 0.0630
1063/3000 [=========>....................] - ETA: 13:16 - loss: 0.3815 - regression_loss: 0.3185 - classification_loss: 0.0630
1064/3000 [=========>....................] - ETA: 13:15 - loss: 0.3817 - regression_loss: 0.3188 - classification_loss: 0.0630
1065/3000 [=========>....................] - ETA: 13:15 - loss: 0.3818 - regression_loss: 0.3188 - classification_loss: 0.0629
1066/3000 [=========>....................] - ETA: 13:15 - loss: 0.3818 - regression_loss: 0.3189 - classification_loss: 0.0629
1067/3000 [=========>....................] - ETA: 13:14 - loss: 0.3818 - regression_loss: 0.3189 - classification_loss: 0.0629
1068/3000 [=========>....................] - ETA: 13:14 - loss: 0.3816 - regression_loss: 0.3188 - classification_loss: 0.0628
1069/3000 [=========>....................] - ETA: 13:13 - loss: 0.3815 - regression_loss: 0.3187 - classification_loss: 0.0628
1070/3000 [=========>....................] - ETA: 13:13 - loss: 0.3813 - regression_loss: 0.3185 - classification_loss: 0.0628
1071/3000 [=========>....................] - ETA: 13:12 - loss: 0.3813 - regression_loss: 0.3185 - classification_loss: 0.0627
1072/3000 [=========>....................] - ETA: 13:12 - loss: 0.3812 - regression_loss: 0.3185 - classification_loss: 0.0627
1073/3000 [=========>....................] - ETA: 13:12 - loss: 0.3813 - regression_loss: 0.3186 - classification_loss: 0.0627
1074/3000 [=========>....................] - ETA: 13:11 - loss: 0.3817 - regression_loss: 0.3189 - classification_loss: 0.0627
1075/3000 [=========>....................] - ETA: 13:11 - loss: 0.3818 - regression_loss: 0.3191 - classification_loss: 0.0627
1076/3000 [=========>....................] - ETA: 13:10 - loss: 0.3816 - regression_loss: 0.3189 - classification_loss: 0.0627
1077/3000 [=========>....................] - ETA: 13:10 - loss: 0.3815 - regression_loss: 0.3188 - classification_loss: 0.0627
1078/3000 [=========>....................] - ETA: 13:10 - loss: 0.3813 - regression_loss: 0.3187 - classification_loss: 0.0626
1079/3000 [=========>....................] - ETA: 13:09 - loss: 0.3812 - regression_loss: 0.3186 - classification_loss: 0.0626
1080/3000 [=========>....................] - ETA: 13:09 - loss: 0.3813 - regression_loss: 0.3187 - classification_loss: 0.0626
1081/3000 [=========>....................] - ETA: 13:08 - loss: 0.3813 - regression_loss: 0.3188 - classification_loss: 0.0625
1082/3000 [=========>....................] - ETA: 13:08 - loss: 0.3813 - regression_loss: 0.3187 - classification_loss: 0.0625
1083/3000 [=========>....................] - ETA: 13:08 - loss: 0.3812 - regression_loss: 0.3187 - classification_loss: 0.0625
1084/3000 [=========>....................] - ETA: 13:07 - loss: 0.3810 - regression_loss: 0.3186 - classification_loss: 0.0625
1085/3000 [=========>....................] - ETA: 13:07 - loss: 0.3809 - regression_loss: 0.3185 - classification_loss: 0.0624
1086/3000 [=========>....................] - ETA: 13:06 - loss: 0.3807 - regression_loss: 0.3183 - classification_loss: 0.0624
1087/3000 [=========>....................] - ETA: 13:06 - loss: 0.3811 - regression_loss: 0.3186 - classification_loss: 0.0624
1088/3000 [=========>....................] - ETA: 13:06 - loss: 0.3812 - regression_loss: 0.3188 - classification_loss: 0.0624
1089/3000 [=========>....................] - ETA: 13:05 - loss: 0.3813 - regression_loss: 0.3189 - classification_loss: 0.0624
1090/3000 [=========>....................] - ETA: 13:05 - loss: 0.3814 - regression_loss: 0.3190 - classification_loss: 0.0624
1091/3000 [=========>....................] - ETA: 13:04 - loss: 0.3813 - regression_loss: 0.3189 - classification_loss: 0.0624
1092/3000 [=========>....................] - ETA: 13:04 - loss: 0.3811 - regression_loss: 0.3188 - classification_loss: 0.0624
1093/3000 [=========>....................] - ETA: 13:04 - loss: 0.3814 - regression_loss: 0.3190 - classification_loss: 0.0623
1094/3000 [=========>....................] - ETA: 13:03 - loss: 0.3816 - regression_loss: 0.3193 - classification_loss: 0.0623
1095/3000 [=========>....................] - ETA: 13:03 - loss: 0.3817 - regression_loss: 0.3193 - classification_loss: 0.0623
1096/3000 [=========>....................] - ETA: 13:02 - loss: 0.3818 - regression_loss: 0.3195 - classification_loss: 0.0623
1097/3000 [=========>....................] - ETA: 13:02 - loss: 0.3819 - regression_loss: 0.3197 - classification_loss: 0.0623
1098/3000 [=========>....................] - ETA: 13:01 - loss: 0.3821 - regression_loss: 0.3199 - classification_loss: 0.0623
1099/3000 [=========>....................] - ETA: 13:01 - loss: 0.3821 - regression_loss: 0.3199 - classification_loss: 0.0622
1100/3000 [==========>...................] - ETA: 13:01 - loss: 0.3821 - regression_loss: 0.3198 - classification_loss: 0.0622
1101/3000 [==========>...................] - ETA: 13:00 - loss: 0.3821 - regression_loss: 0.3199 - classification_loss: 0.0622
1102/3000 [==========>...................] - ETA: 13:00 - loss: 0.3821 - regression_loss: 0.3200 - classification_loss: 0.0621
1103/3000 [==========>...................] - ETA: 12:59 - loss: 0.3820 - regression_loss: 0.3199 - classification_loss: 0.0621
1104/3000 [==========>...................] - ETA: 12:59 - loss: 0.3820 - regression_loss: 0.3200 - classification_loss: 0.0621
1105/3000 [==========>...................] - ETA: 12:59 - loss: 0.3820 - regression_loss: 0.3199 - classification_loss: 0.0621
1106/3000 [==========>...................] - ETA: 12:58 - loss: 0.3820 - regression_loss: 0.3199 - classification_loss: 0.0620
1107/3000 [==========>...................] - ETA: 12:58 - loss: 0.3821 - regression_loss: 0.3200 - classification_loss: 0.0621
1108/3000 [==========>...................] - ETA: 12:57 - loss: 0.3821 - regression_loss: 0.3200 - classification_loss: 0.0621
1109/3000 [==========>...................] - ETA: 12:57 - loss: 0.3820 - regression_loss: 0.3200 - classification_loss: 0.0620
1110/3000 [==========>...................] - ETA: 12:56 - loss: 0.3820 - regression_loss: 0.3199 - classification_loss: 0.0620
1111/3000 [==========>...................] - ETA: 12:56 - loss: 0.3818 - regression_loss: 0.3198 - classification_loss: 0.0620
1112/3000 [==========>...................] - ETA: 12:56 - loss: 0.3818 - regression_loss: 0.3198 - classification_loss: 0.0620
1113/3000 [==========>...................] - ETA: 12:55 - loss: 0.3817 - regression_loss: 0.3198 - classification_loss: 0.0619
1114/3000 [==========>...................] - ETA: 12:55 - loss: 0.3817 - regression_loss: 0.3198 - classification_loss: 0.0619
1115/3000 [==========>...................] - ETA: 12:54 - loss: 0.3816 - regression_loss: 0.3197 - classification_loss: 0.0619
1116/3000 [==========>...................] - ETA: 12:54 - loss: 0.3815 - regression_loss: 0.3196 - classification_loss: 0.0618
1117/3000 [==========>...................] - ETA: 12:54 - loss: 0.3815 - regression_loss: 0.3197 - classification_loss: 0.0618
1118/3000 [==========>...................] - ETA: 12:53 - loss: 0.3816 - regression_loss: 0.3198 - classification_loss: 0.0618
1119/3000 [==========>...................] - ETA: 12:53 - loss: 0.3815 - regression_loss: 0.3198 - classification_loss: 0.0618
1120/3000 [==========>...................] - ETA: 12:52 - loss: 0.3816 - regression_loss: 0.3199 - classification_loss: 0.0618
1121/3000 [==========>...................] - ETA: 12:52 - loss: 0.3818 - regression_loss: 0.3200 - classification_loss: 0.0618
1122/3000 [==========>...................] - ETA: 12:52 - loss: 0.3818 - regression_loss: 0.3200 - classification_loss: 0.0617
1123/3000 [==========>...................] - ETA: 12:51 - loss: 0.3817 - regression_loss: 0.3200 - classification_loss: 0.0617
1124/3000 [==========>...................] - ETA: 12:51 - loss: 0.3817 - regression_loss: 0.3200 - classification_loss: 0.0618
1125/3000 [==========>...................] - ETA: 12:50 - loss: 0.3818 - regression_loss: 0.3201 - classification_loss: 0.0618
1126/3000 [==========>...................] - ETA: 12:50 - loss: 0.3817 - regression_loss: 0.3200 - classification_loss: 0.0618
1127/3000 [==========>...................] - ETA: 12:49 - loss: 0.3817 - regression_loss: 0.3200 - classification_loss: 0.0617
1128/3000 [==========>...................] - ETA: 12:49 - loss: 0.3817 - regression_loss: 0.3200 - classification_loss: 0.0617
1129/3000 [==========>...................] - ETA: 12:49 - loss: 0.3818 - regression_loss: 0.3200 - classification_loss: 0.0617
1130/3000 [==========>...................] - ETA: 12:48 - loss: 0.3819 - regression_loss: 0.3202 - classification_loss: 0.0617
1131/3000 [==========>...................] - ETA: 12:48 - loss: 0.3819 - regression_loss: 0.3202 - classification_loss: 0.0617
1132/3000 [==========>...................] - ETA: 12:47 - loss: 0.3819 - regression_loss: 0.3202 - classification_loss: 0.0617
1133/3000 [==========>...................] - ETA: 12:47 - loss: 0.3818 - regression_loss: 0.3202 - classification_loss: 0.0616
1134/3000 [==========>...................] - ETA: 12:47 - loss: 0.3818 - regression_loss: 0.3202 - classification_loss: 0.0617
1135/3000 [==========>...................] - ETA: 12:46 - loss: 0.3820 - regression_loss: 0.3203 - classification_loss: 0.0616
1136/3000 [==========>...................] - ETA: 12:46 - loss: 0.3821 - regression_loss: 0.3205 - classification_loss: 0.0616
1137/3000 [==========>...................] - ETA: 12:45 - loss: 0.3821 - regression_loss: 0.3205 - classification_loss: 0.0616
1138/3000 [==========>...................] - ETA: 12:45 - loss: 0.3820 - regression_loss: 0.3205 - classification_loss: 0.0616
1139/3000 [==========>...................] - ETA: 12:45 - loss: 0.3821 - regression_loss: 0.3205 - classification_loss: 0.0615
1140/3000 [==========>...................] - ETA: 12:44 - loss: 0.3820 - regression_loss: 0.3205 - classification_loss: 0.0615
1141/3000 [==========>...................] - ETA: 12:44 - loss: 0.3819 - regression_loss: 0.3204 - classification_loss: 0.0615
1142/3000 [==========>...................] - ETA: 12:43 - loss: 0.3817 - regression_loss: 0.3203 - classification_loss: 0.0614
1143/3000 [==========>...................] - ETA: 12:43 - loss: 0.3816 - regression_loss: 0.3202 - classification_loss: 0.0614
1144/3000 [==========>...................] - ETA: 12:43 - loss: 0.3815 - regression_loss: 0.3201 - classification_loss: 0.0613
1145/3000 [==========>...................] - ETA: 12:42 - loss: 0.3814 - regression_loss: 0.3201 - classification_loss: 0.0613
1146/3000 [==========>...................] - ETA: 12:42 - loss: 0.3817 - regression_loss: 0.3203 - classification_loss: 0.0614
1147/3000 [==========>...................] - ETA: 12:41 - loss: 0.3817 - regression_loss: 0.3204 - classification_loss: 0.0614
1148/3000 [==========>...................] - ETA: 12:41 - loss: 0.3817 - regression_loss: 0.3204 - classification_loss: 0.0613
1149/3000 [==========>...................] - ETA: 12:40 - loss: 0.3817 - regression_loss: 0.3203 - classification_loss: 0.0613
1150/3000 [==========>...................] - ETA: 12:40 - loss: 0.3816 - regression_loss: 0.3202 - classification_loss: 0.0613
1151/3000 [==========>...................] - ETA: 12:40 - loss: 0.3814 - regression_loss: 0.3202 - classification_loss: 0.0613
1152/3000 [==========>...................] - ETA: 12:39 - loss: 0.3814 - regression_loss: 0.3201 - classification_loss: 0.0613
1153/3000 [==========>...................] - ETA: 12:39 - loss: 0.3813 - regression_loss: 0.3200 - classification_loss: 0.0612
1154/3000 [==========>...................] - ETA: 12:38 - loss: 0.3814 - regression_loss: 0.3202 - classification_loss: 0.0612
1155/3000 [==========>...................] - ETA: 12:38 - loss: 0.3813 - regression_loss: 0.3201 - classification_loss: 0.0612
1156/3000 [==========>...................] - ETA: 12:38 - loss: 0.3812 - regression_loss: 0.3201 - classification_loss: 0.0612
1157/3000 [==========>...................] - ETA: 12:37 - loss: 0.3811 - regression_loss: 0.3200 - classification_loss: 0.0611
1158/3000 [==========>...................] - ETA: 12:37 - loss: 0.3810 - regression_loss: 0.3199 - classification_loss: 0.0611
1159/3000 [==========>...................] - ETA: 12:36 - loss: 0.3809 - regression_loss: 0.3198 - classification_loss: 0.0611
1160/3000 [==========>...................] - ETA: 12:36 - loss: 0.3810 - regression_loss: 0.3199 - classification_loss: 0.0611
1161/3000 [==========>...................] - ETA: 12:36 - loss: 0.3809 - regression_loss: 0.3199 - classification_loss: 0.0611
1162/3000 [==========>...................] - ETA: 12:35 - loss: 0.3810 - regression_loss: 0.3199 - classification_loss: 0.0611
1163/3000 [==========>...................] - ETA: 12:35 - loss: 0.3810 - regression_loss: 0.3199 - classification_loss: 0.0610
1164/3000 [==========>...................] - ETA: 12:34 - loss: 0.3809 - regression_loss: 0.3199 - classification_loss: 0.0610
1165/3000 [==========>...................] - ETA: 12:34 - loss: 0.3810 - regression_loss: 0.3200 - classification_loss: 0.0610
1166/3000 [==========>...................] - ETA: 12:34 - loss: 0.3810 - regression_loss: 0.3200 - classification_loss: 0.0610
1167/3000 [==========>...................] - ETA: 12:33 - loss: 0.3809 - regression_loss: 0.3199 - classification_loss: 0.0610
1168/3000 [==========>...................] - ETA: 12:33 - loss: 0.3809 - regression_loss: 0.3200 - classification_loss: 0.0610
1169/3000 [==========>...................] - ETA: 12:32 - loss: 0.3808 - regression_loss: 0.3199 - classification_loss: 0.0609
1170/3000 [==========>...................] - ETA: 12:32 - loss: 0.3807 - regression_loss: 0.3198 - classification_loss: 0.0609
1171/3000 [==========>...................] - ETA: 12:32 - loss: 0.3805 - regression_loss: 0.3196 - classification_loss: 0.0609
1172/3000 [==========>...................] - ETA: 12:31 - loss: 0.3803 - regression_loss: 0.3195 - classification_loss: 0.0609
1173/3000 [==========>...................] - ETA: 12:31 - loss: 0.3802 - regression_loss: 0.3194 - classification_loss: 0.0608
1174/3000 [==========>...................] - ETA: 12:30 - loss: 0.3801 - regression_loss: 0.3193 - classification_loss: 0.0608
1175/3000 [==========>...................] - ETA: 12:30 - loss: 0.3801 - regression_loss: 0.3193 - classification_loss: 0.0608
1176/3000 [==========>...................] - ETA: 12:29 - loss: 0.3801 - regression_loss: 0.3193 - classification_loss: 0.0608
1177/3000 [==========>...................] - ETA: 12:29 - loss: 0.3801 - regression_loss: 0.3193 - classification_loss: 0.0608
1178/3000 [==========>...................] - ETA: 12:29 - loss: 0.3802 - regression_loss: 0.3195 - classification_loss: 0.0608
1179/3000 [==========>...................] - ETA: 12:28 - loss: 0.3803 - regression_loss: 0.3195 - classification_loss: 0.0608
1180/3000 [==========>...................] - ETA: 12:28 - loss: 0.3803 - regression_loss: 0.3195 - classification_loss: 0.0608
1181/3000 [==========>...................] - ETA: 12:27 - loss: 0.3803 - regression_loss: 0.3195 - classification_loss: 0.0608
1182/3000 [==========>...................] - ETA: 12:27 - loss: 0.3802 - regression_loss: 0.3194 - classification_loss: 0.0608
1183/3000 [==========>...................] - ETA: 12:27 - loss: 0.3800 - regression_loss: 0.3192 - classification_loss: 0.0607
1184/3000 [==========>...................] - ETA: 12:26 - loss: 0.3799 - regression_loss: 0.3192 - classification_loss: 0.0607
1185/3000 [==========>...................] - ETA: 12:26 - loss: 0.3800 - regression_loss: 0.3193 - classification_loss: 0.0607
1186/3000 [==========>...................] - ETA: 12:25 - loss: 0.3802 - regression_loss: 0.3194 - classification_loss: 0.0608
1187/3000 [==========>...................] - ETA: 12:25 - loss: 0.3801 - regression_loss: 0.3194 - classification_loss: 0.0607
1188/3000 [==========>...................] - ETA: 12:25 - loss: 0.3800 - regression_loss: 0.3193 - classification_loss: 0.0607
1189/3000 [==========>...................] - ETA: 12:24 - loss: 0.3799 - regression_loss: 0.3192 - classification_loss: 0.0607
1190/3000 [==========>...................] - ETA: 12:24 - loss: 0.3799 - regression_loss: 0.3193 - classification_loss: 0.0606
1191/3000 [==========>...................] - ETA: 12:23 - loss: 0.3799 - regression_loss: 0.3193 - classification_loss: 0.0606
1192/3000 [==========>...................] - ETA: 12:23 - loss: 0.3797 - regression_loss: 0.3191 - classification_loss: 0.0606
1193/3000 [==========>...................] - ETA: 12:22 - loss: 0.3796 - regression_loss: 0.3190 - classification_loss: 0.0606
1194/3000 [==========>...................] - ETA: 12:22 - loss: 0.3795 - regression_loss: 0.3190 - classification_loss: 0.0605
1195/3000 [==========>...................] - ETA: 12:22 - loss: 0.3797 - regression_loss: 0.3192 - classification_loss: 0.0605
1196/3000 [==========>...................] - ETA: 12:21 - loss: 0.3796 - regression_loss: 0.3191 - classification_loss: 0.0605
1197/3000 [==========>...................] - ETA: 12:21 - loss: 0.3795 - regression_loss: 0.3190 - classification_loss: 0.0605
1198/3000 [==========>...................] - ETA: 12:20 - loss: 0.3795 - regression_loss: 0.3191 - classification_loss: 0.0604
1199/3000 [==========>...................] - ETA: 12:20 - loss: 0.3796 - regression_loss: 0.3192 - classification_loss: 0.0604
1200/3000 [===========>..................] - ETA: 12:20 - loss: 0.3796 - regression_loss: 0.3192 - classification_loss: 0.0604
1201/3000 [===========>..................] - ETA: 12:19 - loss: 0.3796 - regression_loss: 0.3192 - classification_loss: 0.0604
1202/3000 [===========>..................] - ETA: 12:19 - loss: 0.3797 - regression_loss: 0.3193 - classification_loss: 0.0604
1203/3000 [===========>..................] - ETA: 12:18 - loss: 0.3796 - regression_loss: 0.3192 - classification_loss: 0.0604
1204/3000 [===========>..................] - ETA: 12:18 - loss: 0.3795 - regression_loss: 0.3192 - classification_loss: 0.0603
1205/3000 [===========>..................] - ETA: 12:17 - loss: 0.3793 - regression_loss: 0.3191 - classification_loss: 0.0603
1206/3000 [===========>..................] - ETA: 12:17 - loss: 0.3792 - regression_loss: 0.3190 - classification_loss: 0.0603
1207/3000 [===========>..................] - ETA: 12:17 - loss: 0.3794 - regression_loss: 0.3191 - classification_loss: 0.0603
1208/3000 [===========>..................] - ETA: 12:16 - loss: 0.3794 - regression_loss: 0.3191 - classification_loss: 0.0602
1209/3000 [===========>..................] - ETA: 12:16 - loss: 0.3793 - regression_loss: 0.3191 - classification_loss: 0.0602
1210/3000 [===========>..................] - ETA: 12:15 - loss: 0.3794 - regression_loss: 0.3192 - classification_loss: 0.0602
1211/3000 [===========>..................] - ETA: 12:15 - loss: 0.3793 - regression_loss: 0.3191 - classification_loss: 0.0602
1212/3000 [===========>..................] - ETA: 12:15 - loss: 0.3793 - regression_loss: 0.3191 - classification_loss: 0.0602
1213/3000 [===========>..................] - ETA: 12:14 - loss: 0.3791 - regression_loss: 0.3189 - classification_loss: 0.0601
1214/3000 [===========>..................] - ETA: 12:14 - loss: 0.3789 - regression_loss: 0.3188 - classification_loss: 0.0601
1215/3000 [===========>..................] - ETA: 12:13 - loss: 0.3789 - regression_loss: 0.3188 - classification_loss: 0.0601
1216/3000 [===========>..................] - ETA: 12:13 - loss: 0.3790 - regression_loss: 0.3189 - classification_loss: 0.0601
1217/3000 [===========>..................] - ETA: 12:13 - loss: 0.3789 - regression_loss: 0.3188 - classification_loss: 0.0601
1218/3000 [===========>..................] - ETA: 12:12 - loss: 0.3787 - regression_loss: 0.3186 - classification_loss: 0.0601
1219/3000 [===========>..................] - ETA: 12:12 - loss: 0.3785 - regression_loss: 0.3185 - classification_loss: 0.0600
1220/3000 [===========>..................] - ETA: 12:11 - loss: 0.3784 - regression_loss: 0.3184 - classification_loss: 0.0600
1221/3000 [===========>..................] - ETA: 12:11 - loss: 0.3783 - regression_loss: 0.3183 - classification_loss: 0.0600
1222/3000 [===========>..................] - ETA: 12:10 - loss: 0.3783 - regression_loss: 0.3184 - classification_loss: 0.0600
1223/3000 [===========>..................] - ETA: 12:10 - loss: 0.3784 - regression_loss: 0.3184 - classification_loss: 0.0599
1224/3000 [===========>..................] - ETA: 12:10 - loss: 0.3783 - regression_loss: 0.3184 - classification_loss: 0.0599
1225/3000 [===========>..................] - ETA: 12:09 - loss: 0.3782 - regression_loss: 0.3183 - classification_loss: 0.0599
1226/3000 [===========>..................] - ETA: 12:09 - loss: 0.3781 - regression_loss: 0.3182 - classification_loss: 0.0599
1227/3000 [===========>..................] - ETA: 12:08 - loss: 0.3782 - regression_loss: 0.3183 - classification_loss: 0.0599
1228/3000 [===========>..................] - ETA: 12:08 - loss: 0.3781 - regression_loss: 0.3182 - classification_loss: 0.0599
1229/3000 [===========>..................] - ETA: 12:08 - loss: 0.3781 - regression_loss: 0.3183 - classification_loss: 0.0598
1230/3000 [===========>..................] - ETA: 12:07 - loss: 0.3781 - regression_loss: 0.3183 - classification_loss: 0.0598
1231/3000 [===========>..................] - ETA: 12:07 - loss: 0.3781 - regression_loss: 0.3183 - classification_loss: 0.0598
1232/3000 [===========>..................] - ETA: 12:06 - loss: 0.3780 - regression_loss: 0.3182 - classification_loss: 0.0598
1233/3000 [===========>..................] - ETA: 12:06 - loss: 0.3779 - regression_loss: 0.3181 - classification_loss: 0.0597
1234/3000 [===========>..................] - ETA: 12:06 - loss: 0.3777 - regression_loss: 0.3180 - classification_loss: 0.0597
1235/3000 [===========>..................] - ETA: 12:05 - loss: 0.3776 - regression_loss: 0.3179 - classification_loss: 0.0597
1236/3000 [===========>..................] - ETA: 12:05 - loss: 0.3777 - regression_loss: 0.3180 - classification_loss: 0.0597
1237/3000 [===========>..................] - ETA: 12:04 - loss: 0.3777 - regression_loss: 0.3180 - classification_loss: 0.0597
1238/3000 [===========>..................] - ETA: 12:04 - loss: 0.3776 - regression_loss: 0.3180 - classification_loss: 0.0596
1239/3000 [===========>..................] - ETA: 12:03 - loss: 0.3777 - regression_loss: 0.3180 - classification_loss: 0.0596
1240/3000 [===========>..................] - ETA: 12:03 - loss: 0.3778 - regression_loss: 0.3181 - classification_loss: 0.0596
1241/3000 [===========>..................] - ETA: 12:03 - loss: 0.3777 - regression_loss: 0.3181 - classification_loss: 0.0596
1242/3000 [===========>..................] - ETA: 12:02 - loss: 0.3776 - regression_loss: 0.3181 - classification_loss: 0.0596
1243/3000 [===========>..................] - ETA: 12:02 - loss: 0.3776 - regression_loss: 0.3180 - classification_loss: 0.0596
1244/3000 [===========>..................] - ETA: 12:01 - loss: 0.3775 - regression_loss: 0.3180 - classification_loss: 0.0595
1245/3000 [===========>..................] - ETA: 12:01 - loss: 0.3776 - regression_loss: 0.3181 - classification_loss: 0.0595
1246/3000 [===========>..................] - ETA: 12:01 - loss: 0.3774 - regression_loss: 0.3180 - classification_loss: 0.0595
1247/3000 [===========>..................] - ETA: 12:00 - loss: 0.3773 - regression_loss: 0.3179 - classification_loss: 0.0594
1248/3000 [===========>..................] - ETA: 12:00 - loss: 0.3773 - regression_loss: 0.3178 - classification_loss: 0.0594
1249/3000 [===========>..................] - ETA: 11:59 - loss: 0.3771 - regression_loss: 0.3177 - classification_loss: 0.0594
1250/3000 [===========>..................] - ETA: 11:59 - loss: 0.3770 - regression_loss: 0.3176 - classification_loss: 0.0594
1251/3000 [===========>..................] - ETA: 11:58 - loss: 0.3769 - regression_loss: 0.3175 - classification_loss: 0.0593
1252/3000 [===========>..................] - ETA: 11:58 - loss: 0.3769 - regression_loss: 0.3176 - classification_loss: 0.0593
1253/3000 [===========>..................] - ETA: 11:58 - loss: 0.3770 - regression_loss: 0.3177 - classification_loss: 0.0593
1254/3000 [===========>..................] - ETA: 11:57 - loss: 0.3769 - regression_loss: 0.3177 - classification_loss: 0.0593
1255/3000 [===========>..................] - ETA: 11:57 - loss: 0.3768 - regression_loss: 0.3175 - classification_loss: 0.0592
1256/3000 [===========>..................] - ETA: 11:56 - loss: 0.3766 - regression_loss: 0.3174 - classification_loss: 0.0592
1257/3000 [===========>..................] - ETA: 11:56 - loss: 0.3766 - regression_loss: 0.3174 - classification_loss: 0.0592
1258/3000 [===========>..................] - ETA: 11:56 - loss: 0.3766 - regression_loss: 0.3175 - classification_loss: 0.0592
1259/3000 [===========>..................] - ETA: 11:55 - loss: 0.3765 - regression_loss: 0.3174 - classification_loss: 0.0591
1260/3000 [===========>..................] - ETA: 11:55 - loss: 0.3764 - regression_loss: 0.3173 - classification_loss: 0.0591
1261/3000 [===========>..................] - ETA: 11:54 - loss: 0.3763 - regression_loss: 0.3172 - classification_loss: 0.0591
1262/3000 [===========>..................] - ETA: 11:54 - loss: 0.3765 - regression_loss: 0.3174 - classification_loss: 0.0591
1263/3000 [===========>..................] - ETA: 11:54 - loss: 0.3764 - regression_loss: 0.3173 - classification_loss: 0.0591
1264/3000 [===========>..................] - ETA: 11:53 - loss: 0.3764 - regression_loss: 0.3174 - classification_loss: 0.0591
1265/3000 [===========>..................] - ETA: 11:53 - loss: 0.3764 - regression_loss: 0.3174 - classification_loss: 0.0591
1266/3000 [===========>..................] - ETA: 11:52 - loss: 0.3764 - regression_loss: 0.3173 - classification_loss: 0.0590
1267/3000 [===========>..................] - ETA: 11:52 - loss: 0.3764 - regression_loss: 0.3173 - classification_loss: 0.0590
1268/3000 [===========>..................] - ETA: 11:51 - loss: 0.3762 - regression_loss: 0.3173 - classification_loss: 0.0590
1269/3000 [===========>..................] - ETA: 11:51 - loss: 0.3761 - regression_loss: 0.3172 - classification_loss: 0.0590
1270/3000 [===========>..................] - ETA: 11:51 - loss: 0.3761 - regression_loss: 0.3171 - classification_loss: 0.0589
1271/3000 [===========>..................] - ETA: 11:50 - loss: 0.3761 - regression_loss: 0.3171 - classification_loss: 0.0589
1272/3000 [===========>..................] - ETA: 11:50 - loss: 0.3760 - regression_loss: 0.3170 - classification_loss: 0.0589
1273/3000 [===========>..................] - ETA: 11:49 - loss: 0.3760 - regression_loss: 0.3171 - classification_loss: 0.0589
1274/3000 [===========>..................] - ETA: 11:49 - loss: 0.3760 - regression_loss: 0.3172 - classification_loss: 0.0589
1275/3000 [===========>..................] - ETA: 11:49 - loss: 0.3759 - regression_loss: 0.3171 - classification_loss: 0.0588
1276/3000 [===========>..................] - ETA: 11:48 - loss: 0.3758 - regression_loss: 0.3170 - classification_loss: 0.0588
1277/3000 [===========>..................] - ETA: 11:48 - loss: 0.3759 - regression_loss: 0.3171 - classification_loss: 0.0588
1278/3000 [===========>..................] - ETA: 11:47 - loss: 0.3758 - regression_loss: 0.3170 - classification_loss: 0.0588
1279/3000 [===========>..................] - ETA: 11:47 - loss: 0.3757 - regression_loss: 0.3169 - classification_loss: 0.0588
1280/3000 [===========>..................] - ETA: 11:46 - loss: 0.3759 - regression_loss: 0.3171 - classification_loss: 0.0588
1281/3000 [===========>..................] - ETA: 11:46 - loss: 0.3758 - regression_loss: 0.3171 - classification_loss: 0.0587
1282/3000 [===========>..................] - ETA: 11:46 - loss: 0.3759 - regression_loss: 0.3171 - classification_loss: 0.0587
1283/3000 [===========>..................] - ETA: 11:45 - loss: 0.3758 - regression_loss: 0.3171 - classification_loss: 0.0588
1284/3000 [===========>..................] - ETA: 11:45 - loss: 0.3757 - regression_loss: 0.3170 - classification_loss: 0.0587
1285/3000 [===========>..................] - ETA: 11:44 - loss: 0.3756 - regression_loss: 0.3169 - classification_loss: 0.0587
1286/3000 [===========>..................] - ETA: 11:44 - loss: 0.3756 - regression_loss: 0.3169 - classification_loss: 0.0587
1287/3000 [===========>..................] - ETA: 11:44 - loss: 0.3754 - regression_loss: 0.3168 - classification_loss: 0.0587
1288/3000 [===========>..................] - ETA: 11:43 - loss: 0.3753 - regression_loss: 0.3167 - classification_loss: 0.0586
1289/3000 [===========>..................] - ETA: 11:43 - loss: 0.3753 - regression_loss: 0.3167 - classification_loss: 0.0586
1290/3000 [===========>..................] - ETA: 11:42 - loss: 0.3752 - regression_loss: 0.3166 - classification_loss: 0.0586
1291/3000 [===========>..................] - ETA: 11:42 - loss: 0.3752 - regression_loss: 0.3167 - classification_loss: 0.0586
1292/3000 [===========>..................] - ETA: 11:42 - loss: 0.3750 - regression_loss: 0.3165 - classification_loss: 0.0585
1293/3000 [===========>..................] - ETA: 11:41 - loss: 0.3752 - regression_loss: 0.3167 - classification_loss: 0.0585
1294/3000 [===========>..................] - ETA: 11:41 - loss: 0.3752 - regression_loss: 0.3167 - classification_loss: 0.0585
1295/3000 [===========>..................] - ETA: 11:40 - loss: 0.3750 - regression_loss: 0.3165 - classification_loss: 0.0585
1296/3000 [===========>..................] - ETA: 11:40 - loss: 0.3749 - regression_loss: 0.3165 - classification_loss: 0.0584
1297/3000 [===========>..................] - ETA: 11:40 - loss: 0.3748 - regression_loss: 0.3164 - classification_loss: 0.0584
1298/3000 [===========>..................] - ETA: 11:39 - loss: 0.3749 - regression_loss: 0.3165 - classification_loss: 0.0584
1299/3000 [===========>..................] - ETA: 11:39 - loss: 0.3751 - regression_loss: 0.3166 - classification_loss: 0.0584
1300/3000 [============>.................] - ETA: 11:38 - loss: 0.3750 - regression_loss: 0.3166 - classification_loss: 0.0584
1301/3000 [============>.................] - ETA: 11:38 - loss: 0.3749 - regression_loss: 0.3165 - classification_loss: 0.0584
1302/3000 [============>.................] - ETA: 11:37 - loss: 0.3747 - regression_loss: 0.3164 - classification_loss: 0.0584
1303/3000 [============>.................] - ETA: 11:37 - loss: 0.3748 - regression_loss: 0.3164 - classification_loss: 0.0584
1304/3000 [============>.................] - ETA: 11:37 - loss: 0.3748 - regression_loss: 0.3164 - classification_loss: 0.0584
1305/3000 [============>.................] - ETA: 11:36 - loss: 0.3746 - regression_loss: 0.3163 - classification_loss: 0.0584
1306/3000 [============>.................] - ETA: 11:36 - loss: 0.3746 - regression_loss: 0.3163 - classification_loss: 0.0584
1307/3000 [============>.................] - ETA: 11:35 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0583
1308/3000 [============>.................] - ETA: 11:35 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0583
1309/3000 [============>.................] - ETA: 11:35 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0583
1310/3000 [============>.................] - ETA: 11:34 - loss: 0.3743 - regression_loss: 0.3160 - classification_loss: 0.0583
1311/3000 [============>.................] - ETA: 11:34 - loss: 0.3745 - regression_loss: 0.3161 - classification_loss: 0.0583
1312/3000 [============>.................] - ETA: 11:33 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0583
1313/3000 [============>.................] - ETA: 11:33 - loss: 0.3742 - regression_loss: 0.3159 - classification_loss: 0.0583
1314/3000 [============>.................] - ETA: 11:32 - loss: 0.3742 - regression_loss: 0.3158 - classification_loss: 0.0583
1315/3000 [============>.................] - ETA: 11:32 - loss: 0.3740 - regression_loss: 0.3157 - classification_loss: 0.0583
1316/3000 [============>.................] - ETA: 11:32 - loss: 0.3739 - regression_loss: 0.3156 - classification_loss: 0.0583
1317/3000 [============>.................] - ETA: 11:31 - loss: 0.3740 - regression_loss: 0.3157 - classification_loss: 0.0583
1318/3000 [============>.................] - ETA: 11:31 - loss: 0.3741 - regression_loss: 0.3157 - classification_loss: 0.0584
1319/3000 [============>.................] - ETA: 11:30 - loss: 0.3740 - regression_loss: 0.3157 - classification_loss: 0.0583
1320/3000 [============>.................] - ETA: 11:30 - loss: 0.3742 - regression_loss: 0.3159 - classification_loss: 0.0584
1321/3000 [============>.................] - ETA: 11:30 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0583
1322/3000 [============>.................] - ETA: 11:29 - loss: 0.3745 - regression_loss: 0.3161 - classification_loss: 0.0584
1323/3000 [============>.................] - ETA: 11:29 - loss: 0.3745 - regression_loss: 0.3161 - classification_loss: 0.0584
1324/3000 [============>.................] - ETA: 11:28 - loss: 0.3745 - regression_loss: 0.3161 - classification_loss: 0.0584
1325/3000 [============>.................] - ETA: 11:28 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0584
1326/3000 [============>.................] - ETA: 11:28 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0584
1327/3000 [============>.................] - ETA: 11:27 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0584
1328/3000 [============>.................] - ETA: 11:27 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0584
1329/3000 [============>.................] - ETA: 11:26 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0584
1330/3000 [============>.................] - ETA: 11:26 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0584
1331/3000 [============>.................] - ETA: 11:26 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0584
1332/3000 [============>.................] - ETA: 11:25 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0584
1333/3000 [============>.................] - ETA: 11:25 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0584
1334/3000 [============>.................] - ETA: 11:24 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0584
1335/3000 [============>.................] - ETA: 11:24 - loss: 0.3744 - regression_loss: 0.3159 - classification_loss: 0.0585
1336/3000 [============>.................] - ETA: 11:23 - loss: 0.3744 - regression_loss: 0.3159 - classification_loss: 0.0585
1337/3000 [============>.................] - ETA: 11:23 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0584
1338/3000 [============>.................] - ETA: 11:23 - loss: 0.3744 - regression_loss: 0.3159 - classification_loss: 0.0585
1339/3000 [============>.................] - ETA: 11:22 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0585
1340/3000 [============>.................] - ETA: 11:22 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0584
1341/3000 [============>.................] - ETA: 11:21 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0584
1342/3000 [============>.................] - ETA: 11:21 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0584
1343/3000 [============>.................] - ETA: 11:21 - loss: 0.3743 - regression_loss: 0.3159 - classification_loss: 0.0584
1344/3000 [============>.................] - ETA: 11:20 - loss: 0.3742 - regression_loss: 0.3158 - classification_loss: 0.0584
1345/3000 [============>.................] - ETA: 11:20 - loss: 0.3742 - regression_loss: 0.3158 - classification_loss: 0.0583
1346/3000 [============>.................] - ETA: 11:19 - loss: 0.3741 - regression_loss: 0.3158 - classification_loss: 0.0583
1347/3000 [============>.................] - ETA: 11:19 - loss: 0.3741 - regression_loss: 0.3158 - classification_loss: 0.0583
1348/3000 [============>.................] - ETA: 11:19 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0583
1349/3000 [============>.................] - ETA: 11:18 - loss: 0.3745 - regression_loss: 0.3162 - classification_loss: 0.0583
1350/3000 [============>.................] - ETA: 11:18 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0583
1351/3000 [============>.................] - ETA: 11:17 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0583
1352/3000 [============>.................] - ETA: 11:17 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0583
1353/3000 [============>.................] - ETA: 11:17 - loss: 0.3744 - regression_loss: 0.3162 - classification_loss: 0.0582
1354/3000 [============>.................] - ETA: 11:16 - loss: 0.3745 - regression_loss: 0.3163 - classification_loss: 0.0582
1355/3000 [============>.................] - ETA: 11:16 - loss: 0.3746 - regression_loss: 0.3163 - classification_loss: 0.0582
1356/3000 [============>.................] - ETA: 11:15 - loss: 0.3746 - regression_loss: 0.3164 - classification_loss: 0.0583
1357/3000 [============>.................] - ETA: 11:15 - loss: 0.3746 - regression_loss: 0.3164 - classification_loss: 0.0582
1358/3000 [============>.................] - ETA: 11:14 - loss: 0.3745 - regression_loss: 0.3162 - classification_loss: 0.0582
1359/3000 [============>.................] - ETA: 11:14 - loss: 0.3745 - regression_loss: 0.3162 - classification_loss: 0.0583
1360/3000 [============>.................] - ETA: 11:14 - loss: 0.3746 - regression_loss: 0.3162 - classification_loss: 0.0583
1361/3000 [============>.................] - ETA: 11:13 - loss: 0.3747 - regression_loss: 0.3164 - classification_loss: 0.0583
1362/3000 [============>.................] - ETA: 11:13 - loss: 0.3746 - regression_loss: 0.3163 - classification_loss: 0.0583
1363/3000 [============>.................] - ETA: 11:12 - loss: 0.3746 - regression_loss: 0.3163 - classification_loss: 0.0583
1364/3000 [============>.................] - ETA: 11:12 - loss: 0.3745 - regression_loss: 0.3163 - classification_loss: 0.0583
1365/3000 [============>.................] - ETA: 11:12 - loss: 0.3745 - regression_loss: 0.3162 - classification_loss: 0.0583
1366/3000 [============>.................] - ETA: 11:11 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0583
1367/3000 [============>.................] - ETA: 11:11 - loss: 0.3743 - regression_loss: 0.3160 - classification_loss: 0.0583
1368/3000 [============>.................] - ETA: 11:10 - loss: 0.3743 - regression_loss: 0.3161 - classification_loss: 0.0583
1369/3000 [============>.................] - ETA: 11:10 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0584
1370/3000 [============>.................] - ETA: 11:10 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0584
1371/3000 [============>.................] - ETA: 11:09 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0584
1372/3000 [============>.................] - ETA: 11:09 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0584
1373/3000 [============>.................] - ETA: 11:08 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0584
1374/3000 [============>.................] - ETA: 11:08 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0584
1375/3000 [============>.................] - ETA: 11:08 - loss: 0.3745 - regression_loss: 0.3161 - classification_loss: 0.0584
1376/3000 [============>.................] - ETA: 11:07 - loss: 0.3745 - regression_loss: 0.3161 - classification_loss: 0.0584
1377/3000 [============>.................] - ETA: 11:07 - loss: 0.3744 - regression_loss: 0.3160 - classification_loss: 0.0583
1378/3000 [============>.................] - ETA: 11:06 - loss: 0.3745 - regression_loss: 0.3162 - classification_loss: 0.0583
1379/3000 [============>.................] - ETA: 11:06 - loss: 0.3745 - regression_loss: 0.3162 - classification_loss: 0.0583
1380/3000 [============>.................] - ETA: 11:05 - loss: 0.3746 - regression_loss: 0.3163 - classification_loss: 0.0583
1381/3000 [============>.................] - ETA: 11:05 - loss: 0.3745 - regression_loss: 0.3162 - classification_loss: 0.0583
1382/3000 [============>.................] - ETA: 11:05 - loss: 0.3746 - regression_loss: 0.3163 - classification_loss: 0.0583
1383/3000 [============>.................] - ETA: 11:04 - loss: 0.3744 - regression_loss: 0.3161 - classification_loss: 0.0583
1384/3000 [============>.................] - ETA: 11:04 - loss: 0.3746 - regression_loss: 0.3163 - classification_loss: 0.0583
1385/3000 [============>.................] - ETA: 11:03 - loss: 0.3745 - regression_loss: 0.3162 - classification_loss: 0.0582
1386/3000 [============>.................] - ETA: 11:03 - loss: 0.3746 - regression_loss: 0.3163 - classification_loss: 0.0582
1387/3000 [============>.................] - ETA: 11:03 - loss: 0.3744 - regression_loss: 0.3162 - classification_loss: 0.0582
1388/3000 [============>.................] - ETA: 11:02 - loss: 0.3744 - regression_loss: 0.3162 - classification_loss: 0.0582
1389/3000 [============>.................] - ETA: 11:02 - loss: 0.3744 - regression_loss: 0.3162 - classification_loss: 0.0582
1390/3000 [============>.................] - ETA: 11:01 - loss: 0.3744 - regression_loss: 0.3162 - classification_loss: 0.0582
1391/3000 [============>.................] - ETA: 11:01 - loss: 0.3744 - regression_loss: 0.3163 - classification_loss: 0.0582
1392/3000 [============>.................] - ETA: 11:01 - loss: 0.3744 - regression_loss: 0.3163 - classification_loss: 0.0582
1393/3000 [============>.................] - ETA: 11:00 - loss: 0.3746 - regression_loss: 0.3164 - classification_loss: 0.0582
1394/3000 [============>.................] - ETA: 11:00 - loss: 0.3746 - regression_loss: 0.3164 - classification_loss: 0.0582
1395/3000 [============>.................] - ETA: 10:59 - loss: 0.3745 - regression_loss: 0.3164 - classification_loss: 0.0582
1396/3000 [============>.................] - ETA: 10:59 - loss: 0.3746 - regression_loss: 0.3165 - classification_loss: 0.0581
1397/3000 [============>.................] - ETA: 10:58 - loss: 0.3746 - regression_loss: 0.3165 - classification_loss: 0.0581
1398/3000 [============>.................] - ETA: 10:58 - loss: 0.3746 - regression_loss: 0.3165 - classification_loss: 0.0581
1399/3000 [============>.................] - ETA: 10:58 - loss: 0.3745 - regression_loss: 0.3164 - classification_loss: 0.0581
1400/3000 [=============>................] - ETA: 10:57 - loss: 0.3744 - regression_loss: 0.3163 - classification_loss: 0.0581
1401/3000 [=============>................] - ETA: 10:57 - loss: 0.3743 - regression_loss: 0.3163 - classification_loss: 0.0581
1402/3000 [=============>................] - ETA: 10:56 - loss: 0.3742 - regression_loss: 0.3162 - classification_loss: 0.0581
1403/3000 [=============>................] - ETA: 10:56 - loss: 0.3741 - regression_loss: 0.3161 - classification_loss: 0.0580
1404/3000 [=============>................] - ETA: 10:56 - loss: 0.3740 - regression_loss: 0.3160 - classification_loss: 0.0580
1405/3000 [=============>................] - ETA: 10:55 - loss: 0.3739 - regression_loss: 0.3159 - classification_loss: 0.0580
1406/3000 [=============>................] - ETA: 10:55 - loss: 0.3739 - regression_loss: 0.3160 - classification_loss: 0.0580
1407/3000 [=============>................] - ETA: 10:54 - loss: 0.3741 - regression_loss: 0.3161 - classification_loss: 0.0580
1408/3000 [=============>................] - ETA: 10:54 - loss: 0.3740 - regression_loss: 0.3160 - classification_loss: 0.0580
1409/3000 [=============>................] - ETA: 10:54 - loss: 0.3738 - regression_loss: 0.3158 - classification_loss: 0.0580
1410/3000 [=============>................] - ETA: 10:53 - loss: 0.3737 - regression_loss: 0.3157 - classification_loss: 0.0579
1411/3000 [=============>................] - ETA: 10:53 - loss: 0.3737 - regression_loss: 0.3157 - classification_loss: 0.0579
1412/3000 [=============>................] - ETA: 10:52 - loss: 0.3738 - regression_loss: 0.3158 - classification_loss: 0.0580
1413/3000 [=============>................] - ETA: 10:52 - loss: 0.3737 - regression_loss: 0.3158 - classification_loss: 0.0579
1414/3000 [=============>................] - ETA: 10:51 - loss: 0.3737 - regression_loss: 0.3157 - classification_loss: 0.0579
1415/3000 [=============>................] - ETA: 10:51 - loss: 0.3737 - regression_loss: 0.3157 - classification_loss: 0.0579
1416/3000 [=============>................] - ETA: 10:51 - loss: 0.3736 - regression_loss: 0.3157 - classification_loss: 0.0579
1417/3000 [=============>................] - ETA: 10:50 - loss: 0.3734 - regression_loss: 0.3156 - classification_loss: 0.0579
1418/3000 [=============>................] - ETA: 10:50 - loss: 0.3735 - regression_loss: 0.3156 - classification_loss: 0.0579
1419/3000 [=============>................] - ETA: 10:49 - loss: 0.3736 - regression_loss: 0.3157 - classification_loss: 0.0579
1420/3000 [=============>................] - ETA: 10:49 - loss: 0.3736 - regression_loss: 0.3157 - classification_loss: 0.0579
1421/3000 [=============>................] - ETA: 10:49 - loss: 0.3736 - regression_loss: 0.3157 - classification_loss: 0.0579
1422/3000 [=============>................] - ETA: 10:48 - loss: 0.3736 - regression_loss: 0.3157 - classification_loss: 0.0579
1423/3000 [=============>................] - ETA: 10:48 - loss: 0.3736 - regression_loss: 0.3157 - classification_loss: 0.0579
1424/3000 [=============>................] - ETA: 10:47 - loss: 0.3735 - regression_loss: 0.3157 - classification_loss: 0.0579
1425/3000 [=============>................] - ETA: 10:47 - loss: 0.3737 - regression_loss: 0.3158 - classification_loss: 0.0579
1426/3000 [=============>................] - ETA: 10:47 - loss: 0.3735 - regression_loss: 0.3157 - classification_loss: 0.0579
1427/3000 [=============>................] - ETA: 10:46 - loss: 0.3735 - regression_loss: 0.3157 - classification_loss: 0.0578
1428/3000 [=============>................] - ETA: 10:46 - loss: 0.3735 - regression_loss: 0.3157 - classification_loss: 0.0578
1429/3000 [=============>................] - ETA: 10:45 - loss: 0.3734 - regression_loss: 0.3156 - classification_loss: 0.0578
1430/3000 [=============>................] - ETA: 10:45 - loss: 0.3734 - regression_loss: 0.3156 - classification_loss: 0.0578
1431/3000 [=============>................] - ETA: 10:44 - loss: 0.3734 - regression_loss: 0.3156 - classification_loss: 0.0578
1432/3000 [=============>................] - ETA: 10:44 - loss: 0.3735 - regression_loss: 0.3157 - classification_loss: 0.0578
1433/3000 [=============>................] - ETA: 10:44 - loss: 0.3734 - regression_loss: 0.3157 - classification_loss: 0.0578
1434/3000 [=============>................] - ETA: 10:43 - loss: 0.3734 - regression_loss: 0.3156 - classification_loss: 0.0577
1435/3000 [=============>................] - ETA: 10:43 - loss: 0.3734 - regression_loss: 0.3157 - classification_loss: 0.0577
1436/3000 [=============>................] - ETA: 10:42 - loss: 0.3734 - regression_loss: 0.3157 - classification_loss: 0.0577
1437/3000 [=============>................] - ETA: 10:42 - loss: 0.3733 - regression_loss: 0.3156 - classification_loss: 0.0577
1438/3000 [=============>................] - ETA: 10:42 - loss: 0.3733 - regression_loss: 0.3156 - classification_loss: 0.0577
1439/3000 [=============>................] - ETA: 10:41 - loss: 0.3732 - regression_loss: 0.3155 - classification_loss: 0.0577
1440/3000 [=============>................] - ETA: 10:41 - loss: 0.3731 - regression_loss: 0.3154 - classification_loss: 0.0577
1441/3000 [=============>................] - ETA: 10:40 - loss: 0.3730 - regression_loss: 0.3154 - classification_loss: 0.0577
1442/3000 [=============>................] - ETA: 10:40 - loss: 0.3730 - regression_loss: 0.3154 - classification_loss: 0.0576
1443/3000 [=============>................] - ETA: 10:39 - loss: 0.3729 - regression_loss: 0.3153 - classification_loss: 0.0576
1444/3000 [=============>................] - ETA: 10:39 - loss: 0.3729 - regression_loss: 0.3153 - classification_loss: 0.0576
1445/3000 [=============>................] - ETA: 10:39 - loss: 0.3728 - regression_loss: 0.3152 - classification_loss: 0.0576
1446/3000 [=============>................] - ETA: 10:38 - loss: 0.3726 - regression_loss: 0.3151 - classification_loss: 0.0576
1447/3000 [=============>................] - ETA: 10:38 - loss: 0.3726 - regression_loss: 0.3150 - classification_loss: 0.0575
1448/3000 [=============>................] - ETA: 10:37 - loss: 0.3725 - regression_loss: 0.3150 - classification_loss: 0.0575
1449/3000 [=============>................] - ETA: 10:37 - loss: 0.3726 - regression_loss: 0.3151 - classification_loss: 0.0575
1450/3000 [=============>................] - ETA: 10:37 - loss: 0.3725 - regression_loss: 0.3150 - classification_loss: 0.0575
1451/3000 [=============>................] - ETA: 10:36 - loss: 0.3726 - regression_loss: 0.3151 - classification_loss: 0.0575
1452/3000 [=============>................] - ETA: 10:36 - loss: 0.3726 - regression_loss: 0.3151 - classification_loss: 0.0575
1453/3000 [=============>................] - ETA: 10:35 - loss: 0.3726 - regression_loss: 0.3151 - classification_loss: 0.0575
1454/3000 [=============>................] - ETA: 10:35 - loss: 0.3725 - regression_loss: 0.3151 - classification_loss: 0.0575
1455/3000 [=============>................] - ETA: 10:35 - loss: 0.3726 - regression_loss: 0.3151 - classification_loss: 0.0575
1456/3000 [=============>................] - ETA: 10:34 - loss: 0.3725 - regression_loss: 0.3151 - classification_loss: 0.0574
1457/3000 [=============>................] - ETA: 10:34 - loss: 0.3725 - regression_loss: 0.3151 - classification_loss: 0.0574
1458/3000 [=============>................] - ETA: 10:33 - loss: 0.3726 - regression_loss: 0.3152 - classification_loss: 0.0574
1459/3000 [=============>................] - ETA: 10:33 - loss: 0.3725 - regression_loss: 0.3152 - classification_loss: 0.0574
1460/3000 [=============>................] - ETA: 10:33 - loss: 0.3725 - regression_loss: 0.3151 - classification_loss: 0.0574
1461/3000 [=============>................] - ETA: 10:32 - loss: 0.3723 - regression_loss: 0.3150 - classification_loss: 0.0573
1462/3000 [=============>................] - ETA: 10:32 - loss: 0.3725 - regression_loss: 0.3151 - classification_loss: 0.0574
1463/3000 [=============>................] - ETA: 10:31 - loss: 0.3724 - regression_loss: 0.3151 - classification_loss: 0.0573
1464/3000 [=============>................] - ETA: 10:31 - loss: 0.3723 - regression_loss: 0.3150 - classification_loss: 0.0573
1465/3000 [=============>................] - ETA: 10:30 - loss: 0.3722 - regression_loss: 0.3150 - classification_loss: 0.0573
1466/3000 [=============>................] - ETA: 10:30 - loss: 0.3721 - regression_loss: 0.3148 - classification_loss: 0.0573
1467/3000 [=============>................] - ETA: 10:30 - loss: 0.3722 - regression_loss: 0.3150 - classification_loss: 0.0573
1468/3000 [=============>................] - ETA: 10:29 - loss: 0.3723 - regression_loss: 0.3150 - classification_loss: 0.0573
1469/3000 [=============>................] - ETA: 10:29 - loss: 0.3723 - regression_loss: 0.3150 - classification_loss: 0.0573
1470/3000 [=============>................] - ETA: 10:28 - loss: 0.3724 - regression_loss: 0.3151 - classification_loss: 0.0573
1471/3000 [=============>................] - ETA: 10:28 - loss: 0.3725 - regression_loss: 0.3152 - classification_loss: 0.0572
1472/3000 [=============>................] - ETA: 10:28 - loss: 0.3725 - regression_loss: 0.3153 - classification_loss: 0.0572
1473/3000 [=============>................] - ETA: 10:27 - loss: 0.3726 - regression_loss: 0.3154 - classification_loss: 0.0572
1474/3000 [=============>................] - ETA: 10:27 - loss: 0.3724 - regression_loss: 0.3153 - classification_loss: 0.0572
1475/3000 [=============>................] - ETA: 10:26 - loss: 0.3723 - regression_loss: 0.3151 - classification_loss: 0.0571
1476/3000 [=============>................] - ETA: 10:26 - loss: 0.3723 - regression_loss: 0.3151 - classification_loss: 0.0571
1477/3000 [=============>................] - ETA: 10:25 - loss: 0.3721 - regression_loss: 0.3150 - classification_loss: 0.0571
1478/3000 [=============>................] - ETA: 10:25 - loss: 0.3723 - regression_loss: 0.3152 - classification_loss: 0.0571
1479/3000 [=============>................] - ETA: 10:25 - loss: 0.3724 - regression_loss: 0.3153 - classification_loss: 0.0571
1480/3000 [=============>................] - ETA: 10:24 - loss: 0.3724 - regression_loss: 0.3153 - classification_loss: 0.0571
1481/3000 [=============>................] - ETA: 10:24 - loss: 0.3724 - regression_loss: 0.3153 - classification_loss: 0.0571
1482/3000 [=============>................] - ETA: 10:23 - loss: 0.3723 - regression_loss: 0.3153 - classification_loss: 0.0571
1483/3000 [=============>................] - ETA: 10:23 - loss: 0.3723 - regression_loss: 0.3152 - classification_loss: 0.0570
1484/3000 [=============>................] - ETA: 10:23 - loss: 0.3725 - regression_loss: 0.3154 - classification_loss: 0.0570
1485/3000 [=============>................] - ETA: 10:22 - loss: 0.3724 - regression_loss: 0.3154 - classification_loss: 0.0570
1486/3000 [=============>................] - ETA: 10:22 - loss: 0.3724 - regression_loss: 0.3154 - classification_loss: 0.0570
1487/3000 [=============>................] - ETA: 10:21 - loss: 0.3724 - regression_loss: 0.3154 - classification_loss: 0.0570
1488/3000 [=============>................] - ETA: 10:21 - loss: 0.3725 - regression_loss: 0.3155 - classification_loss: 0.0570
1489/3000 [=============>................] - ETA: 10:21 - loss: 0.3724 - regression_loss: 0.3155 - classification_loss: 0.0569
1490/3000 [=============>................] - ETA: 10:20 - loss: 0.3723 - regression_loss: 0.3154 - classification_loss: 0.0569
1491/3000 [=============>................] - ETA: 10:20 - loss: 0.3722 - regression_loss: 0.3153 - classification_loss: 0.0569
1492/3000 [=============>................] - ETA: 10:19 - loss: 0.3721 - regression_loss: 0.3153 - classification_loss: 0.0569
1493/3000 [=============>................] - ETA: 10:19 - loss: 0.3721 - regression_loss: 0.3153 - classification_loss: 0.0568
1494/3000 [=============>................] - ETA: 10:18 - loss: 0.3721 - regression_loss: 0.3153 - classification_loss: 0.0568
1495/3000 [=============>................] - ETA: 10:18 - loss: 0.3722 - regression_loss: 0.3154 - classification_loss: 0.0568
1496/3000 [=============>................] - ETA: 10:18 - loss: 0.3724 - regression_loss: 0.3155 - classification_loss: 0.0568
1497/3000 [=============>................] - ETA: 10:17 - loss: 0.3724 - regression_loss: 0.3156 - classification_loss: 0.0568
1498/3000 [=============>................] - ETA: 10:17 - loss: 0.3724 - regression_loss: 0.3156 - classification_loss: 0.0568
1499/3000 [=============>................] - ETA: 10:16 - loss: 0.3724 - regression_loss: 0.3156 - classification_loss: 0.0568
1500/3000 [==============>...............] - ETA: 10:16 - loss: 0.3724 - regression_loss: 0.3157 - classification_loss: 0.0568
1501/3000 [==============>...............] - ETA: 10:16 - loss: 0.3724 - regression_loss: 0.3156 - classification_loss: 0.0568
1502/3000 [==============>...............] - ETA: 10:15 - loss: 0.3724 - regression_loss: 0.3156 - classification_loss: 0.0567
1503/3000 [==============>...............] - ETA: 10:15 - loss: 0.3723 - regression_loss: 0.3156 - classification_loss: 0.0567
1504/3000 [==============>...............] - ETA: 10:14 - loss: 0.3727 - regression_loss: 0.3159 - classification_loss: 0.0568
1505/3000 [==============>...............] - ETA: 10:14 - loss: 0.3727 - regression_loss: 0.3160 - classification_loss: 0.0567
1506/3000 [==============>...............] - ETA: 10:14 - loss: 0.3728 - regression_loss: 0.3160 - classification_loss: 0.0568
1507/3000 [==============>...............] - ETA: 10:13 - loss: 0.3728 - regression_loss: 0.3160 - classification_loss: 0.0568
1508/3000 [==============>...............] - ETA: 10:13 - loss: 0.3726 - regression_loss: 0.3159 - classification_loss: 0.0567
1509/3000 [==============>...............] - ETA: 10:12 - loss: 0.3725 - regression_loss: 0.3158 - classification_loss: 0.0567
1510/3000 [==============>...............] - ETA: 10:12 - loss: 0.3724 - regression_loss: 0.3157 - classification_loss: 0.0567
1511/3000 [==============>...............] - ETA: 10:12 - loss: 0.3723 - regression_loss: 0.3156 - classification_loss: 0.0567
1512/3000 [==============>...............] - ETA: 10:11 - loss: 0.3722 - regression_loss: 0.3156 - classification_loss: 0.0567
1513/3000 [==============>...............] - ETA: 10:11 - loss: 0.3722 - regression_loss: 0.3155 - classification_loss: 0.0566
1514/3000 [==============>...............] - ETA: 10:10 - loss: 0.3723 - regression_loss: 0.3156 - classification_loss: 0.0566
1515/3000 [==============>...............] - ETA: 10:10 - loss: 0.3722 - regression_loss: 0.3156 - classification_loss: 0.0566
1516/3000 [==============>...............] - ETA: 10:10 - loss: 0.3723 - regression_loss: 0.3157 - classification_loss: 0.0566
1517/3000 [==============>...............] - ETA: 10:09 - loss: 0.3723 - regression_loss: 0.3157 - classification_loss: 0.0566
1518/3000 [==============>...............] - ETA: 10:09 - loss: 0.3723 - regression_loss: 0.3156 - classification_loss: 0.0566
1519/3000 [==============>...............] - ETA: 10:08 - loss: 0.3725 - regression_loss: 0.3158 - classification_loss: 0.0566
1520/3000 [==============>...............] - ETA: 10:08 - loss: 0.3723 - regression_loss: 0.3157 - classification_loss: 0.0566
1521/3000 [==============>...............] - ETA: 10:08 - loss: 0.3723 - regression_loss: 0.3157 - classification_loss: 0.0566
1522/3000 [==============>...............] - ETA: 10:07 - loss: 0.3722 - regression_loss: 0.3156 - classification_loss: 0.0566
1523/3000 [==============>...............] - ETA: 10:07 - loss: 0.3723 - regression_loss: 0.3157 - classification_loss: 0.0566
1524/3000 [==============>...............] - ETA: 10:06 - loss: 0.3721 - regression_loss: 0.3155 - classification_loss: 0.0566
1525/3000 [==============>...............] - ETA: 10:06 - loss: 0.3721 - regression_loss: 0.3155 - classification_loss: 0.0566
1526/3000 [==============>...............] - ETA: 10:05 - loss: 0.3720 - regression_loss: 0.3154 - classification_loss: 0.0566
1527/3000 [==============>...............] - ETA: 10:05 - loss: 0.3720 - regression_loss: 0.3154 - classification_loss: 0.0566
1528/3000 [==============>...............] - ETA: 10:05 - loss: 0.3719 - regression_loss: 0.3153 - classification_loss: 0.0566
1529/3000 [==============>...............] - ETA: 10:04 - loss: 0.3720 - regression_loss: 0.3154 - classification_loss: 0.0566
1530/3000 [==============>...............] - ETA: 10:04 - loss: 0.3719 - regression_loss: 0.3153 - classification_loss: 0.0565
1531/3000 [==============>...............] - ETA: 10:03 - loss: 0.3719 - regression_loss: 0.3153 - classification_loss: 0.0565
1532/3000 [==============>...............] - ETA: 10:03 - loss: 0.3719 - regression_loss: 0.3154 - classification_loss: 0.0565
1533/3000 [==============>...............] - ETA: 10:03 - loss: 0.3719 - regression_loss: 0.3154 - classification_loss: 0.0565
1534/3000 [==============>...............] - ETA: 10:02 - loss: 0.3718 - regression_loss: 0.3154 - classification_loss: 0.0565
1535/3000 [==============>...............] - ETA: 10:02 - loss: 0.3719 - regression_loss: 0.3154 - classification_loss: 0.0565
1536/3000 [==============>...............] - ETA: 10:01 - loss: 0.3718 - regression_loss: 0.3154 - classification_loss: 0.0565
1537/3000 [==============>...............] - ETA: 10:01 - loss: 0.3718 - regression_loss: 0.3154 - classification_loss: 0.0564
1538/3000 [==============>...............] - ETA: 10:00 - loss: 0.3718 - regression_loss: 0.3154 - classification_loss: 0.0564
1539/3000 [==============>...............] - ETA: 10:00 - loss: 0.3719 - regression_loss: 0.3155 - classification_loss: 0.0564
1540/3000 [==============>...............] - ETA: 10:00 - loss: 0.3719 - regression_loss: 0.3155 - classification_loss: 0.0564
1541/3000 [==============>...............] - ETA: 9:59 - loss: 0.3718 - regression_loss: 0.3154 - classification_loss: 0.0564 
1542/3000 [==============>...............] - ETA: 9:59 - loss: 0.3717 - regression_loss: 0.3153 - classification_loss: 0.0564
1543/3000 [==============>...............] - ETA: 9:58 - loss: 0.3716 - regression_loss: 0.3152 - classification_loss: 0.0563
1544/3000 [==============>...............] - ETA: 9:58 - loss: 0.3714 - regression_loss: 0.3151 - classification_loss: 0.0563
1545/3000 [==============>...............] - ETA: 9:58 - loss: 0.3714 - regression_loss: 0.3151 - classification_loss: 0.0563
1546/3000 [==============>...............] - ETA: 9:57 - loss: 0.3714 - regression_loss: 0.3151 - classification_loss: 0.0563
1547/3000 [==============>...............] - ETA: 9:57 - loss: 0.3715 - regression_loss: 0.3153 - classification_loss: 0.0563
1548/3000 [==============>...............] - ETA: 9:56 - loss: 0.3716 - regression_loss: 0.3153 - classification_loss: 0.0563
1549/3000 [==============>...............] - ETA: 9:56 - loss: 0.3716 - regression_loss: 0.3153 - classification_loss: 0.0563
1550/3000 [==============>...............] - ETA: 9:56 - loss: 0.3715 - regression_loss: 0.3153 - classification_loss: 0.0562
1551/3000 [==============>...............] - ETA: 9:55 - loss: 0.3716 - regression_loss: 0.3154 - classification_loss: 0.0562
1552/3000 [==============>...............] - ETA: 9:55 - loss: 0.3716 - regression_loss: 0.3154 - classification_loss: 0.0562
1553/3000 [==============>...............] - ETA: 9:54 - loss: 0.3716 - regression_loss: 0.3154 - classification_loss: 0.0562
1554/3000 [==============>...............] - ETA: 9:54 - loss: 0.3716 - regression_loss: 0.3155 - classification_loss: 0.0562
1555/3000 [==============>...............] - ETA: 9:53 - loss: 0.3715 - regression_loss: 0.3154 - classification_loss: 0.0562
1556/3000 [==============>...............] - ETA: 9:53 - loss: 0.3715 - regression_loss: 0.3153 - classification_loss: 0.0561
1557/3000 [==============>...............] - ETA: 9:53 - loss: 0.3714 - regression_loss: 0.3153 - classification_loss: 0.0561
1558/3000 [==============>...............] - ETA: 9:52 - loss: 0.3714 - regression_loss: 0.3153 - classification_loss: 0.0561
1559/3000 [==============>...............] - ETA: 9:52 - loss: 0.3712 - regression_loss: 0.3152 - classification_loss: 0.0561
1560/3000 [==============>...............] - ETA: 9:51 - loss: 0.3712 - regression_loss: 0.3151 - classification_loss: 0.0561
1561/3000 [==============>...............] - ETA: 9:51 - loss: 0.3712 - regression_loss: 0.3152 - classification_loss: 0.0561
1562/3000 [==============>...............] - ETA: 9:51 - loss: 0.3712 - regression_loss: 0.3152 - classification_loss: 0.0560
1563/3000 [==============>...............] - ETA: 9:50 - loss: 0.3711 - regression_loss: 0.3151 - classification_loss: 0.0560
1564/3000 [==============>...............] - ETA: 9:50 - loss: 0.3710 - regression_loss: 0.3150 - classification_loss: 0.0560
1565/3000 [==============>...............] - ETA: 9:49 - loss: 0.3709 - regression_loss: 0.3149 - classification_loss: 0.0560
1566/3000 [==============>...............] - ETA: 9:49 - loss: 0.3709 - regression_loss: 0.3149 - classification_loss: 0.0560
1567/3000 [==============>...............] - ETA: 9:49 - loss: 0.3708 - regression_loss: 0.3149 - classification_loss: 0.0560
1568/3000 [==============>...............] - ETA: 9:48 - loss: 0.3707 - regression_loss: 0.3148 - classification_loss: 0.0559
1569/3000 [==============>...............] - ETA: 9:48 - loss: 0.3706 - regression_loss: 0.3147 - classification_loss: 0.0559
1570/3000 [==============>...............] - ETA: 9:47 - loss: 0.3707 - regression_loss: 0.3148 - classification_loss: 0.0559
1571/3000 [==============>...............] - ETA: 9:47 - loss: 0.3706 - regression_loss: 0.3147 - classification_loss: 0.0559
1572/3000 [==============>...............] - ETA: 9:46 - loss: 0.3705 - regression_loss: 0.3146 - classification_loss: 0.0559
1573/3000 [==============>...............] - ETA: 9:46 - loss: 0.3706 - regression_loss: 0.3147 - classification_loss: 0.0559
1574/3000 [==============>...............] - ETA: 9:46 - loss: 0.3705 - regression_loss: 0.3147 - classification_loss: 0.0558
1575/3000 [==============>...............] - ETA: 9:45 - loss: 0.3704 - regression_loss: 0.3146 - classification_loss: 0.0558
1576/3000 [==============>...............] - ETA: 9:45 - loss: 0.3704 - regression_loss: 0.3146 - classification_loss: 0.0558
1577/3000 [==============>...............] - ETA: 9:44 - loss: 0.3704 - regression_loss: 0.3146 - classification_loss: 0.0558
1578/3000 [==============>...............] - ETA: 9:44 - loss: 0.3703 - regression_loss: 0.3146 - classification_loss: 0.0558
1579/3000 [==============>...............] - ETA: 9:44 - loss: 0.3705 - regression_loss: 0.3147 - classification_loss: 0.0558
1580/3000 [==============>...............] - ETA: 9:43 - loss: 0.3705 - regression_loss: 0.3147 - classification_loss: 0.0558
1581/3000 [==============>...............] - ETA: 9:43 - loss: 0.3704 - regression_loss: 0.3147 - classification_loss: 0.0557
1582/3000 [==============>...............] - ETA: 9:42 - loss: 0.3704 - regression_loss: 0.3147 - classification_loss: 0.0557
1583/3000 [==============>...............] - ETA: 9:42 - loss: 0.3703 - regression_loss: 0.3146 - classification_loss: 0.0557
1584/3000 [==============>...............] - ETA: 9:42 - loss: 0.3702 - regression_loss: 0.3146 - classification_loss: 0.0557
1585/3000 [==============>...............] - ETA: 9:41 - loss: 0.3702 - regression_loss: 0.3146 - classification_loss: 0.0556
1586/3000 [==============>...............] - ETA: 9:41 - loss: 0.3704 - regression_loss: 0.3147 - classification_loss: 0.0556
1587/3000 [==============>...............] - ETA: 9:40 - loss: 0.3705 - regression_loss: 0.3149 - classification_loss: 0.0556
1588/3000 [==============>...............] - ETA: 9:40 - loss: 0.3704 - regression_loss: 0.3148 - classification_loss: 0.0556
1589/3000 [==============>...............] - ETA: 9:40 - loss: 0.3705 - regression_loss: 0.3149 - classification_loss: 0.0556
1590/3000 [==============>...............] - ETA: 9:39 - loss: 0.3704 - regression_loss: 0.3149 - classification_loss: 0.0556
1591/3000 [==============>...............] - ETA: 9:39 - loss: 0.3703 - regression_loss: 0.3148 - classification_loss: 0.0556
1592/3000 [==============>...............] - ETA: 9:38 - loss: 0.3703 - regression_loss: 0.3147 - classification_loss: 0.0555
1593/3000 [==============>...............] - ETA: 9:38 - loss: 0.3702 - regression_loss: 0.3147 - classification_loss: 0.0555
1594/3000 [==============>...............] - ETA: 9:37 - loss: 0.3702 - regression_loss: 0.3147 - classification_loss: 0.0555
1595/3000 [==============>...............] - ETA: 9:37 - loss: 0.3702 - regression_loss: 0.3147 - classification_loss: 0.0555
1596/3000 [==============>...............] - ETA: 9:37 - loss: 0.3702 - regression_loss: 0.3147 - classification_loss: 0.0555
1597/3000 [==============>...............] - ETA: 9:36 - loss: 0.3701 - regression_loss: 0.3147 - classification_loss: 0.0555
1598/3000 [==============>...............] - ETA: 9:36 - loss: 0.3702 - regression_loss: 0.3147 - classification_loss: 0.0554
1599/3000 [==============>...............] - ETA: 9:35 - loss: 0.3706 - regression_loss: 0.3151 - classification_loss: 0.0555
1600/3000 [===============>..............] - ETA: 9:35 - loss: 0.3705 - regression_loss: 0.3151 - classification_loss: 0.0555
1601/3000 [===============>..............] - ETA: 9:35 - loss: 0.3705 - regression_loss: 0.3150 - classification_loss: 0.0554
1602/3000 [===============>..............] - ETA: 9:34 - loss: 0.3706 - regression_loss: 0.3152 - classification_loss: 0.0555
1603/3000 [===============>..............] - ETA: 9:34 - loss: 0.3708 - regression_loss: 0.3153 - classification_loss: 0.0555
1604/3000 [===============>..............] - ETA: 9:33 - loss: 0.3708 - regression_loss: 0.3153 - classification_loss: 0.0555
1605/3000 [===============>..............] - ETA: 9:33 - loss: 0.3709 - regression_loss: 0.3154 - classification_loss: 0.0555
1606/3000 [===============>..............] - ETA: 9:33 - loss: 0.3711 - regression_loss: 0.3156 - classification_loss: 0.0555
1607/3000 [===============>..............] - ETA: 9:32 - loss: 0.3711 - regression_loss: 0.3156 - classification_loss: 0.0555
1608/3000 [===============>..............] - ETA: 9:32 - loss: 0.3711 - regression_loss: 0.3156 - classification_loss: 0.0555
1609/3000 [===============>..............] - ETA: 9:31 - loss: 0.3710 - regression_loss: 0.3155 - classification_loss: 0.0554
1610/3000 [===============>..............] - ETA: 9:31 - loss: 0.3710 - regression_loss: 0.3155 - classification_loss: 0.0554
1611/3000 [===============>..............] - ETA: 9:31 - loss: 0.3709 - regression_loss: 0.3155 - classification_loss: 0.0554
1612/3000 [===============>..............] - ETA: 9:30 - loss: 0.3708 - regression_loss: 0.3154 - classification_loss: 0.0554
1613/3000 [===============>..............] - ETA: 9:30 - loss: 0.3708 - regression_loss: 0.3154 - classification_loss: 0.0554
1614/3000 [===============>..............] - ETA: 9:29 - loss: 0.3708 - regression_loss: 0.3154 - classification_loss: 0.0554
1615/3000 [===============>..............] - ETA: 9:29 - loss: 0.3709 - regression_loss: 0.3155 - classification_loss: 0.0554
1616/3000 [===============>..............] - ETA: 9:28 - loss: 0.3711 - regression_loss: 0.3156 - classification_loss: 0.0554
1617/3000 [===============>..............] - ETA: 9:28 - loss: 0.3710 - regression_loss: 0.3156 - classification_loss: 0.0554
1618/3000 [===============>..............] - ETA: 9:28 - loss: 0.3709 - regression_loss: 0.3155 - classification_loss: 0.0554
1619/3000 [===============>..............] - ETA: 9:27 - loss: 0.3709 - regression_loss: 0.3155 - classification_loss: 0.0554
1620/3000 [===============>..............] - ETA: 9:27 - loss: 0.3708 - regression_loss: 0.3155 - classification_loss: 0.0553
1621/3000 [===============>..............] - ETA: 9:26 - loss: 0.3709 - regression_loss: 0.3156 - classification_loss: 0.0553
1622/3000 [===============>..............] - ETA: 9:26 - loss: 0.3709 - regression_loss: 0.3156 - classification_loss: 0.0553
1623/3000 [===============>..............] - ETA: 9:26 - loss: 0.3709 - regression_loss: 0.3156 - classification_loss: 0.0553
1624/3000 [===============>..............] - ETA: 9:25 - loss: 0.3708 - regression_loss: 0.3155 - classification_loss: 0.0553
1625/3000 [===============>..............] - ETA: 9:25 - loss: 0.3708 - regression_loss: 0.3155 - classification_loss: 0.0553
1626/3000 [===============>..............] - ETA: 9:24 - loss: 0.3709 - regression_loss: 0.3156 - classification_loss: 0.0553
1627/3000 [===============>..............] - ETA: 9:24 - loss: 0.3708 - regression_loss: 0.3155 - classification_loss: 0.0553
1628/3000 [===============>..............] - ETA: 9:24 - loss: 0.3708 - regression_loss: 0.3155 - classification_loss: 0.0553
1629/3000 [===============>..............] - ETA: 9:23 - loss: 0.3709 - regression_loss: 0.3156 - classification_loss: 0.0553
1630/3000 [===============>..............] - ETA: 9:23 - loss: 0.3708 - regression_loss: 0.3156 - classification_loss: 0.0552
1631/3000 [===============>..............] - ETA: 9:22 - loss: 0.3707 - regression_loss: 0.3155 - classification_loss: 0.0552
1632/3000 [===============>..............] - ETA: 9:22 - loss: 0.3706 - regression_loss: 0.3154 - classification_loss: 0.0552
1633/3000 [===============>..............] - ETA: 9:21 - loss: 0.3705 - regression_loss: 0.3153 - classification_loss: 0.0552
1634/3000 [===============>..............] - ETA: 9:21 - loss: 0.3706 - regression_loss: 0.3154 - classification_loss: 0.0552
1635/3000 [===============>..............] - ETA: 9:21 - loss: 0.3707 - regression_loss: 0.3155 - classification_loss: 0.0552
1636/3000 [===============>..............] - ETA: 9:20 - loss: 0.3706 - regression_loss: 0.3155 - classification_loss: 0.0551
1637/3000 [===============>..............] - ETA: 9:20 - loss: 0.3706 - regression_loss: 0.3155 - classification_loss: 0.0551
1638/3000 [===============>..............] - ETA: 9:19 - loss: 0.3705 - regression_loss: 0.3154 - classification_loss: 0.0551
1639/3000 [===============>..............] - ETA: 9:19 - loss: 0.3704 - regression_loss: 0.3153 - classification_loss: 0.0551
1640/3000 [===============>..............] - ETA: 9:19 - loss: 0.3704 - regression_loss: 0.3153 - classification_loss: 0.0551
1641/3000 [===============>..............] - ETA: 9:18 - loss: 0.3703 - regression_loss: 0.3152 - classification_loss: 0.0551
1642/3000 [===============>..............] - ETA: 9:18 - loss: 0.3702 - regression_loss: 0.3152 - classification_loss: 0.0551
1643/3000 [===============>..............] - ETA: 9:17 - loss: 0.3702 - regression_loss: 0.3151 - classification_loss: 0.0550
1644/3000 [===============>..............] - ETA: 9:17 - loss: 0.3701 - regression_loss: 0.3151 - classification_loss: 0.0550
1645/3000 [===============>..............] - ETA: 9:17 - loss: 0.3700 - regression_loss: 0.3151 - classification_loss: 0.0550
1646/3000 [===============>..............] - ETA: 9:16 - loss: 0.3699 - regression_loss: 0.3150 - classification_loss: 0.0550
1647/3000 [===============>..............] - ETA: 9:16 - loss: 0.3700 - regression_loss: 0.3150 - classification_loss: 0.0549
1648/3000 [===============>..............] - ETA: 9:15 - loss: 0.3699 - regression_loss: 0.3150 - classification_loss: 0.0549
1649/3000 [===============>..............] - ETA: 9:15 - loss: 0.3699 - regression_loss: 0.3150 - classification_loss: 0.0549
1650/3000 [===============>..............] - ETA: 9:14 - loss: 0.3698 - regression_loss: 0.3149 - classification_loss: 0.0549
1651/3000 [===============>..............] - ETA: 9:14 - loss: 0.3698 - regression_loss: 0.3149 - classification_loss: 0.0549
1652/3000 [===============>..............] - ETA: 9:14 - loss: 0.3697 - regression_loss: 0.3148 - classification_loss: 0.0548
1653/3000 [===============>..............] - ETA: 9:13 - loss: 0.3696 - regression_loss: 0.3147 - classification_loss: 0.0548
1654/3000 [===============>..............] - ETA: 9:13 - loss: 0.3695 - regression_loss: 0.3146 - classification_loss: 0.0548
1655/3000 [===============>..............] - ETA: 9:12 - loss: 0.3694 - regression_loss: 0.3146 - classification_loss: 0.0548
1656/3000 [===============>..............] - ETA: 9:12 - loss: 0.3693 - regression_loss: 0.3145 - classification_loss: 0.0548
1657/3000 [===============>..............] - ETA: 9:12 - loss: 0.3691 - regression_loss: 0.3144 - classification_loss: 0.0547
1658/3000 [===============>..............] - ETA: 9:11 - loss: 0.3691 - regression_loss: 0.3144 - classification_loss: 0.0547
1659/3000 [===============>..............] - ETA: 9:11 - loss: 0.3692 - regression_loss: 0.3144 - classification_loss: 0.0547
1660/3000 [===============>..............] - ETA: 9:10 - loss: 0.3691 - regression_loss: 0.3144 - classification_loss: 0.0547
1661/3000 [===============>..............] - ETA: 9:10 - loss: 0.3691 - regression_loss: 0.3144 - classification_loss: 0.0547
1662/3000 [===============>..............] - ETA: 9:10 - loss: 0.3690 - regression_loss: 0.3143 - classification_loss: 0.0547
1663/3000 [===============>..............] - ETA: 9:09 - loss: 0.3691 - regression_loss: 0.3144 - classification_loss: 0.0547
1664/3000 [===============>..............] - ETA: 9:09 - loss: 0.3690 - regression_loss: 0.3144 - classification_loss: 0.0547
1665/3000 [===============>..............] - ETA: 9:08 - loss: 0.3690 - regression_loss: 0.3144 - classification_loss: 0.0546
1666/3000 [===============>..............] - ETA: 9:08 - loss: 0.3690 - regression_loss: 0.3144 - classification_loss: 0.0546
1667/3000 [===============>..............] - ETA: 9:07 - loss: 0.3690 - regression_loss: 0.3144 - classification_loss: 0.0546
1668/3000 [===============>..............] - ETA: 9:07 - loss: 0.3689 - regression_loss: 0.3143 - classification_loss: 0.0546
1669/3000 [===============>..............] - ETA: 9:07 - loss: 0.3688 - regression_loss: 0.3143 - classification_loss: 0.0546
1670/3000 [===============>..............] - ETA: 9:06 - loss: 0.3688 - regression_loss: 0.3142 - classification_loss: 0.0546
1671/3000 [===============>..............] - ETA: 9:06 - loss: 0.3688 - regression_loss: 0.3143 - classification_loss: 0.0546
1672/3000 [===============>..............] - ETA: 9:05 - loss: 0.3689 - regression_loss: 0.3143 - classification_loss: 0.0545
1673/3000 [===============>..............] - ETA: 9:05 - loss: 0.3688 - regression_loss: 0.3143 - classification_loss: 0.0545
1674/3000 [===============>..............] - ETA: 9:05 - loss: 0.3689 - regression_loss: 0.3143 - classification_loss: 0.0545
1675/3000 [===============>..............] - ETA: 9:04 - loss: 0.3687 - regression_loss: 0.3142 - classification_loss: 0.0545
1676/3000 [===============>..............] - ETA: 9:04 - loss: 0.3686 - regression_loss: 0.3142 - classification_loss: 0.0545
1677/3000 [===============>..............] - ETA: 9:03 - loss: 0.3686 - regression_loss: 0.3141 - classification_loss: 0.0545
1678/3000 [===============>..............] - ETA: 9:03 - loss: 0.3686 - regression_loss: 0.3141 - classification_loss: 0.0545
1679/3000 [===============>..............] - ETA: 9:03 - loss: 0.3686 - regression_loss: 0.3141 - classification_loss: 0.0544
1680/3000 [===============>..............] - ETA: 9:02 - loss: 0.3685 - regression_loss: 0.3141 - classification_loss: 0.0544
1681/3000 [===============>..............] - ETA: 9:02 - loss: 0.3685 - regression_loss: 0.3141 - classification_loss: 0.0544
1682/3000 [===============>..............] - ETA: 9:01 - loss: 0.3684 - regression_loss: 0.3140 - classification_loss: 0.0544
1683/3000 [===============>..............] - ETA: 9:01 - loss: 0.3685 - regression_loss: 0.3142 - classification_loss: 0.0544
1684/3000 [===============>..............] - ETA: 9:01 - loss: 0.3686 - regression_loss: 0.3142 - classification_loss: 0.0544
1685/3000 [===============>..............] - ETA: 9:00 - loss: 0.3685 - regression_loss: 0.3141 - classification_loss: 0.0544
1686/3000 [===============>..............] - ETA: 9:00 - loss: 0.3684 - regression_loss: 0.3141 - classification_loss: 0.0544
1687/3000 [===============>..............] - ETA: 8:59 - loss: 0.3683 - regression_loss: 0.3139 - classification_loss: 0.0543
1688/3000 [===============>..............] - ETA: 8:59 - loss: 0.3682 - regression_loss: 0.3139 - classification_loss: 0.0543
1689/3000 [===============>..............] - ETA: 8:59 - loss: 0.3681 - regression_loss: 0.3138 - classification_loss: 0.0543
1690/3000 [===============>..............] - ETA: 8:58 - loss: 0.3681 - regression_loss: 0.3138 - classification_loss: 0.0543
1691/3000 [===============>..............] - ETA: 8:58 - loss: 0.3680 - regression_loss: 0.3138 - classification_loss: 0.0543
1692/3000 [===============>..............] - ETA: 8:57 - loss: 0.3680 - regression_loss: 0.3137 - classification_loss: 0.0543
1693/3000 [===============>..............] - ETA: 8:57 - loss: 0.3681 - regression_loss: 0.3138 - classification_loss: 0.0543
1694/3000 [===============>..............] - ETA: 8:57 - loss: 0.3679 - regression_loss: 0.3137 - classification_loss: 0.0543
1695/3000 [===============>..............] - ETA: 8:56 - loss: 0.3679 - regression_loss: 0.3136 - classification_loss: 0.0543
1696/3000 [===============>..............] - ETA: 8:56 - loss: 0.3678 - regression_loss: 0.3136 - classification_loss: 0.0542
1697/3000 [===============>..............] - ETA: 8:55 - loss: 0.3679 - regression_loss: 0.3137 - classification_loss: 0.0542
1698/3000 [===============>..............] - ETA: 8:55 - loss: 0.3679 - regression_loss: 0.3136 - classification_loss: 0.0542
1699/3000 [===============>..............] - ETA: 8:54 - loss: 0.3678 - regression_loss: 0.3135 - classification_loss: 0.0542
1700/3000 [================>.............] - ETA: 8:54 - loss: 0.3677 - regression_loss: 0.3135 - classification_loss: 0.0542
1701/3000 [================>.............] - ETA: 8:54 - loss: 0.3676 - regression_loss: 0.3134 - classification_loss: 0.0542
1702/3000 [================>.............] - ETA: 8:53 - loss: 0.3676 - regression_loss: 0.3134 - classification_loss: 0.0542
1703/3000 [================>.............] - ETA: 8:53 - loss: 0.3675 - regression_loss: 0.3133 - classification_loss: 0.0541
1704/3000 [================>.............] - ETA: 8:52 - loss: 0.3675 - regression_loss: 0.3134 - classification_loss: 0.0541
1705/3000 [================>.............] - ETA: 8:52 - loss: 0.3675 - regression_loss: 0.3134 - classification_loss: 0.0541
1706/3000 [================>.............] - ETA: 8:52 - loss: 0.3674 - regression_loss: 0.3133 - classification_loss: 0.0541
1707/3000 [================>.............] - ETA: 8:51 - loss: 0.3674 - regression_loss: 0.3133 - classification_loss: 0.0541
1708/3000 [================>.............] - ETA: 8:51 - loss: 0.3673 - regression_loss: 0.3132 - classification_loss: 0.0541
1709/3000 [================>.............] - ETA: 8:50 - loss: 0.3675 - regression_loss: 0.3134 - classification_loss: 0.0541
1710/3000 [================>.............] - ETA: 8:50 - loss: 0.3674 - regression_loss: 0.3133 - classification_loss: 0.0541
1711/3000 [================>.............] - ETA: 8:49 - loss: 0.3675 - regression_loss: 0.3134 - classification_loss: 0.0541
1712/3000 [================>.............] - ETA: 8:49 - loss: 0.3675 - regression_loss: 0.3134 - classification_loss: 0.0541
1713/3000 [================>.............] - ETA: 8:49 - loss: 0.3674 - regression_loss: 0.3134 - classification_loss: 0.0540
1714/3000 [================>.............] - ETA: 8:48 - loss: 0.3673 - regression_loss: 0.3133 - classification_loss: 0.0540
1715/3000 [================>.............] - ETA: 8:48 - loss: 0.3672 - regression_loss: 0.3132 - classification_loss: 0.0540
1716/3000 [================>.............] - ETA: 8:47 - loss: 0.3671 - regression_loss: 0.3131 - classification_loss: 0.0540
1717/3000 [================>.............] - ETA: 8:47 - loss: 0.3671 - regression_loss: 0.3131 - classification_loss: 0.0540
1718/3000 [================>.............] - ETA: 8:47 - loss: 0.3670 - regression_loss: 0.3130 - classification_loss: 0.0540
1719/3000 [================>.............] - ETA: 8:46 - loss: 0.3671 - regression_loss: 0.3131 - classification_loss: 0.0540
1720/3000 [================>.............] - ETA: 8:46 - loss: 0.3672 - regression_loss: 0.3132 - classification_loss: 0.0540
1721/3000 [================>.............] - ETA: 8:45 - loss: 0.3671 - regression_loss: 0.3131 - classification_loss: 0.0540
1722/3000 [================>.............] - ETA: 8:45 - loss: 0.3671 - regression_loss: 0.3132 - classification_loss: 0.0540
1723/3000 [================>.............] - ETA: 8:45 - loss: 0.3671 - regression_loss: 0.3131 - classification_loss: 0.0540
1724/3000 [================>.............] - ETA: 8:44 - loss: 0.3670 - regression_loss: 0.3130 - classification_loss: 0.0540
1725/3000 [================>.............] - ETA: 8:44 - loss: 0.3669 - regression_loss: 0.3130 - classification_loss: 0.0540
1726/3000 [================>.............] - ETA: 8:43 - loss: 0.3669 - regression_loss: 0.3129 - classification_loss: 0.0540
1727/3000 [================>.............] - ETA: 8:43 - loss: 0.3668 - regression_loss: 0.3129 - classification_loss: 0.0540
1728/3000 [================>.............] - ETA: 8:42 - loss: 0.3667 - regression_loss: 0.3127 - classification_loss: 0.0539
1729/3000 [================>.............] - ETA: 8:42 - loss: 0.3667 - regression_loss: 0.3127 - classification_loss: 0.0539
1730/3000 [================>.............] - ETA: 8:42 - loss: 0.3666 - regression_loss: 0.3127 - classification_loss: 0.0539
1731/3000 [================>.............] - ETA: 8:41 - loss: 0.3665 - regression_loss: 0.3126 - classification_loss: 0.0539
1732/3000 [================>.............] - ETA: 8:41 - loss: 0.3664 - regression_loss: 0.3125 - classification_loss: 0.0539
1733/3000 [================>.............] - ETA: 8:40 - loss: 0.3663 - regression_loss: 0.3124 - classification_loss: 0.0539
1734/3000 [================>.............] - ETA: 8:40 - loss: 0.3663 - regression_loss: 0.3124 - classification_loss: 0.0539
1735/3000 [================>.............] - ETA: 8:40 - loss: 0.3662 - regression_loss: 0.3123 - classification_loss: 0.0539
1736/3000 [================>.............] - ETA: 8:39 - loss: 0.3662 - regression_loss: 0.3124 - classification_loss: 0.0539
1737/3000 [================>.............] - ETA: 8:39 - loss: 0.3662 - regression_loss: 0.3123 - classification_loss: 0.0539
1738/3000 [================>.............] - ETA: 8:38 - loss: 0.3662 - regression_loss: 0.3123 - classification_loss: 0.0538
1739/3000 [================>.............] - ETA: 8:38 - loss: 0.3662 - regression_loss: 0.3124 - classification_loss: 0.0539